// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";

export interface AlbBackendGroupGrpcBackend {
    /**
     * Healthcheck specification that will be used by this backend. Structure is documented below.
     */
    healthcheck?: outputs.AlbBackendGroupGrpcBackendHealthcheck;
    /**
     * Load Balancing Config specification that will be used by this backend. Structure is documented below.
     */
    loadBalancingConfig?: outputs.AlbBackendGroupGrpcBackendLoadBalancingConfig;
    /**
     * Name of the backend.
     */
    name: string;
    /**
     * Port for incoming traffic.
     */
    port?: number;
    /**
     * References target groups for the backend.
     */
    targetGroupIds: string[];
    /**
     * Tls specification that will be used by this backend. Structure is documented below.
     */
    tls?: outputs.AlbBackendGroupGrpcBackendTls;
    /**
     * Weight of the backend. Traffic will be split between backends of the same BackendGroup according to their weights.
     */
    weight?: number;
}

export interface AlbBackendGroupGrpcBackendHealthcheck {
    /**
     * Grpc Healthcheck specification that will be used by this healthcheck. Structure is documented below.
     *
     * > **NOTE:** Only one of `streamHealthcheck` or `httpHealthcheck` or `grpcHealthcheck` should be specified.
     */
    grpcHealthcheck?: outputs.AlbBackendGroupGrpcBackendHealthcheckGrpcHealthcheck;
    /**
     * Optional alternative port for health checking.
     */
    healthcheckPort?: number;
    /**
     * Number of consecutive successful health checks required to promote endpoint into the healthy state. 0 means 1. Note that during startup, only a single successful health check is required to mark a host healthy.
     */
    healthyThreshold?: number;
    /**
     * Http Healthcheck specification that will be used by this healthcheck. Structure is documented below.
     */
    httpHealthcheck?: outputs.AlbBackendGroupGrpcBackendHealthcheckHttpHealthcheck;
    /**
     * Interval between health checks.
     */
    interval: string;
    /**
     * An optional jitter amount as a percentage of interval. If specified, during every interval value of (interval_ms * intervalJitterPercent / 100) will be added to the wait time.
     */
    intervalJitterPercent?: number;
    /**
     * Stream Healthcheck specification that will be used by this healthcheck. Structure is documented below.
     */
    streamHealthcheck?: outputs.AlbBackendGroupGrpcBackendHealthcheckStreamHealthcheck;
    /**
     * Time to wait for a health check response.
     */
    timeout: string;
    /**
     * Number of consecutive failed health checks required to demote endpoint into the unhealthy state. 0 means 1. Note that for HTTP health checks, a single 503 immediately makes endpoint unhealthy.
     */
    unhealthyThreshold?: number;
}

export interface AlbBackendGroupGrpcBackendHealthcheckGrpcHealthcheck {
    /**
     * Service name for grpc.health.v1.HealthCheckRequest message.
     */
    serviceName?: string;
}

export interface AlbBackendGroupGrpcBackendHealthcheckHttpHealthcheck {
    /**
     * "Host" HTTP header value.
     */
    host?: string;
    /**
     * If set, health checks will use HTTP2.
     */
    http2?: boolean;
    /**
     * HTTP path.
     */
    path: string;
}

export interface AlbBackendGroupGrpcBackendHealthcheckStreamHealthcheck {
    /**
     * Data that must be contained in the messages received from targets for a successful health check. If not specified, no messages are expected from targets, and those that are received are not checked.
     */
    receive?: string;
    /**
     * Message sent to targets during TCP data transfer.  If not specified, no data is sent to the target.
     */
    send?: string;
}

export interface AlbBackendGroupGrpcBackendLoadBalancingConfig {
    /**
     * Percent of traffic to be sent to the same availability zone. The rest will be equally divided between other zones.
     */
    localityAwareRoutingPercent?: number;
    /**
     * Load balancing mode for the backend. Possible values: "ROUND_ROBIN", "RANDOM", "LEAST_REQUEST", "MAGLEV_HASH".
     */
    mode?: string;
    /**
     * If percentage of healthy hosts in the backend is lower than panic_threshold, traffic will be routed to all backends no matter what the health status is. This helps to avoid healthy backends overloading  when everything is bad. Zero means no panic threshold.
     */
    panicThreshold?: number;
    /**
     * If set, will route requests only to the same availability zone. Balancer won't know about endpoints in other zones.
     */
    strictLocality?: boolean;
}

export interface AlbBackendGroupGrpcBackendTls {
    /**
     * [SNI](https://en.wikipedia.org/wiki/Server_Name_Indication) string for TLS connections.
     * * `validation_context.0.trusted_ca_id` - (Optional) Trusted CA certificate ID in the Certificate Manager.
     * * `validation_context.0.trusted_ca_bytes` - (Optional) PEM-encoded trusted CA certificate chain.
     *
     * > **NOTE:** Only one of `validation_context.0.trusted_ca_id` or `validation_context.0.trusted_ca_bytes` should be specified.
     */
    sni?: string;
    validationContext?: outputs.AlbBackendGroupGrpcBackendTlsValidationContext;
}

export interface AlbBackendGroupGrpcBackendTlsValidationContext {
    trustedCaBytes?: string;
    trustedCaId?: string;
}

export interface AlbBackendGroupHttpBackend {
    /**
     * Healthcheck specification that will be used by this backend. Structure is documented below.
     */
    healthcheck?: outputs.AlbBackendGroupHttpBackendHealthcheck;
    /**
     * Enables HTTP2 for upstream requests. If not set, HTTP 1.1 will be used by default.
     */
    http2?: boolean;
    /**
     * Load Balancing Config specification that will be used by this backend. Structure is documented below.
     */
    loadBalancingConfig?: outputs.AlbBackendGroupHttpBackendLoadBalancingConfig;
    /**
     * Name of the backend.
     */
    name: string;
    /**
     * Port for incoming traffic.
     */
    port?: number;
    storageBucket?: string;
    /**
     * References target groups for the backend.
     */
    targetGroupIds?: string[];
    /**
     * Tls specification that will be used by this backend. Structure is documented below.
     */
    tls?: outputs.AlbBackendGroupHttpBackendTls;
    /**
     * Weight of the backend. Traffic will be split between backends of the same BackendGroup according to their weights.
     */
    weight?: number;
}

export interface AlbBackendGroupHttpBackendHealthcheck {
    /**
     * Grpc Healthcheck specification that will be used by this healthcheck. Structure is documented below.
     *
     * > **NOTE:** Only one of `streamHealthcheck` or `httpHealthcheck` or `grpcHealthcheck` should be specified.
     */
    grpcHealthcheck?: outputs.AlbBackendGroupHttpBackendHealthcheckGrpcHealthcheck;
    /**
     * Optional alternative port for health checking.
     */
    healthcheckPort?: number;
    /**
     * Number of consecutive successful health checks required to promote endpoint into the healthy state. 0 means 1. Note that during startup, only a single successful health check is required to mark a host healthy.
     */
    healthyThreshold?: number;
    /**
     * Http Healthcheck specification that will be used by this healthcheck. Structure is documented below.
     */
    httpHealthcheck?: outputs.AlbBackendGroupHttpBackendHealthcheckHttpHealthcheck;
    /**
     * Interval between health checks.
     */
    interval: string;
    /**
     * An optional jitter amount as a percentage of interval. If specified, during every interval value of (interval_ms * intervalJitterPercent / 100) will be added to the wait time.
     */
    intervalJitterPercent?: number;
    /**
     * Stream Healthcheck specification that will be used by this healthcheck. Structure is documented below.
     */
    streamHealthcheck?: outputs.AlbBackendGroupHttpBackendHealthcheckStreamHealthcheck;
    /**
     * Time to wait for a health check response.
     */
    timeout: string;
    /**
     * Number of consecutive failed health checks required to demote endpoint into the unhealthy state. 0 means 1. Note that for HTTP health checks, a single 503 immediately makes endpoint unhealthy.
     */
    unhealthyThreshold?: number;
}

export interface AlbBackendGroupHttpBackendHealthcheckGrpcHealthcheck {
    /**
     * Service name for grpc.health.v1.HealthCheckRequest message.
     */
    serviceName?: string;
}

export interface AlbBackendGroupHttpBackendHealthcheckHttpHealthcheck {
    /**
     * "Host" HTTP header value.
     */
    host?: string;
    /**
     * If set, health checks will use HTTP2.
     */
    http2?: boolean;
    /**
     * HTTP path.
     */
    path: string;
}

export interface AlbBackendGroupHttpBackendHealthcheckStreamHealthcheck {
    /**
     * Data that must be contained in the messages received from targets for a successful health check. If not specified, no messages are expected from targets, and those that are received are not checked.
     */
    receive?: string;
    /**
     * Message sent to targets during TCP data transfer.  If not specified, no data is sent to the target.
     */
    send?: string;
}

export interface AlbBackendGroupHttpBackendLoadBalancingConfig {
    /**
     * Percent of traffic to be sent to the same availability zone. The rest will be equally divided between other zones.
     */
    localityAwareRoutingPercent?: number;
    /**
     * Load balancing mode for the backend. Possible values: "ROUND_ROBIN", "RANDOM", "LEAST_REQUEST", "MAGLEV_HASH".
     */
    mode?: string;
    /**
     * If percentage of healthy hosts in the backend is lower than panic_threshold, traffic will be routed to all backends no matter what the health status is. This helps to avoid healthy backends overloading  when everything is bad. Zero means no panic threshold.
     */
    panicThreshold?: number;
    /**
     * If set, will route requests only to the same availability zone. Balancer won't know about endpoints in other zones.
     */
    strictLocality?: boolean;
}

export interface AlbBackendGroupHttpBackendTls {
    /**
     * [SNI](https://en.wikipedia.org/wiki/Server_Name_Indication) string for TLS connections.
     * * `validation_context.0.trusted_ca_id` - (Optional) Trusted CA certificate ID in the Certificate Manager.
     * * `validation_context.0.trusted_ca_bytes` - (Optional) PEM-encoded trusted CA certificate chain.
     *
     * > **NOTE:** Only one of `validation_context.0.trusted_ca_id` or `validation_context.0.trusted_ca_bytes` should be specified.
     */
    sni?: string;
    validationContext?: outputs.AlbBackendGroupHttpBackendTlsValidationContext;
}

export interface AlbBackendGroupHttpBackendTlsValidationContext {
    trustedCaBytes?: string;
    trustedCaId?: string;
}

export interface AlbBackendGroupSessionAffinity {
    /**
     * Requests received from the same IP are combined into a session. Stream backend groups only support session affinity by client IP address. Structure is documented below.
     */
    connection?: outputs.AlbBackendGroupSessionAffinityConnection;
    /**
     * Requests with the same cookie value and the specified file name are combined into a session. Allowed only for HTTP and gRPC backend groups. Structure is documented below.
     */
    cookie?: outputs.AlbBackendGroupSessionAffinityCookie;
    /**
     * Requests with the same value of the specified HTTP header, such as with user authentication data, are combined into a session. Allowed only for HTTP and gRPC backend groups. Structure is documented below.
     *
     * > **NOTE:** Only one type( `connection` or `cookie` or `header` ) of session affinity should be specified.
     */
    header?: outputs.AlbBackendGroupSessionAffinityHeader;
}

export interface AlbBackendGroupSessionAffinityConnection {
    /**
     * Source IP address to use with affinity.
     */
    sourceIp?: boolean;
}

export interface AlbBackendGroupSessionAffinityCookie {
    /**
     * Name of the HTTP cookie to use with affinity.
     */
    name: string;
    /**
     * TTL for the cookie (if not set, session cookie will be used)
     */
    ttl?: string;
}

export interface AlbBackendGroupSessionAffinityHeader {
    /**
     * The name of the request header that will be used with affinity.
     */
    headerName: string;
}

export interface AlbBackendGroupStreamBackend {
    enableProxyProtocol?: boolean;
    /**
     * Healthcheck specification that will be used by this backend. Structure is documented below.
     */
    healthcheck?: outputs.AlbBackendGroupStreamBackendHealthcheck;
    /**
     * Load Balancing Config specification that will be used by this backend. Structure is documented below.
     */
    loadBalancingConfig?: outputs.AlbBackendGroupStreamBackendLoadBalancingConfig;
    /**
     * Name of the backend.
     */
    name: string;
    /**
     * Port for incoming traffic.
     */
    port?: number;
    /**
     * References target groups for the backend.
     */
    targetGroupIds: string[];
    /**
     * Tls specification that will be used by this backend. Structure is documented below.
     */
    tls?: outputs.AlbBackendGroupStreamBackendTls;
    /**
     * Weight of the backend. Traffic will be split between backends of the same BackendGroup according to their weights.
     */
    weight?: number;
}

export interface AlbBackendGroupStreamBackendHealthcheck {
    /**
     * Grpc Healthcheck specification that will be used by this healthcheck. Structure is documented below.
     *
     * > **NOTE:** Only one of `streamHealthcheck` or `httpHealthcheck` or `grpcHealthcheck` should be specified.
     */
    grpcHealthcheck?: outputs.AlbBackendGroupStreamBackendHealthcheckGrpcHealthcheck;
    /**
     * Optional alternative port for health checking.
     */
    healthcheckPort?: number;
    /**
     * Number of consecutive successful health checks required to promote endpoint into the healthy state. 0 means 1. Note that during startup, only a single successful health check is required to mark a host healthy.
     */
    healthyThreshold?: number;
    /**
     * Http Healthcheck specification that will be used by this healthcheck. Structure is documented below.
     */
    httpHealthcheck?: outputs.AlbBackendGroupStreamBackendHealthcheckHttpHealthcheck;
    /**
     * Interval between health checks.
     */
    interval: string;
    /**
     * An optional jitter amount as a percentage of interval. If specified, during every interval value of (interval_ms * intervalJitterPercent / 100) will be added to the wait time.
     */
    intervalJitterPercent?: number;
    /**
     * Stream Healthcheck specification that will be used by this healthcheck. Structure is documented below.
     */
    streamHealthcheck?: outputs.AlbBackendGroupStreamBackendHealthcheckStreamHealthcheck;
    /**
     * Time to wait for a health check response.
     */
    timeout: string;
    /**
     * Number of consecutive failed health checks required to demote endpoint into the unhealthy state. 0 means 1. Note that for HTTP health checks, a single 503 immediately makes endpoint unhealthy.
     */
    unhealthyThreshold?: number;
}

export interface AlbBackendGroupStreamBackendHealthcheckGrpcHealthcheck {
    /**
     * Service name for grpc.health.v1.HealthCheckRequest message.
     */
    serviceName?: string;
}

export interface AlbBackendGroupStreamBackendHealthcheckHttpHealthcheck {
    /**
     * "Host" HTTP header value.
     */
    host?: string;
    /**
     * If set, health checks will use HTTP2.
     */
    http2?: boolean;
    /**
     * HTTP path.
     */
    path: string;
}

export interface AlbBackendGroupStreamBackendHealthcheckStreamHealthcheck {
    /**
     * Data that must be contained in the messages received from targets for a successful health check. If not specified, no messages are expected from targets, and those that are received are not checked.
     */
    receive?: string;
    /**
     * Message sent to targets during TCP data transfer.  If not specified, no data is sent to the target.
     */
    send?: string;
}

export interface AlbBackendGroupStreamBackendLoadBalancingConfig {
    /**
     * Percent of traffic to be sent to the same availability zone. The rest will be equally divided between other zones.
     */
    localityAwareRoutingPercent?: number;
    /**
     * Load balancing mode for the backend. Possible values: "ROUND_ROBIN", "RANDOM", "LEAST_REQUEST", "MAGLEV_HASH".
     */
    mode?: string;
    /**
     * If percentage of healthy hosts in the backend is lower than panic_threshold, traffic will be routed to all backends no matter what the health status is. This helps to avoid healthy backends overloading  when everything is bad. Zero means no panic threshold.
     */
    panicThreshold?: number;
    /**
     * If set, will route requests only to the same availability zone. Balancer won't know about endpoints in other zones.
     */
    strictLocality?: boolean;
}

export interface AlbBackendGroupStreamBackendTls {
    /**
     * [SNI](https://en.wikipedia.org/wiki/Server_Name_Indication) string for TLS connections.
     * * `validation_context.0.trusted_ca_id` - (Optional) Trusted CA certificate ID in the Certificate Manager.
     * * `validation_context.0.trusted_ca_bytes` - (Optional) PEM-encoded trusted CA certificate chain.
     *
     * > **NOTE:** Only one of `validation_context.0.trusted_ca_id` or `validation_context.0.trusted_ca_bytes` should be specified.
     */
    sni?: string;
    validationContext?: outputs.AlbBackendGroupStreamBackendTlsValidationContext;
}

export interface AlbBackendGroupStreamBackendTlsValidationContext {
    trustedCaBytes?: string;
    trustedCaId?: string;
}

export interface AlbHttpRouterRouteOptions {
    rbac?: outputs.AlbHttpRouterRouteOptionsRbac;
    securityProfileId?: string;
}

export interface AlbHttpRouterRouteOptionsRbac {
    action?: string;
    principals: outputs.AlbHttpRouterRouteOptionsRbacPrincipal[];
}

export interface AlbHttpRouterRouteOptionsRbacPrincipal {
    andPrincipals: outputs.AlbHttpRouterRouteOptionsRbacPrincipalAndPrincipal[];
}

export interface AlbHttpRouterRouteOptionsRbacPrincipalAndPrincipal {
    any?: boolean;
    header?: outputs.AlbHttpRouterRouteOptionsRbacPrincipalAndPrincipalHeader;
    remoteIp?: string;
}

export interface AlbHttpRouterRouteOptionsRbacPrincipalAndPrincipalHeader {
    /**
     * Name of the HTTP Router. Provided by the client when the HTTP Router is created.
     */
    name: string;
    value?: outputs.AlbHttpRouterRouteOptionsRbacPrincipalAndPrincipalHeaderValue;
}

export interface AlbHttpRouterRouteOptionsRbacPrincipalAndPrincipalHeaderValue {
    exact?: string;
    prefix?: string;
    regex?: string;
}

export interface AlbLoadBalancerAllocationPolicy {
    /**
     * Unique set of locations. The structure is documented below.
     */
    locations: outputs.AlbLoadBalancerAllocationPolicyLocation[];
}

export interface AlbLoadBalancerAllocationPolicyLocation {
    /**
     * If set, will disable all L7 instances in the zone for request handling.
     */
    disableTraffic?: boolean;
    /**
     * ID of the subnet that location is located at.
     */
    subnetId: string;
    /**
     * ID of the zone that location is located at.
     */
    zoneId: string;
}

export interface AlbLoadBalancerListener {
    /**
     * Network endpoints (addresses and ports) of the listener. The structure is documented below.
     */
    endpoints?: outputs.AlbLoadBalancerListenerEndpoint[];
    /**
     * HTTP listener resource. The structure is documented below.
     */
    http?: outputs.AlbLoadBalancerListenerHttp;
    /**
     * name of the listener.
     */
    name: string;
    /**
     * Stream listener resource. The structure is documented below.
     */
    stream?: outputs.AlbLoadBalancerListenerStream;
    /**
     * TLS listener resource. The structure is documented below.
     *
     * > **NOTE:** Exactly one listener type: `http` or `tls` or `stream` should be specified.
     */
    tls?: outputs.AlbLoadBalancerListenerTls;
}

export interface AlbLoadBalancerListenerEndpoint {
    /**
     * One or more addresses to listen on. The structure is documented below.
     */
    addresses: outputs.AlbLoadBalancerListenerEndpointAddress[];
    /**
     * One or more ports to listen on.
     */
    ports: number[];
}

export interface AlbLoadBalancerListenerEndpointAddress {
    /**
     * External IPv4 address. The structure is documented below.
     */
    externalIpv4Address?: outputs.AlbLoadBalancerListenerEndpointAddressExternalIpv4Address;
    /**
     * External IPv6 address. The structure is documented below.
     *
     * > **NOTE:** Exactly one type of addresses `externalIpv4Address` or `internalIpv4Address` or `externalIpv6Address`
     * should be specified.
     */
    externalIpv6Address?: outputs.AlbLoadBalancerListenerEndpointAddressExternalIpv6Address;
    /**
     * Internal IPv4 address. The structure is documented below.
     */
    internalIpv4Address?: outputs.AlbLoadBalancerListenerEndpointAddressInternalIpv4Address;
}

export interface AlbLoadBalancerListenerEndpointAddressExternalIpv4Address {
    /**
     * Provided by the client or computed automatically.
     */
    address: string;
}

export interface AlbLoadBalancerListenerEndpointAddressExternalIpv6Address {
    /**
     * Provided by the client or computed automatically.
     */
    address: string;
}

export interface AlbLoadBalancerListenerEndpointAddressInternalIpv4Address {
    /**
     * Provided by the client or computed automatically.
     */
    address: string;
    /**
     * Provided by the client or computed automatically.
     */
    subnetId: string;
}

export interface AlbLoadBalancerListenerHttp {
    /**
     * HTTP handler that sets plaintext HTTP router. The structure is documented below.
     */
    handler?: outputs.AlbLoadBalancerListenerHttpHandler;
    /**
     * Shortcut for adding http > https redirects. The structure is documented below.
     *
     * > **NOTE:** Only one type of fields `handler` or `redirects` should be specified.
     */
    redirects?: outputs.AlbLoadBalancerListenerHttpRedirects;
}

export interface AlbLoadBalancerListenerHttpHandler {
    /**
     * If set, will enable only HTTP1 protocol with HTTP1.0 support.
     *
     * > **NOTE:** Only one type of protocol settings `http2Options` or `allowHttp10` should be specified.
     */
    allowHttp10?: boolean;
    /**
     * If set, will enable HTTP2 protocol for the handler. The structure is documented below.
     */
    http2Options?: outputs.AlbLoadBalancerListenerHttpHandlerHttp2Options;
    /**
     * HTTP router id.
     */
    httpRouterId?: string;
    /**
     * When unset, will preserve the incoming x-request-id header, otherwise would rewrite it with a new value.
     */
    rewriteRequestId?: boolean;
}

export interface AlbLoadBalancerListenerHttpHandlerHttp2Options {
    /**
     * Maximum number of concurrent streams.
     */
    maxConcurrentStreams?: number;
}

export interface AlbLoadBalancerListenerHttpRedirects {
    /**
     * If set redirects all unencrypted HTTP requests to the same URI with scheme changed to `https`.
     */
    httpToHttps?: boolean;
}

export interface AlbLoadBalancerListenerStream {
    /**
     * Stream handler that sets plaintext Stream backend group. The structure is documented below.
     */
    handler?: outputs.AlbLoadBalancerListenerStreamHandler;
}

export interface AlbLoadBalancerListenerStreamHandler {
    /**
     * Backend group id.
     */
    backendGroupId?: string;
}

export interface AlbLoadBalancerListenerTls {
    /**
     * TLS handler resource. The structure is documented below.
     */
    defaultHandler: outputs.AlbLoadBalancerListenerTlsDefaultHandler;
    /**
     * SNI match resource. The structure is documented below.
     */
    sniHandlers?: outputs.AlbLoadBalancerListenerTlsSniHandler[];
}

export interface AlbLoadBalancerListenerTlsDefaultHandler {
    /**
     * Certificate IDs in the Certificate Manager. Multiple TLS certificates can be associated
     * with the same context to allow both RSA and ECDSA certificates. Only the first certificate of each type will be used.
     *
     * > **NOTE:** Exactly one handler type `httpHandler` or `streamHandler` should be specified.
     */
    certificateIds: string[];
    /**
     * HTTP handler resource. The structure is documented below.
     */
    httpHandler?: outputs.AlbLoadBalancerListenerTlsDefaultHandlerHttpHandler;
    /**
     * Stream handler resource. The structure is documented below.
     */
    streamHandler?: outputs.AlbLoadBalancerListenerTlsDefaultHandlerStreamHandler;
}

export interface AlbLoadBalancerListenerTlsDefaultHandlerHttpHandler {
    /**
     * If set, will enable only HTTP1 protocol with HTTP1.0 support.
     *
     * > **NOTE:** Only one type of protocol settings `http2Options` or `allowHttp10` should be specified.
     */
    allowHttp10?: boolean;
    /**
     * If set, will enable HTTP2 protocol for the handler. The structure is documented below.
     */
    http2Options?: outputs.AlbLoadBalancerListenerTlsDefaultHandlerHttpHandlerHttp2Options;
    /**
     * HTTP router id.
     */
    httpRouterId?: string;
    /**
     * When unset, will preserve the incoming x-request-id header, otherwise would rewrite it with a new value.
     */
    rewriteRequestId?: boolean;
}

export interface AlbLoadBalancerListenerTlsDefaultHandlerHttpHandlerHttp2Options {
    /**
     * Maximum number of concurrent streams.
     */
    maxConcurrentStreams?: number;
}

export interface AlbLoadBalancerListenerTlsDefaultHandlerStreamHandler {
    /**
     * Backend group id.
     */
    backendGroupId?: string;
}

export interface AlbLoadBalancerListenerTlsSniHandler {
    /**
     * TLS handler resource. The structure is documented below.
     */
    handler: outputs.AlbLoadBalancerListenerTlsSniHandlerHandler;
    /**
     * name of SNI match.
     */
    name: string;
    /**
     * A set of server names.
     */
    serverNames: string[];
}

export interface AlbLoadBalancerListenerTlsSniHandlerHandler {
    /**
     * Certificate IDs in the Certificate Manager. Multiple TLS certificates can be associated
     * with the same context to allow both RSA and ECDSA certificates. Only the first certificate of each type will be used.
     *
     * > **NOTE:** Exactly one handler type `httpHandler` or `streamHandler` should be specified.
     */
    certificateIds: string[];
    /**
     * HTTP handler resource. The structure is documented below.
     */
    httpHandler?: outputs.AlbLoadBalancerListenerTlsSniHandlerHandlerHttpHandler;
    /**
     * Stream handler resource. The structure is documented below.
     */
    streamHandler?: outputs.AlbLoadBalancerListenerTlsSniHandlerHandlerStreamHandler;
}

export interface AlbLoadBalancerListenerTlsSniHandlerHandlerHttpHandler {
    /**
     * If set, will enable only HTTP1 protocol with HTTP1.0 support.
     *
     * > **NOTE:** Only one type of protocol settings `http2Options` or `allowHttp10` should be specified.
     */
    allowHttp10?: boolean;
    /**
     * If set, will enable HTTP2 protocol for the handler. The structure is documented below.
     */
    http2Options?: outputs.AlbLoadBalancerListenerTlsSniHandlerHandlerHttpHandlerHttp2Options;
    /**
     * HTTP router id.
     */
    httpRouterId?: string;
    /**
     * When unset, will preserve the incoming x-request-id header, otherwise would rewrite it with a new value.
     */
    rewriteRequestId?: boolean;
}

export interface AlbLoadBalancerListenerTlsSniHandlerHandlerHttpHandlerHttp2Options {
    /**
     * Maximum number of concurrent streams.
     */
    maxConcurrentStreams?: number;
}

export interface AlbLoadBalancerListenerTlsSniHandlerHandlerStreamHandler {
    /**
     * Backend group id.
     */
    backendGroupId?: string;
}

export interface AlbLoadBalancerLogOptions {
    /**
     * Set to true to disable Cloud Logging for the balancer
     */
    disable?: boolean;
    /**
     * List of rules to discard a fraction of logs. The structure is documented below.
     */
    discardRules?: outputs.AlbLoadBalancerLogOptionsDiscardRule[];
    /**
     * Cloud Logging group ID to send logs to. Leave empty to use the balancer folder default log group.
     */
    logGroupId?: string;
}

export interface AlbLoadBalancerLogOptionsDiscardRule {
    discardPercent?: number;
    /**
     * list of grpc codes by name, e.g, _["NOT_FOUND", "RESOURCE_EXHAUSTED"]_
     */
    grpcCodes?: string[];
    /**
     * list of http code intervals _1XX_-_5XX_ or _ALL_
     */
    httpCodeIntervals?: string[];
    /**
     * list of http codes _100_-_599_
     */
    httpCodes?: number[];
}

export interface AlbTargetGroupTarget {
    /**
     * IP address of the target.
     */
    ipAddress: string;
    privateIpv4Address?: boolean;
    /**
     * ID of the subnet that targets are connected to.
     * All targets in the target group must be connected to the same subnet within a single availability zone.
     */
    subnetId?: string;
}

export interface AlbVirtualHostModifyRequestHeader {
    /**
     * Append string to the header value.
     */
    append?: string;
    /**
     * name of the header to modify.
     */
    name: string;
    /**
     * If set, remove the header.
     *
     * > **NOTE:** Only one type of actions `append` or `replace` or `remove` should be specified.
     */
    remove?: boolean;
    /**
     * New value for a header. Header values support the following 
     * [formatters](https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#custom-request-response-headers).
     */
    replace?: string;
}

export interface AlbVirtualHostModifyResponseHeader {
    /**
     * Append string to the header value.
     */
    append?: string;
    /**
     * name of the header to modify.
     */
    name: string;
    /**
     * If set, remove the header.
     *
     * > **NOTE:** Only one type of actions `append` or `replace` or `remove` should be specified.
     */
    remove?: boolean;
    /**
     * New value for a header. Header values support the following 
     * [formatters](https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#custom-request-response-headers).
     */
    replace?: string;
}

export interface AlbVirtualHostRoute {
    /**
     * GRPC route resource. The structure is documented below.
     *
     * > **NOTE:** Exactly one type of routes `httpRoute` or `grpcRoute` should be specified.
     */
    grpcRoute?: outputs.AlbVirtualHostRouteGrpcRoute;
    /**
     * HTTP route resource. The structure is documented below.
     */
    httpRoute?: outputs.AlbVirtualHostRouteHttpRoute;
    /**
     * name of the route.
     */
    name?: string;
    /**
     * Route options for the virtual host. The structure is documented below.
     */
    routeOptions?: outputs.AlbVirtualHostRouteRouteOptions;
}

export interface AlbVirtualHostRouteGrpcRoute {
    /**
     * Checks "/" prefix by default. The structure is documented below.
     */
    grpcMatches?: outputs.AlbVirtualHostRouteGrpcRouteGrpcMatch[];
    /**
     * GRPC route action resource. The structure is documented below.
     */
    grpcRouteAction?: outputs.AlbVirtualHostRouteGrpcRouteGrpcRouteAction;
    /**
     * GRPC status response action resource. The structure is documented below.
     *
     * > **NOTE:** Exactly one type of actions `grpcRouteAction` or `grpcStatusResponseAction` should be specified.
     */
    grpcStatusResponseAction?: outputs.AlbVirtualHostRouteGrpcRouteGrpcStatusResponseAction;
}

export interface AlbVirtualHostRouteGrpcRouteGrpcMatch {
    /**
     * If not set, all services/methods are assumed. The structure is documented below.
     */
    fqmn?: outputs.AlbVirtualHostRouteGrpcRouteGrpcMatchFqmn;
}

export interface AlbVirtualHostRouteGrpcRouteGrpcMatchFqmn {
    /**
     * Match exactly.
     */
    exact?: string;
    /**
     * Match prefix.
     */
    prefix?: string;
    /**
     * Match regex.
     *
     * > **NOTE:** Exactly one type of string matches `exact`, `prefix` or `regex` should be
     * specified.
     */
    regex?: string;
}

export interface AlbVirtualHostRouteGrpcRouteGrpcRouteAction {
    /**
     * If set, will automatically rewrite host.
     *
     * > **NOTE:** Only one type of host rewrite specifiers `hostRewrite` or `autoHostRewrite` should be
     * specified.
     */
    autoHostRewrite?: boolean;
    /**
     * Backend group to route requests.
     */
    backendGroupId: string;
    /**
     * Host rewrite specifier.
     */
    hostRewrite?: string;
    /**
     * Specifies the idle timeout (time without any data transfer for the active request) for the
     * route. It is useful for streaming scenarios - one should set idleTimeout to something meaningful and maxTimeout
     * to the maximum time the stream is allowed to be alive. If not specified, there is no
     * per-route idle timeout.
     */
    idleTimeout?: string;
    /**
     * Lower timeout may be specified by the client (using grpc-timeout header). If not set, default is 
     * 60 seconds.
     */
    maxTimeout?: string;
}

export interface AlbVirtualHostRouteGrpcRouteGrpcStatusResponseAction {
    /**
     * The status of the response. Supported values are: ok, invalid_argumet, not_found, 
     * permission_denied, unauthenticated, unimplemented, internal, unavailable.
     */
    status?: string;
}

export interface AlbVirtualHostRouteHttpRoute {
    /**
     * Direct response action resource. The structure is documented below.
     *
     * > **NOTE:** Exactly one type of actions `httpRouteAction` or `redirectAction` or `directResponseAction` should be
     * specified.
     */
    directResponseAction?: outputs.AlbVirtualHostRouteHttpRouteDirectResponseAction;
    /**
     * Checks "/" prefix by default. The structure is documented below.
     */
    httpMatches?: outputs.AlbVirtualHostRouteHttpRouteHttpMatch[];
    /**
     * HTTP route action resource. The structure is documented below.
     */
    httpRouteAction?: outputs.AlbVirtualHostRouteHttpRouteHttpRouteAction;
    /**
     * Redirect action resource. The structure is documented below.
     */
    redirectAction?: outputs.AlbVirtualHostRouteHttpRouteRedirectAction;
}

export interface AlbVirtualHostRouteHttpRouteDirectResponseAction {
    /**
     * Response body text.
     */
    body?: string;
    /**
     * HTTP response status. Should be between 100 and 599.
     */
    status?: number;
}

export interface AlbVirtualHostRouteHttpRouteHttpMatch {
    /**
     * List of methods(strings).
     */
    httpMethods?: string[];
    /**
     * If not set, '/' is assumed. The structure is documented below.
     */
    path?: outputs.AlbVirtualHostRouteHttpRouteHttpMatchPath;
}

export interface AlbVirtualHostRouteHttpRouteHttpMatchPath {
    /**
     * Match exactly.
     */
    exact?: string;
    /**
     * Match prefix.
     */
    prefix?: string;
    /**
     * Match regex.
     *
     * > **NOTE:** Exactly one type of string matches `exact`, `prefix` or `regex` should be
     * specified.
     */
    regex?: string;
}

export interface AlbVirtualHostRouteHttpRouteHttpRouteAction {
    /**
     * If set, will automatically rewrite host.
     */
    autoHostRewrite?: boolean;
    /**
     * Backend group to route requests.
     */
    backendGroupId: string;
    /**
     * Host rewrite specifier.
     */
    hostRewrite?: string;
    /**
     * Specifies the idle timeout (time without any data transfer for the active request) for the 
     * route. It is useful for streaming scenarios (i.e. long-polling, server-sent events) - one should set idleTimeout to
     * something meaningful and timeout to the maximum time the stream is allowed to be alive. If not specified, there is no
     * per-route idle timeout.
     */
    idleTimeout?: string;
    /**
     * If not empty, matched path prefix will be replaced by this value.
     */
    prefixRewrite?: string;
    /**
     * Specifies the request timeout (overall time request processing is allowed to take) for the 
     * route. If not set, default is 60 seconds.
     */
    timeout?: string;
    /**
     * List of upgrade types. Only specified upgrade types will be allowed. For example, 
     * "websocket".
     *
     * > **NOTE:** Only one type of host rewrite specifiers `hostRewrite` or `autoHostRewrite` should be
     * specified.
     */
    upgradeTypes?: string[];
}

export interface AlbVirtualHostRouteHttpRouteRedirectAction {
    removeQuery?: boolean;
    /**
     * Replaces hostname.
     */
    replaceHost?: string;
    /**
     * Replace path.
     */
    replacePath?: string;
    /**
     * Replaces port.
     */
    replacePort?: number;
    /**
     * Replace only matched prefix. Example:<br/> match:{ prefix_match: "/some" } <br/> 
     * redirect: { replace_prefix: "/other" } <br/> will redirect "/something" to "/otherthing".
     *
     * * `remove query` - (Optional) If set, remove query part.
     */
    replacePrefix?: string;
    /**
     * Replaces scheme. If the original scheme is `http` or `https`, will also remove the 
     * 80 or 443 port, if present.
     */
    replaceScheme?: string;
    /**
     * The HTTP status code to use in the redirect response. Supported values are: 
     * moved_permanently, found, see_other, temporary_redirect, permanent_redirect.
     *
     * > **NOTE:** Only one type of paths `replacePath` or `replacePrefix` should be specified.
     */
    responseCode?: string;
}

export interface AlbVirtualHostRouteOptions {
    /**
     * RBAC configuration.
     */
    rbac?: outputs.AlbVirtualHostRouteOptionsRbac;
    /**
     * SWS profile ID.
     */
    securityProfileId?: string;
}

export interface AlbVirtualHostRouteOptionsRbac {
    action?: string;
    principals: outputs.AlbVirtualHostRouteOptionsRbacPrincipal[];
}

export interface AlbVirtualHostRouteOptionsRbacPrincipal {
    andPrincipals: outputs.AlbVirtualHostRouteOptionsRbacPrincipalAndPrincipal[];
}

export interface AlbVirtualHostRouteOptionsRbacPrincipalAndPrincipal {
    any?: boolean;
    header?: outputs.AlbVirtualHostRouteOptionsRbacPrincipalAndPrincipalHeader;
    remoteIp?: string;
}

export interface AlbVirtualHostRouteOptionsRbacPrincipalAndPrincipalHeader {
    /**
     * name of the header to modify.
     */
    name: string;
    value?: outputs.AlbVirtualHostRouteOptionsRbacPrincipalAndPrincipalHeaderValue;
}

export interface AlbVirtualHostRouteOptionsRbacPrincipalAndPrincipalHeaderValue {
    /**
     * Match exactly.
     */
    exact?: string;
    /**
     * Match prefix.
     */
    prefix?: string;
    /**
     * Match regex.
     *
     * > **NOTE:** Exactly one type of string matches `exact`, `prefix` or `regex` should be
     * specified.
     */
    regex?: string;
}

export interface AlbVirtualHostRouteRouteOptions {
    /**
     * RBAC configuration.
     */
    rbac?: outputs.AlbVirtualHostRouteRouteOptionsRbac;
    /**
     * SWS profile ID.
     */
    securityProfileId?: string;
}

export interface AlbVirtualHostRouteRouteOptionsRbac {
    action?: string;
    principals: outputs.AlbVirtualHostRouteRouteOptionsRbacPrincipal[];
}

export interface AlbVirtualHostRouteRouteOptionsRbacPrincipal {
    andPrincipals: outputs.AlbVirtualHostRouteRouteOptionsRbacPrincipalAndPrincipal[];
}

export interface AlbVirtualHostRouteRouteOptionsRbacPrincipalAndPrincipal {
    any?: boolean;
    header?: outputs.AlbVirtualHostRouteRouteOptionsRbacPrincipalAndPrincipalHeader;
    remoteIp?: string;
}

export interface AlbVirtualHostRouteRouteOptionsRbacPrincipalAndPrincipalHeader {
    /**
     * name of the header to modify.
     */
    name: string;
    value?: outputs.AlbVirtualHostRouteRouteOptionsRbacPrincipalAndPrincipalHeaderValue;
}

export interface AlbVirtualHostRouteRouteOptionsRbacPrincipalAndPrincipalHeaderValue {
    /**
     * Match exactly.
     */
    exact?: string;
    /**
     * Match prefix.
     */
    prefix?: string;
    /**
     * Match regex.
     *
     * > **NOTE:** Exactly one type of string matches `exact`, `prefix` or `regex` should be
     * specified.
     */
    regex?: string;
}

export interface ApiGatewayCanary {
    /**
     * A set of values for variables in gateway specification.
     */
    variables?: {[key: string]: string};
    weight?: number;
}

export interface ApiGatewayConnectivity {
    networkId: string;
}

export interface ApiGatewayCustomDomain {
    certificateId: string;
    domainId: string;
    fqdn: string;
}

export interface ApiGatewayLogOptions {
    /**
     * Is logging from API Gateway disabled
     */
    disabled?: boolean;
    /**
     * Folder ID for the Yandex Cloud API Gateway. If it is not provided, the default provider folder is used.
     */
    folderId?: string;
    /**
     * Log entries are written to specified log group
     */
    logGroupId?: string;
    /**
     * Minimum log entry level
     */
    minLevel?: string;
}

export interface AuditTrailsTrailDataStreamDestination {
    /**
     * ID of the [YDB](https://cloud.yandex.ru/ru/docs/ydb/concepts/resources) hosting the destination data stream.
     */
    databaseId: string;
    /**
     * Name of the [YDS stream](https://cloud.yandex.ru/ru/docs/data-streams/concepts/glossary#stream-concepts) belonging to the specified YDB.
     */
    streamName: string;
}

export interface AuditTrailsTrailFilter {
    /**
     * Structure describing filtering process for the service-specific data plane events
     */
    eventFilters?: outputs.AuditTrailsTrailFilterEventFilter[];
    /**
     * Structure describing filtering process based on cloud resources for the described event set. Structurally equal to the `filter.path_filter`
     */
    pathFilter?: outputs.AuditTrailsTrailFilterPathFilter;
}

export interface AuditTrailsTrailFilterEventFilter {
    /**
     * List of structures describing categories of gathered data plane events
     */
    categories: outputs.AuditTrailsTrailFilterEventFilterCategory[];
    /**
     * Structure describing filtering process based on cloud resources for the described event set. Structurally equal to the `filter.path_filter`
     */
    pathFilter: outputs.AuditTrailsTrailFilterEventFilterPathFilter;
    /**
     * ID of the service which events will be gathered
     */
    service: string;
}

export interface AuditTrailsTrailFilterEventFilterCategory {
    /**
     * Type of the event by its relation to the cloud resource model. Possible values: `CONTROL_PLANE`/`DATA_PLANE`
     */
    plane: string;
    /**
     * Type of the event by its operation effect on the resource. Possible values: `READ`/`WRITE`
     */
    type: string;
}

export interface AuditTrailsTrailFilterEventFilterPathFilter {
    /**
     * Structure describing that events will be gathered from all cloud resources that belong to the parent resource. Mutually exclusive with `someFilter`.
     */
    anyFilter?: outputs.AuditTrailsTrailFilterEventFilterPathFilterAnyFilter;
    /**
     * Structure describing that events will be gathered from some of the cloud resources that belong to the parent resource. Mutually exclusive with `anyFilter`.
     */
    someFilter?: outputs.AuditTrailsTrailFilterEventFilterPathFilterSomeFilter;
}

export interface AuditTrailsTrailFilterEventFilterPathFilterAnyFilter {
    /**
     * ID of the child resource.
     */
    resourceId: string;
    /**
     * Resource type of the child resource.
     */
    resourceType: string;
}

export interface AuditTrailsTrailFilterEventFilterPathFilterSomeFilter {
    /**
     * List of child resources from which events will be gathered
     */
    anyFilters: outputs.AuditTrailsTrailFilterEventFilterPathFilterSomeFilterAnyFilter[];
    /**
     * ID of the child resource.
     */
    resourceId: string;
    /**
     * Resource type of the child resource.
     */
    resourceType: string;
}

export interface AuditTrailsTrailFilterEventFilterPathFilterSomeFilterAnyFilter {
    /**
     * ID of the child resource.
     */
    resourceId: string;
    /**
     * Resource type of the child resource.
     */
    resourceType: string;
}

export interface AuditTrailsTrailFilterPathFilter {
    /**
     * Structure describing that events will be gathered from all cloud resources that belong to the parent resource. Mutually exclusive with `someFilter`.
     */
    anyFilter?: outputs.AuditTrailsTrailFilterPathFilterAnyFilter;
    /**
     * Structure describing that events will be gathered from some of the cloud resources that belong to the parent resource. Mutually exclusive with `anyFilter`.
     */
    someFilter?: outputs.AuditTrailsTrailFilterPathFilterSomeFilter;
}

export interface AuditTrailsTrailFilterPathFilterAnyFilter {
    /**
     * ID of the child resource.
     */
    resourceId: string;
    /**
     * Resource type of the child resource.
     */
    resourceType: string;
}

export interface AuditTrailsTrailFilterPathFilterSomeFilter {
    /**
     * List of child resources from which events will be gathered
     */
    anyFilters: outputs.AuditTrailsTrailFilterPathFilterSomeFilterAnyFilter[];
    /**
     * ID of the child resource.
     */
    resourceId: string;
    /**
     * Resource type of the child resource.
     */
    resourceType: string;
}

export interface AuditTrailsTrailFilterPathFilterSomeFilterAnyFilter {
    /**
     * ID of the child resource.
     */
    resourceId: string;
    /**
     * Resource type of the child resource.
     */
    resourceType: string;
}

export interface AuditTrailsTrailLoggingDestination {
    /**
     * ID of the destination [Cloud Logging Group](https://cloud.yandex.ru/ru/docs/logging/concepts/log-group)
     */
    logGroupId: string;
}

export interface AuditTrailsTrailStorageDestination {
    /**
     * Name of the [destination bucket](https://cloud.yandex.ru/en/docs/storage/concepts/bucket)
     */
    bucketName: string;
    /**
     * Additional prefix of the uploaded objects. If not specified, objects will be uploaded with prefix equal to `trailId`
     */
    objectPrefix?: string;
}

export interface BackupPolicyReattempts {
    /**
     * — enables or disables scheduling.
     */
    enabled?: boolean;
    /**
     * — Retry interval. See `intervalType` for available values
     */
    interval?: string;
    /**
     * — Maximum number of attempts before throwing an error
     */
    maxAttempts?: number;
}

export interface BackupPolicyRetention {
    /**
     * — Defines whether retention rule applies after creating backup or before.
     */
    afterBackup?: boolean;
    rules?: outputs.BackupPolicyRetentionRule[];
}

export interface BackupPolicyRetentionRule {
    /**
     * — Deletes backups that older than `maxAge`. Exactly one of `maxCount` or `maxAge` should be set.
     */
    maxAge?: string;
    /**
     * — Deletes backups if it's count exceeds `maxCount`. Exactly one of `maxCount` or `maxAge` should be set.
     */
    maxCount?: number;
    repeatPeriods?: string[];
}

export interface BackupPolicyScheduling {
    /**
     * A list of schedules with backup sets that compose the whole scheme.
     */
    backupSets?: outputs.BackupPolicySchedulingBackupSet[];
    /**
     * — enables or disables scheduling.
     */
    enabled?: boolean;
    /**
     * — Perform backup by interval, since last backup of the host. Maximum value is: 9999 days.
     * See `intervalType` for available values. Exactly on of options should be set: `executeByInterval` or `executeByTime`.
     *
     * @deprecated The 'execute_by_interval' field has been deprecated. Please use 'backup_sets' instead.
     */
    executeByInterval?: number;
    /**
     * — Perform backup periodically at specific time. Exactly on of options should be set: `executeByInterval` or `executeByTime`.
     *
     * @deprecated The 'execute_by_time' field has been deprecated. Please use 'backup_sets' instead.
     */
    executeByTimes?: outputs.BackupPolicySchedulingExecuteByTime[];
    /**
     * — Maximum number of backup processes allowed to run in parallel. 0 for unlimited.
     */
    maxParallelBackups?: number;
    /**
     * — Configuration of the random delay between the execution of parallel tasks.
     * See `intervalType` for available values.
     */
    randomMaxDelay?: string;
    /**
     * — Scheme of the backups.
     * Available values are: `"ALWAYS_INCREMENTAL"`, `"ALWAYS_FULL"`, `"WEEKLY_FULL_DAILY_INCREMENTAL"`, `'WEEKLY_INCREMENTAL"`.
     */
    scheme?: string;
    /**
     * — A day of week to start weekly backups.
     * See `dayType` for available values.
     */
    weeklyBackupDay?: string;
}

export interface BackupPolicySchedulingBackupSet {
    /**
     * — Perform backup by interval, since last backup of the host. Maximum value is: 9999 days.
     * See `intervalType` for available values. Exactly on of options should be set: `executeByInterval` or `executeByTime`.
     */
    executeByInterval?: number;
    /**
     * — Perform backup periodically at specific time. Exactly on of options should be set: `executeByInterval` or `executeByTime`.
     */
    executeByTimes?: outputs.BackupPolicySchedulingBackupSetExecuteByTime[];
    /**
     * — Type of the scheduling. Available values are: `"HOURLY"`, `"DAILY"`, `"WEEKLY"`, `"MONTHLY"`.
     */
    type?: string;
}

export interface BackupPolicySchedulingBackupSetExecuteByTime {
    /**
     * — If true, schedule will be applied on the last day of month.
     * See `dayType` for available values.
     */
    includeLastDayOfMonth?: boolean;
    /**
     * — List of days when schedule applies. Used in `"MONTHLY"` type.
     */
    monthdays?: number[];
    months?: number[];
    /**
     * — List of time in format `"HH:MM" (24-hours format)`, when the schedule applies.
     */
    repeatAts?: string[];
    /**
     * — Frequency of backup repetition. See `intervalType` for available values.
     */
    repeatEvery?: string;
    /**
     * — Type of the scheduling. Available values are: `"HOURLY"`, `"DAILY"`, `"WEEKLY"`, `"MONTHLY"`.
     */
    type: string;
    /**
     * — List of weekdays when the backup will be applied. Used in `"WEEKLY"` type.
     */
    weekdays?: string[];
}

export interface BackupPolicySchedulingExecuteByTime {
    /**
     * — If true, schedule will be applied on the last day of month.
     * See `dayType` for available values.
     */
    includeLastDayOfMonth?: boolean;
    /**
     * — List of days when schedule applies. Used in `"MONTHLY"` type.
     */
    monthdays?: number[];
    months?: number[];
    /**
     * — List of time in format `"HH:MM" (24-hours format)`, when the schedule applies.
     */
    repeatAts?: string[];
    /**
     * — Frequency of backup repetition. See `intervalType` for available values.
     */
    repeatEvery?: string;
    /**
     * — Type of the scheduling. Available values are: `"HOURLY"`, `"DAILY"`, `"WEEKLY"`, `"MONTHLY"`.
     */
    type: string;
    /**
     * — List of weekdays when the backup will be applied. Used in `"WEEKLY"` type.
     */
    weekdays?: string[];
}

export interface BackupPolicyVmSnapshotReattempts {
    /**
     * — enables or disables scheduling.
     */
    enabled?: boolean;
    /**
     * — Retry interval. See `intervalType` for available values
     */
    interval?: string;
    /**
     * — Maximum number of attempts before throwing an error
     */
    maxAttempts?: number;
}

export interface CdnOriginGroupOrigin {
    backup?: boolean;
    enabled?: boolean;
    originGroupId: number;
    source: string;
}

export interface CdnResourceOptions {
    /**
     * HTTP methods for your CDN content. By default the following methods are allowed: GET, HEAD, POST, PUT, PATCH, DELETE, OPTIONS. In case some methods are not allowed to the user, they will get the 405 (Method Not Allowed) response. If the method is not supported, the user gets the 501 (Not Implemented) response.
     */
    allowedHttpMethods: string[];
    /**
     * set up a cache period for the end-users browser. Content will be cached due to origin settings. If there are no cache settings on your origin, the content will not be cached. The list of HTTP response codes that can be cached in browsers: 200, 201, 204, 206, 301, 302, 303, 304, 307, 308. Other response codes will not be cached. The default value is 4 days.
     */
    browserCacheSettings: number;
    /**
     * list HTTP headers that must be included in responses to clients.
     */
    cacheHttpHeaders: string[];
    /**
     * parameter that lets browsers get access to selected resources from a domain different to a domain from which the request is received.
     */
    cors: string[];
    /**
     * custom value for the Host header. Your server must be able to process requests with the chosen header.
     */
    customHostHeader: string;
    /**
     * wildcard additional CNAME. If a resource has a wildcard additional CNAME, you can use your own certificate for content delivery via HTTPS. Read-only.
     */
    customServerName: string;
    /**
     * setup a cache status.
     */
    disableCache: boolean;
    /**
     * disabling proxy force ranges.
     */
    disableProxyForceRanges: boolean;
    /**
     * content will be cached according to origin cache settings. The value applies for a response with codes 200, 201, 204, 206, 301, 302, 303, 304, 307, 308 if an origin server does not have caching HTTP headers. Responses with other codes will not be cached.
     */
    edgeCacheSettings: number;
    /**
     * enable access limiting by IP addresses, option available only with setting secure_key.
     *
     * * `ip_address_acl.excepted_values` - the list of specified IP addresses to be allowed or denied depending on acl policy type.
     *
     * * `ip_address_acl.policy_type` - the policy type for ipAddressAcl option, one of "allow" or "deny" values.
     */
    enableIpUrlSigning: boolean;
    /**
     * option helps you to reduce the bandwidth between origin and CDN servers. Also, content delivery speed becomes higher because of reducing the time for compressing files in a CDN.
     */
    fetchedCompressed: boolean;
    /**
     * choose the Forward Host header option if is important to send in the request to the Origin the same Host header as was sent in the request to CDN server.
     */
    forwardHostHeader: boolean;
    /**
     * GZip compression at CDN servers reduces file size by 70% and can be as high as 90%.
     */
    gzipOn: boolean;
    /**
     * set for ignoring cookie.
     */
    ignoreCookie: boolean;
    /**
     * files with different query parameters are cached as objects with the same key regardless of the parameter value. selected by default.
     */
    ignoreQueryParams: boolean;
    ipAddressAcl: outputs.CdnResourceOptionsIpAddressAcl;
    /**
     * allows caching for GET, HEAD and POST requests.
     */
    proxyCacheMethodsSet: boolean;
    /**
     * files with the specified query parameters are cached as objects with the same key, files with other parameters are cached as objects with different keys.
     */
    queryParamsBlacklists: string[];
    /**
     * files with the specified query parameters are cached as objects with different keys, files with other parameters are cached as objects with the same key.
     */
    queryParamsWhitelists: string[];
    /**
     * set up a redirect from HTTP to HTTPS.
     */
    redirectHttpToHttps: boolean;
    /**
     * set up a redirect from HTTPS to HTTP.
     */
    redirectHttpsToHttp: boolean;
    /**
     * set secure key for url encoding to protect contect and limit access by IP addresses and time limits.
     */
    secureKey: string;
    /**
     * files larger than 10 MB will be requested and cached in parts (no larger than 10 MB each part). It reduces time to first byte. The origin must support HTTP Range requests.
     */
    slice: boolean;
    /**
     * set up custom headers that CDN servers will send in requests to origins.
     */
    staticRequestHeaders: {[key: string]: string};
    /**
     * set up custom headers that CDN servers will send in response to clients.
     */
    staticResponseHeaders: {[key: string]: string};
}

export interface CdnResourceOptionsIpAddressAcl {
    exceptedValues: string[];
    policyType: string;
}

export interface CdnResourceSslCertificate {
    certificateManagerId?: string;
    status: string;
    type: string;
}

export interface CmCertificateChallenge {
    /**
     * Time the challenge was created.
     */
    createdAt: string;
    /**
     * DNS record name (only for DNS challenge).
     */
    dnsName: string;
    /**
     * DNS record type: `"TXT"` or `"CNAME"` (only for DNS challenge).
     */
    dnsType: string;
    /**
     * DNS record value (only for DNS challenge).
     */
    dnsValue: string;
    /**
     * Validated domain.
     */
    domain: string;
    /**
     * The content that should be made accessible with the given `httpUrl` (only for HTTP challenge).
     */
    httpContent: string;
    /**
     * URL where the challenge content httpContent should be placed (only for HTTP challenge).
     */
    httpUrl: string;
    /**
     * Current status message.
     */
    message: string;
    /**
     * Challenge type `"DNS"` or `"HTTP"`.
     */
    type: string;
    /**
     * Last time the challenge was updated.
     */
    updatedAt: string;
}

export interface CmCertificateManaged {
    /**
     * . Expected number of challenge count needed to validate certificate. 
     * Resource creation will fail if the specified value does not match the actual number of challenges received from issue provider.
     * This argument is helpful for safe automatic resource creation for passing challenges for multi-domain certificates.
     *
     * > **NOTE:** Resource creation awaits getting challenges from issue provider.
     */
    challengeCount?: number;
    /**
     * Domain owner-check method. Possible values:
     * - "DNS_CNAME" - you will need to create a CNAME dns record with the specified value. Recommended for fully automated certificate renewal;
     * - "DNS_TXT" - you will need to create a TXT dns record with specified value;
     * - "HTTP" - you will need to place specified value into specified url.
     */
    challengeType: string;
}

export interface CmCertificateSelfManaged {
    /**
     * Certificate with chain.
     */
    certificate: string;
    /**
     * Private key of certificate.
     */
    privateKey?: string;
    /**
     * Lockbox secret specification for getting private key. Structure is documented below.
     *
     * > **NOTE:** Only one type `privateKey` or `privateKeyLockboxSecret` should be specified.
     */
    privateKeyLockboxSecret?: outputs.CmCertificateSelfManagedPrivateKeyLockboxSecret;
}

export interface CmCertificateSelfManagedPrivateKeyLockboxSecret {
    /**
     * Lockbox secret Id.
     */
    id: string;
    /**
     * Key of the Lockbox secret, the value of which contains the private key of the certificate.
     */
    key: string;
}

export interface ComputeDiskDiskPlacementPolicy {
    /**
     * Specifies Disk Placement Group id.
     *
     * > **NOTE:** Only one of `imageId` or `snapshotId` can be specified.
     */
    diskPlacementGroupId: string;
}

export interface ComputeInstanceBootDisk {
    /**
     * Defines whether the disk will be auto-deleted when the instance
     * is deleted. The default value is `True`.
     */
    autoDelete?: boolean;
    /**
     * Name that can be used to access an attached disk.
     */
    deviceName: string;
    /**
     * The ID of the existing disk (such as those managed by
     * `yandex.ComputeDisk`) to attach as a boot disk.
     */
    diskId: string;
    /**
     * Parameters for a new disk that will be created
     * alongside the new instance. Either `initializeParams` or `diskId` must be set. The structure is documented below.
     *
     * > **NOTE:** Either `initializeParams` or `diskId` must be specified.
     */
    initializeParams: outputs.ComputeInstanceBootDiskInitializeParams;
    /**
     * Type of access to the disk resource. By default, a disk is attached in `READ_WRITE` mode.
     */
    mode: string;
}

export interface ComputeInstanceBootDiskInitializeParams {
    /**
     * Block size of the disk, specified in bytes.
     */
    blockSize: number;
    /**
     * Description of the boot disk.
     */
    description: string;
    /**
     * A disk image to initialize this disk from.
     */
    imageId: string;
    /**
     * Name of the boot disk.
     */
    name: string;
    /**
     * Size of the disk in GB.
     */
    size: number;
    /**
     * A snapshot to initialize this disk from.
     *
     * > **NOTE:** Either `imageId` or `snapshotId` must be specified.
     */
    snapshotId: string;
    /**
     * Disk type.
     */
    type?: string;
}

export interface ComputeInstanceFilesystem {
    /**
     * Name of the device representing the filesystem on the instance.
     */
    deviceName: string;
    /**
     * ID of the filesystem that should be attached.
     */
    filesystemId: string;
    /**
     * Mode of access to the filesystem that should be attached. By default, filesystem is attached 
     * in `READ_WRITE` mode.
     */
    mode?: string;
}

export interface ComputeInstanceGroupAllocationPolicy {
    /**
     * Array of availability zone IDs with list of instance tags.
     */
    instanceTagsPools?: outputs.ComputeInstanceGroupAllocationPolicyInstanceTagsPool[];
    /**
     * A list of availability zones.
     */
    zones: string[];
}

export interface ComputeInstanceGroupAllocationPolicyInstanceTagsPool {
    /**
     * List of tags for instances in zone.
     */
    tags: string[];
    /**
     * Availability zone.
     */
    zone: string;
}

export interface ComputeInstanceGroupApplicationLoadBalancer {
    /**
     * Do not wait load balancer health checks.
     */
    ignoreHealthChecks?: boolean;
    /**
     * Timeout for waiting for the VM to be checked by the load balancer. If the timeout is exceeded, the VM will be turned off based on the deployment policy. Specified in seconds.
     */
    maxOpeningTrafficDuration?: number;
    /**
     * The status message of the instance.
     */
    statusMessage: string;
    /**
     * A description of the target group.
     */
    targetGroupDescription?: string;
    targetGroupId: string;
    /**
     * A set of key/value label pairs.
     */
    targetGroupLabels?: {[key: string]: string};
    /**
     * The name of the target group.
     */
    targetGroupName?: string;
}

export interface ComputeInstanceGroupDeployPolicy {
    /**
     * The maximum number of instances that can be created at the same time.
     */
    maxCreating?: number;
    /**
     * The maximum number of instances that can be deleted at the same time.
     */
    maxDeleting?: number;
    /**
     * The maximum number of instances that can be temporarily allocated above the group's target size
     * during the update process.
     *
     * - - -
     */
    maxExpansion: number;
    /**
     * The maximum number of running instances that can be taken offline (stopped or deleted) at the same time
     * during the update process.
     */
    maxUnavailable: number;
    /**
     * The amount of time in seconds to allow for an instance to start.
     * Instance will be considered up and running (and start receiving traffic) only after the startupDuration
     * has elapsed and all health checks are passed.
     */
    startupDuration?: number;
    /**
     * Affects the lifecycle of the instance during deployment. If set to `proactive` (default), Instance Groups
     * can forcefully stop a running instance. If `opportunistic`, Instance Groups does not stop a running instance. Instead,
     * it will wait until the instance stops itself or becomes unhealthy.
     */
    strategy: string;
}

export interface ComputeInstanceGroupHealthCheck {
    /**
     * The number of successful health checks before the managed instance is declared healthy.
     */
    healthyThreshold?: number;
    /**
     * HTTP check options. The structure is documented below.
     */
    httpOptions?: outputs.ComputeInstanceGroupHealthCheckHttpOptions;
    /**
     * The interval to wait between health checks in seconds.
     */
    interval?: number;
    /**
     * TCP check options. The structure is documented below.
     */
    tcpOptions?: outputs.ComputeInstanceGroupHealthCheckTcpOptions;
    /**
     * The length of time to wait for a response before the health check times out in seconds.
     */
    timeout?: number;
    /**
     * The number of failed health checks before the managed instance is declared unhealthy.
     */
    unhealthyThreshold?: number;
}

export interface ComputeInstanceGroupHealthCheckHttpOptions {
    /**
     * The URL path used for health check requests.
     */
    path: string;
    /**
     * The port used for HTTP health checks.
     */
    port: number;
}

export interface ComputeInstanceGroupHealthCheckTcpOptions {
    /**
     * The port used for TCP health checks.
     */
    port: number;
}

export interface ComputeInstanceGroupInstance {
    /**
     * DNS record fqdn (must have dot at the end).
     */
    fqdn: string;
    /**
     * The ID of the instance.
     */
    instanceId: string;
    instanceTag: string;
    /**
     * The name of the instance group.
     */
    name: string;
    /**
     * Network specifications for the instance. This can be used multiple times for adding multiple interfaces. The structure is documented below.
     */
    networkInterfaces: outputs.ComputeInstanceGroupInstanceNetworkInterface[];
    /**
     * The status of the instance.
     */
    status: string;
    statusChangedAt: string;
    /**
     * The status message of the instance.
     */
    statusMessage: string;
    /**
     * The ID of the availability zone where the instance resides.
     */
    zoneId: string;
}

export interface ComputeInstanceGroupInstanceNetworkInterface {
    /**
     * The index of the network interface as generated by the server.
     */
    index: number;
    /**
     * Manual set static IP address.
     */
    ipAddress: string;
    /**
     * True if IPv4 address allocated for the network interface.
     */
    ipv4: boolean;
    ipv6: boolean;
    /**
     * Manual set static IPv6 address.
     */
    ipv6Address: string;
    /**
     * The MAC address assigned to the network interface.
     */
    macAddress: string;
    /**
     * Flag for using NAT.
     */
    nat: boolean;
    /**
     * A public address that can be used to access the internet over NAT. Use `variables` to set.
     */
    natIpAddress: string;
    /**
     * The IP version for the public address.
     */
    natIpVersion: string;
    /**
     * The ID of the subnet to attach this interface to. The subnet must reside in the same zone where this instance was created.
     */
    subnetId: string;
}

export interface ComputeInstanceGroupInstanceTemplate {
    /**
     * Boot disk specifications for the instance. The structure is documented below.
     */
    bootDisk: outputs.ComputeInstanceGroupInstanceTemplateBootDisk;
    /**
     * A description of the instance.
     */
    description?: string;
    /**
     * List of filesystems to attach to the instance. The structure is documented below.
     */
    filesystems?: outputs.ComputeInstanceGroupInstanceTemplateFilesystem[];
    /**
     * Hostname template for the instance.   
     * This field is used to generate the FQDN value of instance.
     * The hostname must be unique within the network and region.
     * If not specified, the hostname will be equal to id of the instance
     * and FQDN will be `<id>.auto.internal`. Otherwise FQDN will be `<hostname>.<region_id>.internal`.
     * In order to be unique it must contain at least on of instance unique placeholders:
     * {instance.short_id}
     * {instance.index}
     * combination of {instance.zone_id} and {instance.index_in_zone}
     * Example: my-instance-{instance.index}
     * If not set, `name` value will be used
     * It may also contain another placeholders, see metadata doc for full list.
     */
    hostname?: string;
    /**
     * A set of key/value label pairs to assign to the instance.
     */
    labels: {[key: string]: string};
    /**
     * A set of metadata key/value pairs to make available from within the instance.
     */
    metadata: {[key: string]: string};
    /**
     * Options allow user to configure access to managed instances metadata
     */
    metadataOptions: outputs.ComputeInstanceGroupInstanceTemplateMetadataOptions;
    /**
     * Name template of the instance.  
     * In order to be unique it must contain at least one of instance unique placeholders:
     * {instance.short_id}
     * {instance.index}
     * combination of {instance.zone_id} and {instance.index_in_zone}
     * Example: my-instance-{instance.index}
     * If not set, default is used: {instance_group.id}-{instance.short_id}
     * It may also contain another placeholders, see metadata doc for full list.
     */
    name?: string;
    /**
     * Network specifications for the instance. This can be used multiple times for adding multiple interfaces. The structure is documented below.
     */
    networkInterfaces: outputs.ComputeInstanceGroupInstanceTemplateNetworkInterface[];
    /**
     * Network acceleration type for instance. The structure is documented below.
     */
    networkSettings?: outputs.ComputeInstanceGroupInstanceTemplateNetworkSetting[];
    /**
     * The placement policy configuration. The structure is documented below.
     */
    placementPolicy?: outputs.ComputeInstanceGroupInstanceTemplatePlacementPolicy;
    /**
     * The ID of the hardware platform configuration for the instance. The default is 'standard-v1'.
     */
    platformId?: string;
    /**
     * Compute resource specifications for the instance. The structure is documented below.
     */
    resources: outputs.ComputeInstanceGroupInstanceTemplateResources;
    /**
     * The scheduling policy configuration. The structure is documented below.
     */
    schedulingPolicy: outputs.ComputeInstanceGroupInstanceTemplateSchedulingPolicy;
    /**
     * A list of disks to attach to the instance. The structure is documented below.
     */
    secondaryDisks?: outputs.ComputeInstanceGroupInstanceTemplateSecondaryDisk[];
    /**
     * The ID of the service account authorized for this instance.
     */
    serviceAccountId?: string;
}

export interface ComputeInstanceGroupInstanceTemplateBootDisk {
    /**
     * This value can be used to reference the device under `/dev/disk/by-id/`.
     */
    deviceName: string;
    /**
     * ID of the existing disk. To set use variables.
     */
    diskId?: string;
    /**
     * Parameters for creating a disk alongside the instance. The structure is documented below.
     */
    initializeParams?: outputs.ComputeInstanceGroupInstanceTemplateBootDiskInitializeParams;
    /**
     * The access mode to the disk resource. By default a disk is attached in `READ_WRITE` mode.
     */
    mode?: string;
    /**
     * When set can be later used to change DiskSpec of actual disk.
     */
    name?: string;
}

export interface ComputeInstanceGroupInstanceTemplateBootDiskInitializeParams {
    /**
     * A description of the boot disk.
     */
    description?: string;
    /**
     * The disk image to initialize this disk from.
     */
    imageId: string;
    /**
     * The size of the disk in GB.
     */
    size: number;
    /**
     * The snapshot to initialize this disk from.
     *
     * > **NOTE:** `imageId` or `snapshotId` must be specified.
     */
    snapshotId: string;
    /**
     * The disk type.
     */
    type?: string;
}

export interface ComputeInstanceGroupInstanceTemplateFilesystem {
    /**
     * This value can be used to reference the device under `/dev/disk/by-id/`.
     */
    deviceName?: string;
    /**
     * (Required) ID of the filesystem that should be attached.
     */
    filesystemId: string;
    /**
     * The access mode to the disk resource. By default a disk is attached in `READ_WRITE` mode.
     */
    mode?: string;
}

export interface ComputeInstanceGroupInstanceTemplateMetadataOptions {
    awsV1HttpEndpoint: number;
    awsV1HttpToken: number;
    gceHttpEndpoint: number;
    gceHttpToken: number;
}

export interface ComputeInstanceGroupInstanceTemplateNetworkInterface {
    /**
     * List of dns records.  The structure is documented below.
     */
    dnsRecords?: outputs.ComputeInstanceGroupInstanceTemplateNetworkInterfaceDnsRecord[];
    /**
     * Manual set static IP address.
     */
    ipAddress: string;
    /**
     * True if IPv4 address allocated for the network interface.
     */
    ipv4?: boolean;
    ipv6: boolean;
    /**
     * Manual set static IPv6 address.
     */
    ipv6Address: string;
    /**
     * List of ipv6 dns records.  The structure is documented below.
     */
    ipv6DnsRecords?: outputs.ComputeInstanceGroupInstanceTemplateNetworkInterfaceIpv6DnsRecord[];
    /**
     * Flag for using NAT.
     */
    nat: boolean;
    /**
     * List of nat dns records.  The structure is documented below.
     */
    natDnsRecords?: outputs.ComputeInstanceGroupInstanceTemplateNetworkInterfaceNatDnsRecord[];
    /**
     * A public address that can be used to access the internet over NAT. Use `variables` to set.
     */
    natIpAddress?: string;
    /**
     * The ID of the network.
     */
    networkId?: string;
    /**
     * Security group ids for network interface.
     */
    securityGroupIds?: string[];
    /**
     * The ID of the subnets to attach this interface to.
     */
    subnetIds?: string[];
}

export interface ComputeInstanceGroupInstanceTemplateNetworkInterfaceDnsRecord {
    /**
     * DNS zone id (if not set, private zone used).
     */
    dnsZoneId?: string;
    /**
     * DNS record fqdn (must have dot at the end).
     */
    fqdn: string;
    /**
     * When set to true, also create PTR DNS record.
     */
    ptr: boolean;
    /**
     * DNS record TTL.
     */
    ttl?: number;
}

export interface ComputeInstanceGroupInstanceTemplateNetworkInterfaceIpv6DnsRecord {
    /**
     * DNS zone id (if not set, private zone used).
     */
    dnsZoneId?: string;
    /**
     * DNS record fqdn (must have dot at the end).
     */
    fqdn: string;
    /**
     * When set to true, also create PTR DNS record.
     */
    ptr: boolean;
    /**
     * DNS record TTL.
     */
    ttl?: number;
}

export interface ComputeInstanceGroupInstanceTemplateNetworkInterfaceNatDnsRecord {
    /**
     * DNS zone id (if not set, private zone used).
     */
    dnsZoneId?: string;
    /**
     * DNS record fqdn (must have dot at the end).
     */
    fqdn: string;
    /**
     * When set to true, also create PTR DNS record.
     */
    ptr: boolean;
    /**
     * DNS record TTL.
     */
    ttl?: number;
}

export interface ComputeInstanceGroupInstanceTemplateNetworkSetting {
    /**
     * Network acceleration type. By default a network is in `STANDARD` mode.
     */
    type?: string;
}

export interface ComputeInstanceGroupInstanceTemplatePlacementPolicy {
    /**
     * Specifies the id of the Placement Group to assign to the instances.
     */
    placementGroupId: string;
}

export interface ComputeInstanceGroupInstanceTemplateResources {
    /**
     * If provided, specifies baseline core performance as a percent.
     */
    coreFraction?: number;
    /**
     * The number of CPU cores for the instance.
     *
     * - - -
     */
    cores: number;
    gpus?: number;
    /**
     * The memory size in GB.
     */
    memory: number;
}

export interface ComputeInstanceGroupInstanceTemplateSchedulingPolicy {
    /**
     * Specifies if the instance is preemptible. Defaults to false.
     */
    preemptible?: boolean;
}

export interface ComputeInstanceGroupInstanceTemplateSecondaryDisk {
    /**
     * This value can be used to reference the device under `/dev/disk/by-id/`.
     */
    deviceName?: string;
    /**
     * ID of the existing disk. To set use variables.
     */
    diskId?: string;
    /**
     * Parameters used for creating a disk alongside the instance. The structure is documented below.
     */
    initializeParams?: outputs.ComputeInstanceGroupInstanceTemplateSecondaryDiskInitializeParams;
    /**
     * The access mode to the disk resource. By default a disk is attached in `READ_WRITE` mode.
     */
    mode?: string;
    /**
     * When set can be later used to change DiskSpec of actual disk.
     */
    name?: string;
}

export interface ComputeInstanceGroupInstanceTemplateSecondaryDiskInitializeParams {
    /**
     * A description of the boot disk.
     */
    description?: string;
    /**
     * The disk image to initialize this disk from.
     */
    imageId?: string;
    /**
     * The size of the disk in GB.
     */
    size?: number;
    /**
     * The snapshot to initialize this disk from.
     *
     * > **NOTE:** `imageId` or `snapshotId` must be specified.
     */
    snapshotId?: string;
    /**
     * The disk type.
     */
    type?: string;
}

export interface ComputeInstanceGroupLoadBalancer {
    /**
     * Do not wait load balancer health checks.
     */
    ignoreHealthChecks?: boolean;
    /**
     * Timeout for waiting for the VM to be checked by the load balancer. If the timeout is exceeded, the VM will be turned off based on the deployment policy. Specified in seconds.
     */
    maxOpeningTrafficDuration?: number;
    /**
     * The status message of the target group.
     */
    statusMessage: string;
    /**
     * A description of the target group.
     */
    targetGroupDescription?: string;
    /**
     * The ID of the target group.
     */
    targetGroupId: string;
    /**
     * A set of key/value label pairs.
     */
    targetGroupLabels?: {[key: string]: string};
    /**
     * The name of the target group.
     */
    targetGroupName?: string;
}

export interface ComputeInstanceGroupScalePolicy {
    /**
     * The auto scaling policy of the instance group. The structure is documented below.
     *
     * > **NOTE:** Either `fixedScale` or `autoScale` must be specified.
     */
    autoScale?: outputs.ComputeInstanceGroupScalePolicyAutoScale;
    /**
     * The fixed scaling policy of the instance group. The structure is documented below.
     */
    fixedScale?: outputs.ComputeInstanceGroupScalePolicyFixedScale;
    /**
     * The test auto scaling policy of the instance group. Use it to test how the auto scale works. The structure is documented below.
     */
    testAutoScale?: outputs.ComputeInstanceGroupScalePolicyTestAutoScale;
}

export interface ComputeInstanceGroupScalePolicyAutoScale {
    /**
     * . Autoscale type, can be `ZONAL` or `REGIONAL`. By default `ZONAL` type is used.
     */
    autoScaleType?: string;
    /**
     * Target CPU load level.
     */
    cpuUtilizationTarget?: number;
    /**
     * A list of custom rules. The structure is documented below.
     */
    customRules?: outputs.ComputeInstanceGroupScalePolicyAutoScaleCustomRule[];
    /**
     * The initial number of instances in the instance group.
     */
    initialSize: number;
    /**
     * The maximum number of virtual machines in the group.
     */
    maxSize?: number;
    /**
     * The amount of time, in seconds, that metrics are averaged for.
     * If the average value at the end of the interval is higher than the `cpuUtilizationTarget`,
     * the instance group will increase the number of virtual machines in the group.
     */
    measurementDuration: number;
    /**
     * The minimum number of virtual machines in a single availability zone.
     */
    minZoneSize?: number;
    /**
     * The minimum time interval, in seconds, to monitor the load before
     * an instance group can reduce the number of virtual machines in the group. During this time, the group
     * will not decrease even if the average load falls below the value of `cpuUtilizationTarget`.
     */
    stabilizationDuration: number;
    /**
     * The warm-up time of the virtual machine, in seconds. During this time,
     * traffic is fed to the virtual machine, but load metrics are not taken into account.
     */
    warmupDuration: number;
}

export interface ComputeInstanceGroupScalePolicyAutoScaleCustomRule {
    /**
     * Folder ID of custom metric in Yandex Monitoring that should be used for scaling.
     */
    folderId?: string;
    /**
     * A map of labels of metric.
     */
    labels?: {[key: string]: string};
    /**
     * The name of metric.
     */
    metricName: string;
    /**
     * Metric type, `GAUGE` or `COUNTER`.
     */
    metricType: string;
    /**
     * Rule type: `UTILIZATION` - This type means that the metric applies to one instance.
     * First, Instance Groups calculates the average metric value for each instance,
     * then averages the values for instances in one availability zone.
     * This type of metric must have the `instanceId` label. `WORKLOAD` - This type means that the metric applies to instances in one availability zone.
     * This type of metric must have the `zoneId` label.
     */
    ruleType: string;
    /**
     * Service of custom metric in Yandex Monitoring that should be used for scaling.
     */
    service?: string;
    /**
     * Target metric value level.
     */
    target: number;
}

export interface ComputeInstanceGroupScalePolicyFixedScale {
    /**
     * The number of instances in the instance group.
     */
    size: number;
}

export interface ComputeInstanceGroupScalePolicyTestAutoScale {
    /**
     * . Autoscale type, can be `ZONAL` or `REGIONAL`. By default `ZONAL` type is used.
     */
    autoScaleType?: string;
    /**
     * Target CPU load level.
     */
    cpuUtilizationTarget?: number;
    /**
     * A list of custom rules. The structure is documented below.
     */
    customRules?: outputs.ComputeInstanceGroupScalePolicyTestAutoScaleCustomRule[];
    /**
     * The initial number of instances in the instance group.
     */
    initialSize: number;
    /**
     * The maximum number of virtual machines in the group.
     */
    maxSize?: number;
    /**
     * The amount of time, in seconds, that metrics are averaged for.
     * If the average value at the end of the interval is higher than the `cpuUtilizationTarget`,
     * the instance group will increase the number of virtual machines in the group.
     */
    measurementDuration: number;
    /**
     * The minimum number of virtual machines in a single availability zone.
     */
    minZoneSize?: number;
    /**
     * The minimum time interval, in seconds, to monitor the load before
     * an instance group can reduce the number of virtual machines in the group. During this time, the group
     * will not decrease even if the average load falls below the value of `cpuUtilizationTarget`.
     */
    stabilizationDuration: number;
    /**
     * The warm-up time of the virtual machine, in seconds. During this time,
     * traffic is fed to the virtual machine, but load metrics are not taken into account.
     */
    warmupDuration: number;
}

export interface ComputeInstanceGroupScalePolicyTestAutoScaleCustomRule {
    /**
     * Folder ID of custom metric in Yandex Monitoring that should be used for scaling.
     */
    folderId?: string;
    /**
     * A map of labels of metric.
     */
    labels?: {[key: string]: string};
    /**
     * The name of metric.
     */
    metricName: string;
    /**
     * Metric type, `GAUGE` or `COUNTER`.
     */
    metricType: string;
    /**
     * Rule type: `UTILIZATION` - This type means that the metric applies to one instance.
     * First, Instance Groups calculates the average metric value for each instance,
     * then averages the values for instances in one availability zone.
     * This type of metric must have the `instanceId` label. `WORKLOAD` - This type means that the metric applies to instances in one availability zone.
     * This type of metric must have the `zoneId` label.
     */
    ruleType: string;
    /**
     * Service of custom metric in Yandex Monitoring that should be used for scaling.
     */
    service?: string;
    /**
     * Target metric value level.
     */
    target: number;
}

export interface ComputeInstanceLocalDisk {
    /**
     * Name that can be used to access an attached disk.
     */
    deviceName: string;
    /**
     * Size of the disk, specified in bytes.
     *
     * > **NOTE:** Local disks are not available for all users by default.
     */
    sizeBytes: number;
}

export interface ComputeInstanceMetadataOptions {
    awsV1HttpEndpoint: number;
    awsV1HttpToken: number;
    gceHttpEndpoint: number;
    gceHttpToken: number;
}

export interface ComputeInstanceNetworkInterface {
    /**
     * List of configurations for creating ipv4 DNS records. The structure is documented below.
     */
    dnsRecords?: outputs.ComputeInstanceNetworkInterfaceDnsRecord[];
    /**
     * Index of network interface, will be calculated automatically for instance create or update operations
     * if not specified. Required for attach/detach operations.
     */
    index: number;
    /**
     * The private IP address to assign to the instance. If
     * empty, the address will be automatically assigned from the specified subnet.
     */
    ipAddress: string;
    /**
     * Allocate an IPv4 address for the interface. The default value is `true`.
     */
    ipv4?: boolean;
    /**
     * If true, allocate an IPv6 address for the interface.
     * The address will be automatically assigned from the specified subnet.
     */
    ipv6: boolean;
    /**
     * The private IPv6 address to assign to the instance.
     */
    ipv6Address: string;
    /**
     * List of configurations for creating ipv6 DNS records. The structure is documented below.
     */
    ipv6DnsRecords?: outputs.ComputeInstanceNetworkInterfaceIpv6DnsRecord[];
    macAddress: string;
    /**
     * Provide a public address, for instance, to access the internet over NAT.
     */
    nat?: boolean;
    /**
     * List of configurations for creating ipv4 NAT DNS records. The structure is documented below.
     */
    natDnsRecords?: outputs.ComputeInstanceNetworkInterfaceNatDnsRecord[];
    /**
     * Provide a public address, for instance, to access the internet over NAT. Address should be already reserved in web UI.
     */
    natIpAddress: string;
    natIpVersion: string;
    /**
     * Security group ids for network interface.
     */
    securityGroupIds: string[];
    /**
     * ID of the subnet to attach this
     * interface to. The subnet must exist in the same zone where this instance will be
     * created.
     */
    subnetId: string;
}

export interface ComputeInstanceNetworkInterfaceDnsRecord {
    /**
     * DNS zone ID (if not set, private zone used).
     */
    dnsZoneId?: string;
    /**
     * DNS record FQDN (must have a dot at the end).
     */
    fqdn: string;
    /**
     * When set to true, also create a PTR DNS record.
     */
    ptr?: boolean;
    /**
     * DNS record TTL. in seconds
     */
    ttl?: number;
}

export interface ComputeInstanceNetworkInterfaceIpv6DnsRecord {
    /**
     * DNS zone ID (if not set, private zone used).
     */
    dnsZoneId?: string;
    /**
     * DNS record FQDN (must have a dot at the end).
     */
    fqdn: string;
    /**
     * When set to true, also create a PTR DNS record.
     */
    ptr?: boolean;
    /**
     * DNS record TTL. in seconds
     */
    ttl?: number;
}

export interface ComputeInstanceNetworkInterfaceNatDnsRecord {
    /**
     * DNS zone ID (if not set, private zone used).
     */
    dnsZoneId?: string;
    /**
     * DNS record FQDN (must have a dot at the end).
     */
    fqdn: string;
    /**
     * When set to true, also create a PTR DNS record.
     */
    ptr?: boolean;
    /**
     * DNS record TTL. in seconds
     */
    ttl?: number;
}

export interface ComputeInstancePlacementPolicy {
    hostAffinityRules: outputs.ComputeInstancePlacementPolicyHostAffinityRule[];
    /**
     * Specifies the id of the Placement Group to assign to the instance.
     */
    placementGroupId?: string;
    placementGroupPartition?: number;
}

export interface ComputeInstancePlacementPolicyHostAffinityRule {
    /**
     * Affinity label or one of reserved values - `yc.hostId`, `yc.hostGroupId`.
     */
    key: string;
    /**
     * Affinity action. The only value supported is `IN`.
     */
    op: string;
    values: string[];
}

export interface ComputeInstanceResources {
    /**
     * If provided, specifies baseline performance for a core as a percent.
     */
    coreFraction?: number;
    /**
     * CPU cores for the instance.
     */
    cores: number;
    /**
     * If provided, specifies the number of GPU devices for the instance
     */
    gpus?: number;
    /**
     * Memory size in GB.
     */
    memory: number;
}

export interface ComputeInstanceSchedulingPolicy {
    /**
     * Specifies if the instance is preemptible. Defaults to false.
     */
    preemptible?: boolean;
}

export interface ComputeInstanceSecondaryDisk {
    /**
     * Whether the disk is auto-deleted when the instance
     * is deleted. The default value is false.
     */
    autoDelete?: boolean;
    /**
     * Name that can be used to access an attached disk
     * under `/dev/disk/by-id/`.
     */
    deviceName: string;
    /**
     * ID of the disk that is attached to the instance.
     */
    diskId: string;
    /**
     * Type of access to the disk resource. By default, a disk is attached in `READ_WRITE` mode.
     */
    mode?: string;
}

export interface ComputeSnapshotScheduleSchedulePolicy {
    /**
     * Cron expression to schedule snapshots (in cron format "* * * * *").
     */
    expression?: string;
    /**
     * Time to start the snapshot schedule (in format RFC3339 "2006-01-02T15:04:05Z07:00"). If empty current time will be used. Unlike an `expression` that specifies regularity rules, the `startAt` parameter determines from what point these rules will be applied.
     */
    startAt: string;
}

export interface ComputeSnapshotScheduleSnapshotSpec {
    /**
     * Description to assign to snapshots created by this snapshot schedule.
     */
    description?: string;
    /**
     * A set of key/value label pairs to assign to snapshots created by this snapshot schedule.
     */
    labels?: {[key: string]: string};
}

export interface ContainerRepositoryLifecyclePolicyRule {
    /**
     * Description of the lifecycle policy.
     */
    description: string;
    /**
     * The period of time that must pass after creating a image for it to suit the automatic deletion criteria. It must be a multiple of 24 hours.
     */
    expirePeriod?: string;
    /**
     * The number of images to be retained even if the expirePeriod already expired.
     */
    retainedTop: number;
    /**
     * Tag to specify a filter as a regular expression. For example `.*` - all images with tags.
     */
    tagRegexp?: string;
    /**
     * If enabled, rules apply to untagged Docker images.
     */
    untagged: boolean;
}

export interface DataprocClusterClusterConfig {
    /**
     * Data Proc specific options. The structure is documented below.
     */
    hadoop?: outputs.DataprocClusterClusterConfigHadoop;
    /**
     * Configuration of the Data Proc subcluster. The structure is documented below.
     */
    subclusterSpecs: outputs.DataprocClusterClusterConfigSubclusterSpec[];
    /**
     * Version of Data Proc image.
     */
    versionId: string;
}

export interface DataprocClusterClusterConfigHadoop {
    /**
     * List of initialization scripts. The structure is documented below.
     */
    initializationActions?: outputs.DataprocClusterClusterConfigHadoopInitializationAction[];
    /**
     * A set of key/value pairs that are used to configure cluster services.
     */
    properties?: {[key: string]: string};
    /**
     * List of services to run on Data Proc cluster.
     */
    services?: string[];
    /**
     * List of SSH public keys to put to the hosts of the cluster. For information on how to connect to the cluster, see [the official documentation](https://cloud.yandex.com/docs/data-proc/operations/connect).
     */
    sshPublicKeys?: string[];
}

export interface DataprocClusterClusterConfigHadoopInitializationAction {
    /**
     * List of arguments of the initialization script.
     */
    args: string[];
    /**
     * Script execution timeout, in seconds.
     */
    timeout: string;
    /**
     * Script URI.
     */
    uri: string;
}

export interface DataprocClusterClusterConfigSubclusterSpec {
    /**
     * If true then assign public IP addresses to the hosts of the subclusters.
     */
    assignPublicIp?: boolean;
    /**
     * Autoscaling configuration for compute subclusters.
     */
    autoscalingConfig?: outputs.DataprocClusterClusterConfigSubclusterSpecAutoscalingConfig;
    /**
     * Number of hosts within Data Proc subcluster.
     */
    hostsCount: number;
    /**
     * (Computed) ID of a new Data Proc cluster.
     */
    id: string;
    /**
     * Name of the Data Proc subcluster.
     */
    name: string;
    /**
     * Resources allocated to each host of the Data Proc subcluster. The structure is documented below.
     */
    resources: outputs.DataprocClusterClusterConfigSubclusterSpecResources;
    /**
     * Role of the subcluster in the Data Proc cluster.
     */
    role: string;
    /**
     * The ID of the subnet, to which hosts of the subcluster belong. Subnets of all the subclusters must belong to the same VPC network.
     */
    subnetId: string;
}

export interface DataprocClusterClusterConfigSubclusterSpecAutoscalingConfig {
    /**
     * Defines an autoscaling rule based on the average CPU utilization of the instance group. If not set default autoscaling metric will be used.
     */
    cpuUtilizationTarget: string;
    /**
     * Timeout to gracefully decommission nodes during downscaling. In seconds.
     */
    decommissionTimeout: string;
    /**
     * Maximum number of nodes in autoscaling subclusters.
     */
    maxHostsCount: number;
    /**
     * Time in seconds allotted for averaging metrics.
     */
    measurementDuration: string;
    /**
     * Bool flag -- whether to use preemptible compute instances. Preemptible instances are stopped at least once every 24 hours, and can be stopped at any time if their resources are needed by Compute. For more information, see [Preemptible Virtual Machines](https://cloud.yandex.com/docs/compute/concepts/preemptible-vm).
     */
    preemptible?: boolean;
    /**
     * Minimum amount of time in seconds allotted for monitoring before Instance Groups can reduce the number of instances in the group. During this time, the group size doesn't decrease, even if the new metric values indicate that it should.
     */
    stabilizationDuration: string;
    /**
     * The warmup time of the instance in seconds. During this time, traffic is sent to the instance, but instance metrics are not collected.
     */
    warmupDuration: string;
}

export interface DataprocClusterClusterConfigSubclusterSpecResources {
    /**
     * Volume of the storage available to a host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of a host. One of `network-hdd` (default) or `network-ssd`.
     */
    diskTypeId?: string;
    /**
     * The ID of the preset for computational resources available to a host. All available presets are listed in the [documentation](https://cloud.yandex.com/docs/data-proc/concepts/instance-types).
     */
    resourcePresetId: string;
}

export interface DatatransferEndpointSettings {
    /**
     * Settings specific to the ClickHouse source endpoint.
     */
    clickhouseSource?: outputs.DatatransferEndpointSettingsClickhouseSource;
    /**
     * Settings specific to the ClickHouse target endpoint.
     */
    clickhouseTarget?: outputs.DatatransferEndpointSettingsClickhouseTarget;
    /**
     * Settings specific to the Kafka source endpoint.
     */
    kafkaSource?: outputs.DatatransferEndpointSettingsKafkaSource;
    /**
     * Settings specific to the Kafka target endpoint.
     */
    kafkaTarget?: outputs.DatatransferEndpointSettingsKafkaTarget;
    metrikaSource?: outputs.DatatransferEndpointSettingsMetrikaSource;
    /**
     * Settings specific to the MongoDB source endpoint.
     */
    mongoSource?: outputs.DatatransferEndpointSettingsMongoSource;
    /**
     * Settings specific to the MongoDB target endpoint.
     */
    mongoTarget?: outputs.DatatransferEndpointSettingsMongoTarget;
    /**
     * Settings specific to the MySQL source endpoint.
     */
    mysqlSource?: outputs.DatatransferEndpointSettingsMysqlSource;
    /**
     * Settings specific to the MySQL target endpoint.
     */
    mysqlTarget?: outputs.DatatransferEndpointSettingsMysqlTarget;
    /**
     * Settings specific to the PostgreSQL source endpoint.
     */
    postgresSource?: outputs.DatatransferEndpointSettingsPostgresSource;
    /**
     * Settings specific to the PostgreSQL target endpoint.
     */
    postgresTarget?: outputs.DatatransferEndpointSettingsPostgresTarget;
    /**
     * Settings specific to the YDB source endpoint.
     */
    ydbSource?: outputs.DatatransferEndpointSettingsYdbSource;
    /**
     * Settings specific to the YDB target endpoint.
     */
    ydbTarget?: outputs.DatatransferEndpointSettingsYdbTarget;
    /**
     * Settings specific to the YDS source endpoint.
     */
    ydsSource?: outputs.DatatransferEndpointSettingsYdsSource;
    /**
     * Settings specific to the YDS target endpoint.
     *
     * For the documentation of the specific endpoint settings see below.
     */
    ydsTarget?: outputs.DatatransferEndpointSettingsYdsTarget;
}

export interface DatatransferEndpointSettingsClickhouseSource {
    /**
     * Name of the ClickHouse cluster. For managed ClickHouse clusters defaults to managed cluster ID.
     */
    clickhouseClusterName: string;
    /**
     * Connection settings. The structure is documented below.
     */
    connection: outputs.DatatransferEndpointSettingsClickhouseSourceConnection;
    /**
     * The list of tables that should not be transferred.
     */
    excludeTables: string[];
    /**
     * The list of tables that should be transferred. Leave empty if all tables should be transferred.
     */
    includeTables: string[];
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
}

export interface DatatransferEndpointSettingsClickhouseSourceConnection {
    /**
     * Connection options. The structure is documented below.
     */
    connectionOptions: outputs.DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptions;
}

export interface DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptions {
    /**
     * Database name.
     */
    database: string;
    /**
     * Identifier of the Managed ClickHouse cluster.
     */
    mdbClusterId?: string;
    /**
     * Connection settings of the on-premise ClickHouse server.
     */
    onPremise?: outputs.DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsOnPremise;
    /**
     * Password for the database access. This is a block with a single field named `raw` which should contain the password.
     */
    password: outputs.DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsPassword;
    /**
     * User for database access.
     */
    user: string;
}

export interface DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsOnPremise {
    /**
     * TCP port number for the HTTP interface of the ClickHouse server.
     */
    httpPort: number;
    /**
     * TCP port number for the native interface of the ClickHouse server.
     */
    nativePort: number;
    /**
     * The list of ClickHouse shards. The structure is documented below.
     */
    shards: outputs.DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsOnPremiseShard[];
    /**
     * TLS settings for the server connection. Empty implies plaintext connection. The structure is documented below.
     */
    tlsMode: outputs.DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsOnPremiseTlsMode;
}

export interface DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsOnPremiseShard {
    /**
     * List of ClickHouse server host names.
     */
    hosts: string[];
    /**
     * Arbitrary shard name. This name may be used in `sharding` block to specify custom sharding rules.
     */
    name: string;
}

export interface DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsOnPremiseTlsMode {
    /**
     * Empty block designating that the connection is not secured, i.e. plaintext connection.
     */
    disabled?: outputs.DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsOnPremiseTlsModeDisabled;
    /**
     * If this attribute is not an empty block, then TLS is used for the server connection. The structure is documented below.
     */
    enabled?: outputs.DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsOnPremiseTlsModeEnabled;
}

export interface DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsOnPremiseTlsModeDisabled {
}

export interface DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsOnPremiseTlsModeEnabled {
    /**
     * X.509 certificate of the certificate authority which issued the server's certificate, in PEM format. If empty, the server's certificate must be signed by a well-known CA.
     */
    caCertificate: string;
}

export interface DatatransferEndpointSettingsClickhouseSourceConnectionConnectionOptionsPassword {
    raw: string;
}

export interface DatatransferEndpointSettingsClickhouseTarget {
    /**
     * Table renaming rules. The structure is documented below.
     */
    altNames: outputs.DatatransferEndpointSettingsClickhouseTargetAltName[];
    /**
     * How to clean collections when activating the transfer. One of "CLICKHOUSE_CLEANUP_POLICY_DISABLED" or "CLICKHOUSE_CLEANUP_POLICY_DROP".
     */
    cleanupPolicy: string;
    /**
     * Name of the ClickHouse cluster. For managed ClickHouse clusters defaults to managed cluster ID.
     */
    clickhouseClusterName: string;
    /**
     * Connection settings. The structure is documented below.
     */
    connection: outputs.DatatransferEndpointSettingsClickhouseTargetConnection;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * Shard selection rules for the data being transferred. The structure is documented below.
     */
    sharding: outputs.DatatransferEndpointSettingsClickhouseTargetSharding;
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
}

export interface DatatransferEndpointSettingsClickhouseTargetAltName {
    fromName: string;
    toName: string;
}

export interface DatatransferEndpointSettingsClickhouseTargetConnection {
    /**
     * Connection options. The structure is documented below.
     */
    connectionOptions: outputs.DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptions;
}

export interface DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptions {
    /**
     * Database name.
     */
    database: string;
    /**
     * Identifier of the Managed ClickHouse cluster.
     */
    mdbClusterId?: string;
    /**
     * Connection settings of the on-premise ClickHouse server.
     */
    onPremise?: outputs.DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsOnPremise;
    /**
     * Password for the database access. This is a block with a single field named `raw` which should contain the password.
     */
    password: outputs.DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsPassword;
    /**
     * User for database access.
     */
    user: string;
}

export interface DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsOnPremise {
    /**
     * TCP port number for the HTTP interface of the ClickHouse server.
     */
    httpPort: number;
    /**
     * TCP port number for the native interface of the ClickHouse server.
     */
    nativePort: number;
    /**
     * The list of ClickHouse shards. The structure is documented below.
     */
    shards: outputs.DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsOnPremiseShard[];
    /**
     * TLS settings for the server connection. Empty implies plaintext connection. The structure is documented below.
     */
    tlsMode: outputs.DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsOnPremiseTlsMode;
}

export interface DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsOnPremiseShard {
    /**
     * List of ClickHouse server host names.
     */
    hosts: string[];
    /**
     * Arbitrary shard name. This name may be used in `sharding` block to specify custom sharding rules.
     */
    name: string;
}

export interface DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsOnPremiseTlsMode {
    /**
     * Empty block designating that the connection is not secured, i.e. plaintext connection.
     */
    disabled?: outputs.DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsOnPremiseTlsModeDisabled;
    /**
     * If this attribute is not an empty block, then TLS is used for the server connection. The structure is documented below.
     */
    enabled?: outputs.DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsOnPremiseTlsModeEnabled;
}

export interface DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsOnPremiseTlsModeDisabled {
}

export interface DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsOnPremiseTlsModeEnabled {
    /**
     * X.509 certificate of the certificate authority which issued the server's certificate, in PEM format. If empty, the server's certificate must be signed by a well-known CA.
     */
    caCertificate: string;
}

export interface DatatransferEndpointSettingsClickhouseTargetConnectionConnectionOptionsPassword {
    raw: string;
}

export interface DatatransferEndpointSettingsClickhouseTargetSharding {
    /**
     * Shard data by the hash value of the specified column. The structure is documented below.
     */
    columnValueHash?: outputs.DatatransferEndpointSettingsClickhouseTargetShardingColumnValueHash;
    /**
     * A custom shard mapping by the value of the specified column. The structure is documented below.
     */
    customMapping?: outputs.DatatransferEndpointSettingsClickhouseTargetShardingCustomMapping;
    /**
     * Distribute incoming rows between ClickHouse shards in a round-robin manner. Specify as an empty block to enable.
     */
    roundRobin?: outputs.DatatransferEndpointSettingsClickhouseTargetShardingRoundRobin;
    /**
     * Shard data by ID of the transfer.
     */
    transferId?: outputs.DatatransferEndpointSettingsClickhouseTargetShardingTransferId;
}

export interface DatatransferEndpointSettingsClickhouseTargetShardingColumnValueHash {
    /**
     * The name of the column to calculate hash from.
     */
    columnName: string;
}

export interface DatatransferEndpointSettingsClickhouseTargetShardingCustomMapping {
    /**
     * The name of the column to inspect when deciding the shard to chose for an incoming row.
     */
    columnName: string;
    /**
     * The mapping of the specified column values to the shard names. The structure is documented below.
     */
    mappings: outputs.DatatransferEndpointSettingsClickhouseTargetShardingCustomMappingMapping[];
}

export interface DatatransferEndpointSettingsClickhouseTargetShardingCustomMappingMapping {
    /**
     * The value of the column. Currently only the string columns are supported. The structure is documented below.
     */
    columnValue: outputs.DatatransferEndpointSettingsClickhouseTargetShardingCustomMappingMappingColumnValue;
    /**
     * The name of the shard into which all the rows with the specified `columnValue` will be written.
     */
    shardName: string;
}

export interface DatatransferEndpointSettingsClickhouseTargetShardingCustomMappingMappingColumnValue {
    /**
     * The string value of the column.
     */
    stringValue: string;
}

export interface DatatransferEndpointSettingsClickhouseTargetShardingRoundRobin {
}

export interface DatatransferEndpointSettingsClickhouseTargetShardingTransferId {
}

export interface DatatransferEndpointSettingsKafkaSource {
    /**
     * Authentication data.
     */
    auth: outputs.DatatransferEndpointSettingsKafkaSourceAuth;
    /**
     * Connection settings.
     */
    connection: outputs.DatatransferEndpointSettingsKafkaSourceConnection;
    /**
     * Data parsing parameters. If not set, the source messages are read in raw.
     */
    parser: outputs.DatatransferEndpointSettingsKafkaSourceParser;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * Deprecated. Please use `topicNames` instead.
     */
    topicName: string;
    /**
     * The list of full source topic names.
     */
    topicNames: string[];
    /**
     * Transform data with a custom Cloud Function.
     */
    transformer: outputs.DatatransferEndpointSettingsKafkaSourceTransformer;
}

export interface DatatransferEndpointSettingsKafkaSourceAuth {
    /**
     * Connection without authentication data.
     */
    noAuth?: outputs.DatatransferEndpointSettingsKafkaSourceAuthNoAuth;
    /**
     * Authentication using sasl.
     */
    sasl?: outputs.DatatransferEndpointSettingsKafkaSourceAuthSasl;
}

export interface DatatransferEndpointSettingsKafkaSourceAuthNoAuth {
}

export interface DatatransferEndpointSettingsKafkaSourceAuthSasl {
    mechanism: string;
    /**
     * Password for the database access. This is a block with a single field named `raw` which should contain the password.
     */
    password: outputs.DatatransferEndpointSettingsKafkaSourceAuthSaslPassword;
    /**
     * User for database access.
     */
    user: string;
}

export interface DatatransferEndpointSettingsKafkaSourceAuthSaslPassword {
    raw: string;
}

export interface DatatransferEndpointSettingsKafkaSourceConnection {
    /**
     * Identifier of the Managed Kafka cluster.
     */
    clusterId?: string;
    /**
     * Connection settings of the on-premise Kafka server.
     */
    onPremise?: outputs.DatatransferEndpointSettingsKafkaSourceConnectionOnPremise;
}

export interface DatatransferEndpointSettingsKafkaSourceConnectionOnPremise {
    /**
     * List of Kafka broker URLs.
     */
    brokerUrls: string[];
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
    /**
     * TLS settings for the server connection. Empty implies plaintext connection. The structure is documented below.
     */
    tlsMode: outputs.DatatransferEndpointSettingsKafkaSourceConnectionOnPremiseTlsMode;
}

export interface DatatransferEndpointSettingsKafkaSourceConnectionOnPremiseTlsMode {
    /**
     * Empty block designating that the connection is not secured, i.e. plaintext connection.
     */
    disabled?: outputs.DatatransferEndpointSettingsKafkaSourceConnectionOnPremiseTlsModeDisabled;
    /**
     * If this attribute is not an empty block, then TLS is used for the server connection. The structure is documented below.
     */
    enabled?: outputs.DatatransferEndpointSettingsKafkaSourceConnectionOnPremiseTlsModeEnabled;
}

export interface DatatransferEndpointSettingsKafkaSourceConnectionOnPremiseTlsModeDisabled {
}

export interface DatatransferEndpointSettingsKafkaSourceConnectionOnPremiseTlsModeEnabled {
    /**
     * X.509 certificate of the certificate authority which issued the server's certificate, in PEM format. If empty, the server's certificate must be signed by a well-known CA.
     */
    caCertificate: string;
}

export interface DatatransferEndpointSettingsKafkaSourceParser {
    /**
     * Parse Audit Trails data. Empty struct.
     */
    auditTrailsV1Parser?: outputs.DatatransferEndpointSettingsKafkaSourceParserAuditTrailsV1Parser;
    /**
     * Parse Cloud Logging data. Empty struct.
     */
    cloudLoggingParser?: outputs.DatatransferEndpointSettingsKafkaSourceParserCloudLoggingParser;
    /**
     * Parse data in json format.
     */
    jsonParser?: outputs.DatatransferEndpointSettingsKafkaSourceParserJsonParser;
    /**
     * Parse data if tskv format.
     */
    tskvParser?: outputs.DatatransferEndpointSettingsKafkaSourceParserTskvParser;
}

export interface DatatransferEndpointSettingsKafkaSourceParserAuditTrailsV1Parser {
}

export interface DatatransferEndpointSettingsKafkaSourceParserCloudLoggingParser {
}

export interface DatatransferEndpointSettingsKafkaSourceParserJsonParser {
    /**
     * Add fields, that are not in the schema, into the _rest column.
     */
    addRestColumn: boolean;
    /**
     * Data parsing scheme.The structure is documented below.
     */
    dataSchema: outputs.DatatransferEndpointSettingsKafkaSourceParserJsonParserDataSchema;
    /**
     * Allow null keys. If `false` - null keys will be putted to unparsed data
     */
    nullKeysAllowed: boolean;
    /**
     * Allow unescape string values.
     */
    unescapeStringValues: boolean;
}

export interface DatatransferEndpointSettingsKafkaSourceParserJsonParserDataSchema {
    /**
     * Description of the data schema in the array of `fields` structure (documented below).
     */
    fields?: outputs.DatatransferEndpointSettingsKafkaSourceParserJsonParserDataSchemaFields;
    /**
     * Description of the data schema as JSON specification.
     */
    jsonFields?: string;
}

export interface DatatransferEndpointSettingsKafkaSourceParserJsonParserDataSchemaFields {
    /**
     * Description of the data schema in the array of `fields` structure (documented below).
     */
    fields: outputs.DatatransferEndpointSettingsKafkaSourceParserJsonParserDataSchemaFieldsField[];
}

export interface DatatransferEndpointSettingsKafkaSourceParserJsonParserDataSchemaFieldsField {
    /**
     * Mark field as Primary Key.
     */
    key: boolean;
    /**
     * Field name.
     */
    name: string;
    /**
     * Path to the field.
     */
    path: string;
    /**
     * Mark field as required.
     */
    required: boolean;
    /**
     * Field type, one of: `INT64`, `INT32`, `INT16`, `INT8`, `UINT64`, `UINT32`, `UINT16`, `UINT8`, `DOUBLE`, `BOOLEAN`, `STRING`, `UTF8`, `ANY`, `DATETIME`.
     */
    type: string;
}

export interface DatatransferEndpointSettingsKafkaSourceParserTskvParser {
    /**
     * Add fields, that are not in the schema, into the _rest column.
     */
    addRestColumn: boolean;
    /**
     * Data parsing scheme.The structure is documented below.
     */
    dataSchema: outputs.DatatransferEndpointSettingsKafkaSourceParserTskvParserDataSchema;
    /**
     * Allow null keys. If `false` - null keys will be putted to unparsed data
     */
    nullKeysAllowed: boolean;
    /**
     * Allow unescape string values.
     */
    unescapeStringValues: boolean;
}

export interface DatatransferEndpointSettingsKafkaSourceParserTskvParserDataSchema {
    /**
     * Description of the data schema in the array of `fields` structure (documented below).
     */
    fields?: outputs.DatatransferEndpointSettingsKafkaSourceParserTskvParserDataSchemaFields;
    /**
     * Description of the data schema as JSON specification.
     */
    jsonFields?: string;
}

export interface DatatransferEndpointSettingsKafkaSourceParserTskvParserDataSchemaFields {
    /**
     * Description of the data schema in the array of `fields` structure (documented below).
     */
    fields: outputs.DatatransferEndpointSettingsKafkaSourceParserTskvParserDataSchemaFieldsField[];
}

export interface DatatransferEndpointSettingsKafkaSourceParserTskvParserDataSchemaFieldsField {
    /**
     * Mark field as Primary Key.
     */
    key: boolean;
    /**
     * Field name.
     */
    name: string;
    /**
     * Path to the field.
     */
    path: string;
    /**
     * Mark field as required.
     */
    required: boolean;
    /**
     * Field type, one of: `INT64`, `INT32`, `INT16`, `INT8`, `UINT64`, `UINT32`, `UINT16`, `UINT8`, `DOUBLE`, `BOOLEAN`, `STRING`, `UTF8`, `ANY`, `DATETIME`.
     */
    type: string;
}

export interface DatatransferEndpointSettingsKafkaSourceTransformer {
    bufferFlushInterval: string;
    bufferSize: string;
    cloudFunction: string;
    invocationTimeout: string;
    numberOfRetries: number;
    /**
     * - (Required) Service account ID for interaction with database.
     */
    serviceAccountId: string;
}

export interface DatatransferEndpointSettingsKafkaTarget {
    /**
     * Authentication data.
     */
    auth: outputs.DatatransferEndpointSettingsKafkaTargetAuth;
    /**
     * Connection settings.
     */
    connection: outputs.DatatransferEndpointSettingsKafkaTargetConnection;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * Data serialization settings.
     */
    serializer: outputs.DatatransferEndpointSettingsKafkaTargetSerializer;
    /**
     * Target topic settings.
     */
    topicSettings: outputs.DatatransferEndpointSettingsKafkaTargetTopicSettings;
}

export interface DatatransferEndpointSettingsKafkaTargetAuth {
    /**
     * Connection without authentication data.
     */
    noAuth?: outputs.DatatransferEndpointSettingsKafkaTargetAuthNoAuth;
    /**
     * Authentication using sasl.
     */
    sasl?: outputs.DatatransferEndpointSettingsKafkaTargetAuthSasl;
}

export interface DatatransferEndpointSettingsKafkaTargetAuthNoAuth {
}

export interface DatatransferEndpointSettingsKafkaTargetAuthSasl {
    mechanism: string;
    /**
     * Password for the database access. This is a block with a single field named `raw` which should contain the password.
     */
    password: outputs.DatatransferEndpointSettingsKafkaTargetAuthSaslPassword;
    /**
     * User for database access.
     */
    user: string;
}

export interface DatatransferEndpointSettingsKafkaTargetAuthSaslPassword {
    raw: string;
}

export interface DatatransferEndpointSettingsKafkaTargetConnection {
    /**
     * Identifier of the Managed Kafka cluster.
     */
    clusterId?: string;
    /**
     * Connection settings of the on-premise Kafka server.
     */
    onPremise?: outputs.DatatransferEndpointSettingsKafkaTargetConnectionOnPremise;
}

export interface DatatransferEndpointSettingsKafkaTargetConnectionOnPremise {
    /**
     * List of Kafka broker URLs.
     */
    brokerUrls: string[];
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
    /**
     * TLS settings for the server connection. Empty implies plaintext connection. The structure is documented below.
     */
    tlsMode: outputs.DatatransferEndpointSettingsKafkaTargetConnectionOnPremiseTlsMode;
}

export interface DatatransferEndpointSettingsKafkaTargetConnectionOnPremiseTlsMode {
    /**
     * Empty block designating that the connection is not secured, i.e. plaintext connection.
     */
    disabled?: outputs.DatatransferEndpointSettingsKafkaTargetConnectionOnPremiseTlsModeDisabled;
    /**
     * If this attribute is not an empty block, then TLS is used for the server connection. The structure is documented below.
     */
    enabled?: outputs.DatatransferEndpointSettingsKafkaTargetConnectionOnPremiseTlsModeEnabled;
}

export interface DatatransferEndpointSettingsKafkaTargetConnectionOnPremiseTlsModeDisabled {
}

export interface DatatransferEndpointSettingsKafkaTargetConnectionOnPremiseTlsModeEnabled {
    /**
     * X.509 certificate of the certificate authority which issued the server's certificate, in PEM format. If empty, the server's certificate must be signed by a well-known CA.
     */
    caCertificate: string;
}

export interface DatatransferEndpointSettingsKafkaTargetSerializer {
    /**
     * Empty block. Select data serialization format automatically.
     */
    serializerAuto?: outputs.DatatransferEndpointSettingsKafkaTargetSerializerSerializerAuto;
    /**
     * Serialize data in json format. The structure is documented below.
     */
    serializerDebezium?: outputs.DatatransferEndpointSettingsKafkaTargetSerializerSerializerDebezium;
    /**
     * Empty block. Serialize data in json format.
     */
    serializerJson?: outputs.DatatransferEndpointSettingsKafkaTargetSerializerSerializerJson;
}

export interface DatatransferEndpointSettingsKafkaTargetSerializerSerializerAuto {
}

export interface DatatransferEndpointSettingsKafkaTargetSerializerSerializerDebezium {
    /**
     * A list of debezium parameters set by the structure of the `key` and `value` string fields.
     */
    serializerParameters: outputs.DatatransferEndpointSettingsKafkaTargetSerializerSerializerDebeziumSerializerParameter[];
}

export interface DatatransferEndpointSettingsKafkaTargetSerializerSerializerDebeziumSerializerParameter {
    /**
     * Mark field as Primary Key.
     */
    key: string;
    value: string;
}

export interface DatatransferEndpointSettingsKafkaTargetSerializerSerializerJson {
}

export interface DatatransferEndpointSettingsKafkaTargetTopicSettings {
    /**
     * All messages will be sent to one topic. The structure is documented below.
     */
    topic?: outputs.DatatransferEndpointSettingsKafkaTargetTopicSettingsTopic;
    /**
     * Topic name prefix. Messages will be sent to topic with name <topic_prefix>.<schema>.<table_name>.
     */
    topicPrefix?: string;
}

export interface DatatransferEndpointSettingsKafkaTargetTopicSettingsTopic {
    /**
     * Not to split events queue into separate per-table queues.
     */
    saveTxOrder: boolean;
    /**
     * Full topic name
     */
    topicName: string;
}

export interface DatatransferEndpointSettingsMetrikaSource {
    counterIds: number[];
    streams: outputs.DatatransferEndpointSettingsMetrikaSourceStream[];
    token: outputs.DatatransferEndpointSettingsMetrikaSourceToken;
}

export interface DatatransferEndpointSettingsMetrikaSourceStream {
    columns: string[];
    /**
     * Field type, one of: `INT64`, `INT32`, `INT16`, `INT8`, `UINT64`, `UINT32`, `UINT16`, `UINT8`, `DOUBLE`, `BOOLEAN`, `STRING`, `UTF8`, `ANY`, `DATETIME`.
     */
    type: string;
}

export interface DatatransferEndpointSettingsMetrikaSourceToken {
    raw: string;
}

export interface DatatransferEndpointSettingsMongoSource {
    /**
     * The list of the MongoDB collections that should be transferred. If omitted, all available collections will be transferred. The structure of the list item is documented below.
     */
    collections: outputs.DatatransferEndpointSettingsMongoSourceCollection[];
    /**
     * Connection settings. The structure is documented below.
     */
    connection: outputs.DatatransferEndpointSettingsMongoSourceConnection;
    /**
     * The list of the MongoDB collections that should not be transferred.
     */
    excludedCollections: outputs.DatatransferEndpointSettingsMongoSourceExcludedCollection[];
    /**
     * whether the secondary server should be preferred to the primary when copying data.
     */
    secondaryPreferredMode: boolean;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
}

export interface DatatransferEndpointSettingsMongoSourceCollection {
    /**
     * Collection name.
     */
    collectionName: string;
    /**
     * Database name.
     */
    databaseName: string;
}

export interface DatatransferEndpointSettingsMongoSourceConnection {
    /**
     * Connection options. The structure is documented below.
     */
    connectionOptions: outputs.DatatransferEndpointSettingsMongoSourceConnectionConnectionOptions;
}

export interface DatatransferEndpointSettingsMongoSourceConnectionConnectionOptions {
    /**
     * Name of the database associated with the credentials.
     */
    authSource: string;
    /**
     * Identifier of the Managed ClickHouse cluster.
     */
    mdbClusterId?: string;
    /**
     * Connection settings of the on-premise ClickHouse server.
     */
    onPremise?: outputs.DatatransferEndpointSettingsMongoSourceConnectionConnectionOptionsOnPremise;
    /**
     * Password for the database access. This is a block with a single field named `raw` which should contain the password.
     */
    password: outputs.DatatransferEndpointSettingsMongoSourceConnectionConnectionOptionsPassword;
    /**
     * User for database access.
     */
    user: string;
}

export interface DatatransferEndpointSettingsMongoSourceConnectionConnectionOptionsOnPremise {
    /**
     * Host names of the replica set.
     */
    hosts: string[];
    /**
     * TCP Port number.
     */
    port: number;
    /**
     * Replica set name.
     */
    replicaSet: string;
    /**
     * TLS settings for the server connection. Empty implies plaintext connection. The structure is documented below.
     */
    tlsMode: outputs.DatatransferEndpointSettingsMongoSourceConnectionConnectionOptionsOnPremiseTlsMode;
}

export interface DatatransferEndpointSettingsMongoSourceConnectionConnectionOptionsOnPremiseTlsMode {
    /**
     * Empty block designating that the connection is not secured, i.e. plaintext connection.
     */
    disabled?: outputs.DatatransferEndpointSettingsMongoSourceConnectionConnectionOptionsOnPremiseTlsModeDisabled;
    /**
     * If this attribute is not an empty block, then TLS is used for the server connection. The structure is documented below.
     */
    enabled?: outputs.DatatransferEndpointSettingsMongoSourceConnectionConnectionOptionsOnPremiseTlsModeEnabled;
}

export interface DatatransferEndpointSettingsMongoSourceConnectionConnectionOptionsOnPremiseTlsModeDisabled {
}

export interface DatatransferEndpointSettingsMongoSourceConnectionConnectionOptionsOnPremiseTlsModeEnabled {
    /**
     * X.509 certificate of the certificate authority which issued the server's certificate, in PEM format. If empty, the server's certificate must be signed by a well-known CA.
     */
    caCertificate: string;
}

export interface DatatransferEndpointSettingsMongoSourceConnectionConnectionOptionsPassword {
    raw: string;
}

export interface DatatransferEndpointSettingsMongoSourceExcludedCollection {
    /**
     * Collection name.
     */
    collectionName: string;
    /**
     * Database name.
     */
    databaseName: string;
}

export interface DatatransferEndpointSettingsMongoTarget {
    /**
     * How to clean collections when activating the transfer. One of "DISABLED", "DROP" or "TRUNCATE".
     */
    cleanupPolicy: string;
    /**
     * Connection settings. The structure is documented below.
     */
    connection: outputs.DatatransferEndpointSettingsMongoTargetConnection;
    /**
     * If not empty, then all the data will be written to the database with the specified name; otherwise the database name is the same as in the source endpoint.
     */
    database: string;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
}

export interface DatatransferEndpointSettingsMongoTargetConnection {
    /**
     * Connection options. The structure is documented below.
     */
    connectionOptions: outputs.DatatransferEndpointSettingsMongoTargetConnectionConnectionOptions;
}

export interface DatatransferEndpointSettingsMongoTargetConnectionConnectionOptions {
    /**
     * Name of the database associated with the credentials.
     */
    authSource: string;
    /**
     * Identifier of the Managed ClickHouse cluster.
     */
    mdbClusterId?: string;
    /**
     * Connection settings of the on-premise ClickHouse server.
     */
    onPremise?: outputs.DatatransferEndpointSettingsMongoTargetConnectionConnectionOptionsOnPremise;
    /**
     * Password for the database access. This is a block with a single field named `raw` which should contain the password.
     */
    password: outputs.DatatransferEndpointSettingsMongoTargetConnectionConnectionOptionsPassword;
    /**
     * User for database access.
     */
    user: string;
}

export interface DatatransferEndpointSettingsMongoTargetConnectionConnectionOptionsOnPremise {
    /**
     * Host names of the replica set.
     */
    hosts: string[];
    /**
     * TCP Port number.
     */
    port: number;
    /**
     * Replica set name.
     */
    replicaSet: string;
    /**
     * TLS settings for the server connection. Empty implies plaintext connection. The structure is documented below.
     */
    tlsMode: outputs.DatatransferEndpointSettingsMongoTargetConnectionConnectionOptionsOnPremiseTlsMode;
}

export interface DatatransferEndpointSettingsMongoTargetConnectionConnectionOptionsOnPremiseTlsMode {
    /**
     * Empty block designating that the connection is not secured, i.e. plaintext connection.
     */
    disabled?: outputs.DatatransferEndpointSettingsMongoTargetConnectionConnectionOptionsOnPremiseTlsModeDisabled;
    /**
     * If this attribute is not an empty block, then TLS is used for the server connection. The structure is documented below.
     */
    enabled?: outputs.DatatransferEndpointSettingsMongoTargetConnectionConnectionOptionsOnPremiseTlsModeEnabled;
}

export interface DatatransferEndpointSettingsMongoTargetConnectionConnectionOptionsOnPremiseTlsModeDisabled {
}

export interface DatatransferEndpointSettingsMongoTargetConnectionConnectionOptionsOnPremiseTlsModeEnabled {
    /**
     * X.509 certificate of the certificate authority which issued the server's certificate, in PEM format. If empty, the server's certificate must be signed by a well-known CA.
     */
    caCertificate: string;
}

export interface DatatransferEndpointSettingsMongoTargetConnectionConnectionOptionsPassword {
    raw: string;
}

export interface DatatransferEndpointSettingsMysqlSource {
    /**
     * Connection settings. The structure is documented below.
     */
    connection: outputs.DatatransferEndpointSettingsMysqlSourceConnection;
    /**
     * Name of the database to transfer.
     */
    database: string;
    /**
     * Opposite of `includeTableRegex`. The tables matching the specified regular expressions will not be transferred.
     */
    excludeTablesRegexes: string[];
    /**
     * List of regular expressions of table names which should be transferred. A table name is formatted as schemaname.tablename. For example, a single regular expression may look like `^mydb.employees$`.
     */
    includeTablesRegexes: string[];
    /**
     * Defines which database schema objects should be transferred, e.g. views, routines, etc.
     */
    objectTransferSettings: outputs.DatatransferEndpointSettingsMysqlSourceObjectTransferSettings;
    /**
     * Password for the database access. This is a block with a single field named `raw` which should contain the password.
     */
    password: outputs.DatatransferEndpointSettingsMysqlSourcePassword;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * The name of the database where technical tables (`__tm_keeper`, `__tm_gtid_keeper`) will be created. Default is the value of the attribute `database`.
     */
    serviceDatabase: string;
    /**
     * Timezone to use for parsing timestamps for saving source timezones. Accepts values from IANA timezone database. Default: local timezone.
     */
    timezone: string;
    /**
     * User for the database access.
     */
    user: string;
}

export interface DatatransferEndpointSettingsMysqlSourceConnection {
    /**
     * Identifier of the Managed MySQL cluster.
     */
    mdbClusterId?: string;
    /**
     * Connection settings of the on-premise Kafka server.
     */
    onPremise?: outputs.DatatransferEndpointSettingsMysqlSourceConnectionOnPremise;
}

export interface DatatransferEndpointSettingsMysqlSourceConnectionOnPremise {
    /**
     * Host names of the replica set.
     */
    hosts: string[];
    /**
     * TCP Port number.
     */
    port: number;
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
    /**
     * TLS settings for the server connection. Empty implies plaintext connection. The structure is documented below.
     */
    tlsMode: outputs.DatatransferEndpointSettingsMysqlSourceConnectionOnPremiseTlsMode;
}

export interface DatatransferEndpointSettingsMysqlSourceConnectionOnPremiseTlsMode {
    /**
     * Empty block designating that the connection is not secured, i.e. plaintext connection.
     */
    disabled?: outputs.DatatransferEndpointSettingsMysqlSourceConnectionOnPremiseTlsModeDisabled;
    /**
     * If this attribute is not an empty block, then TLS is used for the server connection. The structure is documented below.
     */
    enabled?: outputs.DatatransferEndpointSettingsMysqlSourceConnectionOnPremiseTlsModeEnabled;
}

export interface DatatransferEndpointSettingsMysqlSourceConnectionOnPremiseTlsModeDisabled {
}

export interface DatatransferEndpointSettingsMysqlSourceConnectionOnPremiseTlsModeEnabled {
    /**
     * X.509 certificate of the certificate authority which issued the server's certificate, in PEM format. If empty, the server's certificate must be signed by a well-known CA.
     */
    caCertificate: string;
}

export interface DatatransferEndpointSettingsMysqlSourceObjectTransferSettings {
    routine: string;
    tables: string;
    /**
     * All of the attrubutes are optional and should be either "BEFORE_DATA", "AFTER_DATA" or "NEVER".
     */
    trigger: string;
    view: string;
}

export interface DatatransferEndpointSettingsMysqlSourcePassword {
    raw: string;
}

export interface DatatransferEndpointSettingsMysqlTarget {
    /**
     * How to clean tables when activating the transfer. One of "DISABLED", "DROP" or "TRUNCATE".
     */
    cleanupPolicy: string;
    /**
     * Connection settings. The structure is documented below.
     */
    connection: outputs.DatatransferEndpointSettingsMysqlTargetConnection;
    /**
     * Name of the database to transfer.
     */
    database: string;
    /**
     * Password for the database access. This is a block with a single field named `raw` which should contain the password.
     */
    password: outputs.DatatransferEndpointSettingsMysqlTargetPassword;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * The name of the database where technical tables (`__tm_keeper`, `__tm_gtid_keeper`) will be created. Default is the value of the attribute `database`.
     */
    serviceDatabase: string;
    /**
     * When true, disables foreign key checks. See [foreignKeyChecks](https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_foreign_key_checks). False by default.
     */
    skipConstraintChecks: boolean;
    /**
     * [sqlMode](https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html) to use when interacting with the server. Defaults to "NO_AUTO_VALUE_ON_ZERO,NO_DIR_IN_CREATE,NO_ENGINE_SUBSTITUTION".
     */
    sqlMode: string;
    /**
     * Timezone to use for parsing timestamps for saving source timezones. Accepts values from IANA timezone database. Default: local timezone.
     */
    timezone: string;
    /**
     * User for the database access.
     */
    user: string;
}

export interface DatatransferEndpointSettingsMysqlTargetConnection {
    /**
     * Identifier of the Managed MySQL cluster.
     */
    mdbClusterId?: string;
    /**
     * Connection settings of the on-premise Kafka server.
     */
    onPremise?: outputs.DatatransferEndpointSettingsMysqlTargetConnectionOnPremise;
}

export interface DatatransferEndpointSettingsMysqlTargetConnectionOnPremise {
    /**
     * Host names of the replica set.
     */
    hosts: string[];
    /**
     * TCP Port number.
     */
    port: number;
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
    /**
     * TLS settings for the server connection. Empty implies plaintext connection. The structure is documented below.
     */
    tlsMode: outputs.DatatransferEndpointSettingsMysqlTargetConnectionOnPremiseTlsMode;
}

export interface DatatransferEndpointSettingsMysqlTargetConnectionOnPremiseTlsMode {
    /**
     * Empty block designating that the connection is not secured, i.e. plaintext connection.
     */
    disabled?: outputs.DatatransferEndpointSettingsMysqlTargetConnectionOnPremiseTlsModeDisabled;
    /**
     * If this attribute is not an empty block, then TLS is used for the server connection. The structure is documented below.
     */
    enabled?: outputs.DatatransferEndpointSettingsMysqlTargetConnectionOnPremiseTlsModeEnabled;
}

export interface DatatransferEndpointSettingsMysqlTargetConnectionOnPremiseTlsModeDisabled {
}

export interface DatatransferEndpointSettingsMysqlTargetConnectionOnPremiseTlsModeEnabled {
    /**
     * X.509 certificate of the certificate authority which issued the server's certificate, in PEM format. If empty, the server's certificate must be signed by a well-known CA.
     */
    caCertificate: string;
}

export interface DatatransferEndpointSettingsMysqlTargetPassword {
    raw: string;
}

export interface DatatransferEndpointSettingsPostgresSource {
    /**
     * Connection settings. The structure is documented below.
     */
    connection: outputs.DatatransferEndpointSettingsPostgresSourceConnection;
    /**
     * Name of the database to transfer.
     */
    database: string;
    /**
     * List of tables which will not be transfered, formatted as `schemaname.tablename`.
     */
    excludeTables: string[];
    /**
     * List of tables to transfer, formatted as `schemaname.tablename`. If omitted or an empty list is specified, all tables will be transferred.
     */
    includeTables: string[];
    /**
     * Defines which database schema objects should be transferred, e.g. views, functions, etc.
     */
    objectTransferSettings: outputs.DatatransferEndpointSettingsPostgresSourceObjectTransferSettings;
    /**
     * Password for the database access. This is a block with a single field named `raw` which should contain the password.
     */
    password: outputs.DatatransferEndpointSettingsPostgresSourcePassword;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * Name of the database schema in which auxiliary tables needed for the transfer will be created. Empty `serviceSchema` implies schema "public".
     */
    serviceSchema: string;
    /**
     * Maximum WAL size held by the replication slot, in gigabytes. Exceeding this limit will result in a replication failure and deletion of the replication slot. Unlimited by default.
     */
    slotGigabyteLagLimit: number;
    /**
     * User for the database access.
     */
    user: string;
}

export interface DatatransferEndpointSettingsPostgresSourceConnection {
    /**
     * Identifier of the Managed MySQL cluster.
     */
    mdbClusterId?: string;
    /**
     * Connection settings of the on-premise Kafka server.
     */
    onPremise?: outputs.DatatransferEndpointSettingsPostgresSourceConnectionOnPremise;
}

export interface DatatransferEndpointSettingsPostgresSourceConnectionOnPremise {
    /**
     * Host names of the replica set.
     */
    hosts: string[];
    /**
     * TCP Port number.
     */
    port: number;
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
    /**
     * TLS settings for the server connection. Empty implies plaintext connection. The structure is documented below.
     */
    tlsMode: outputs.DatatransferEndpointSettingsPostgresSourceConnectionOnPremiseTlsMode;
}

export interface DatatransferEndpointSettingsPostgresSourceConnectionOnPremiseTlsMode {
    /**
     * Empty block designating that the connection is not secured, i.e. plaintext connection.
     */
    disabled?: outputs.DatatransferEndpointSettingsPostgresSourceConnectionOnPremiseTlsModeDisabled;
    /**
     * If this attribute is not an empty block, then TLS is used for the server connection. The structure is documented below.
     */
    enabled?: outputs.DatatransferEndpointSettingsPostgresSourceConnectionOnPremiseTlsModeEnabled;
}

export interface DatatransferEndpointSettingsPostgresSourceConnectionOnPremiseTlsModeDisabled {
}

export interface DatatransferEndpointSettingsPostgresSourceConnectionOnPremiseTlsModeEnabled {
    /**
     * X.509 certificate of the certificate authority which issued the server's certificate, in PEM format. If empty, the server's certificate must be signed by a well-known CA.
     */
    caCertificate: string;
}

export interface DatatransferEndpointSettingsPostgresSourceObjectTransferSettings {
    /**
     * All of the attrubutes are optional and should be either "BEFORE_DATA", "AFTER_DATA" or "NEVER".
     */
    cast: string;
    collation: string;
    constraint: string;
    defaultValues: string;
    fkConstraint: string;
    function: string;
    index: string;
    materializedView: string;
    policy: string;
    primaryKey: string;
    rule: string;
    sequence: string;
    sequenceOwnedBy: string;
    sequenceSet: string;
    table: string;
    /**
     * All of the attrubutes are optional and should be either "BEFORE_DATA", "AFTER_DATA" or "NEVER".
     */
    trigger: string;
    type: string;
    view: string;
}

export interface DatatransferEndpointSettingsPostgresSourcePassword {
    raw: string;
}

export interface DatatransferEndpointSettingsPostgresTarget {
    /**
     * How to clean collections when activating the transfer. One of "CLICKHOUSE_CLEANUP_POLICY_DISABLED" or "CLICKHOUSE_CLEANUP_POLICY_DROP".
     */
    cleanupPolicy: string;
    /**
     * Connection settings. The structure is documented below.
     */
    connection: outputs.DatatransferEndpointSettingsPostgresTargetConnection;
    /**
     * Name of the database to transfer.
     */
    database: string;
    /**
     * Password for the database access. This is a block with a single field named `raw` which should contain the password.
     */
    password: outputs.DatatransferEndpointSettingsPostgresTargetPassword;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * User for the database access.
     */
    user: string;
}

export interface DatatransferEndpointSettingsPostgresTargetConnection {
    /**
     * Identifier of the Managed MySQL cluster.
     */
    mdbClusterId?: string;
    /**
     * Connection settings of the on-premise Kafka server.
     */
    onPremise?: outputs.DatatransferEndpointSettingsPostgresTargetConnectionOnPremise;
}

export interface DatatransferEndpointSettingsPostgresTargetConnectionOnPremise {
    /**
     * Host names of the replica set.
     */
    hosts: string[];
    /**
     * TCP Port number.
     */
    port: number;
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
    /**
     * TLS settings for the server connection. Empty implies plaintext connection. The structure is documented below.
     */
    tlsMode: outputs.DatatransferEndpointSettingsPostgresTargetConnectionOnPremiseTlsMode;
}

export interface DatatransferEndpointSettingsPostgresTargetConnectionOnPremiseTlsMode {
    /**
     * Empty block designating that the connection is not secured, i.e. plaintext connection.
     */
    disabled?: outputs.DatatransferEndpointSettingsPostgresTargetConnectionOnPremiseTlsModeDisabled;
    /**
     * If this attribute is not an empty block, then TLS is used for the server connection. The structure is documented below.
     */
    enabled?: outputs.DatatransferEndpointSettingsPostgresTargetConnectionOnPremiseTlsModeEnabled;
}

export interface DatatransferEndpointSettingsPostgresTargetConnectionOnPremiseTlsModeDisabled {
}

export interface DatatransferEndpointSettingsPostgresTargetConnectionOnPremiseTlsModeEnabled {
    /**
     * X.509 certificate of the certificate authority which issued the server's certificate, in PEM format. If empty, the server's certificate must be signed by a well-known CA.
     */
    caCertificate: string;
}

export interface DatatransferEndpointSettingsPostgresTargetPassword {
    raw: string;
}

export interface DatatransferEndpointSettingsYdbSource {
    /**
     * - (Optional) Custom name for changefeed.
     */
    changefeedCustomName: string;
    /**
     * Database name.
     */
    database: string;
    /**
     * - (Optional) Instance of YDB. Example: "my-cute-ydb.cloud.yandex.ru:2135".
     */
    instance: string;
    /**
     * - (Optional) A list of paths which should be uploaded. When not specified, all available tables are uploaded.
     */
    paths: string[];
    /**
     * - (Optional, Sensitive) Authentication key.
     */
    saKeyContent: string;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * - (Required) Service account ID for interaction with database.
     */
    serviceAccountId: string;
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
}

export interface DatatransferEndpointSettingsYdbTarget {
    /**
     * How to clean collections when activating the transfer. One of "CLICKHOUSE_CLEANUP_POLICY_DISABLED" or "CLICKHOUSE_CLEANUP_POLICY_DROP".
     */
    cleanupPolicy: string;
    /**
     * Database name.
     */
    database: string;
    /**
     * - (Optional) Compression that will be used for default columns family on YDB table creation One of "YDB_DEFAULT_COMPRESSION_UNSPECIFIED", "YDB_DEFAULT_COMPRESSION_DISABLED", "YDB_DEFAULT_COMPRESSION_LZ4".
     */
    defaultCompression: string;
    /**
     * - (Optional) Instance of YDB. Example: "my-cute-ydb.cloud.yandex.ru:2135".
     */
    instance: string;
    /**
     * - (Optional) Whether a column-oriented (i.e. OLAP) tables should be created. Default is `false` (create row-oriented OLTP tables).
     */
    isTableColumnOriented: boolean;
    /**
     * Path to the field.
     */
    path: string;
    /**
     * - (Optional, Sensitive) Authentication key.
     */
    saKeyContent: string;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * - (Required) Service account ID for interaction with database.
     */
    serviceAccountId: string;
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
}

export interface DatatransferEndpointSettingsYdsSource {
    /**
     * - (Optional) Should continue working, if consumer read lag exceed TTL of topic.
     */
    allowTtlRewind: boolean;
    /**
     * - (Optional) Consumer.
     */
    consumer: string;
    /**
     * Database name.
     */
    database: string;
    /**
     * - (Optional) YDS Endpoint.
     */
    endpoint: string;
    /**
     * Data parsing parameters. If not set, the source messages are read in raw.
     */
    parser: outputs.DatatransferEndpointSettingsYdsSourceParser;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * - (Required) Service account ID for interaction with database.
     */
    serviceAccountId: string;
    /**
     * - (Optional) Stream.
     */
    stream: string;
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
    /**
     * - (Optional) List of supported compression codec.
     */
    supportedCodecs: string[];
}

export interface DatatransferEndpointSettingsYdsSourceParser {
    /**
     * Parse Audit Trails data. Empty struct.
     */
    auditTrailsV1Parser?: outputs.DatatransferEndpointSettingsYdsSourceParserAuditTrailsV1Parser;
    /**
     * Parse Cloud Logging data. Empty struct.
     */
    cloudLoggingParser?: outputs.DatatransferEndpointSettingsYdsSourceParserCloudLoggingParser;
    /**
     * Parse data in json format.
     */
    jsonParser?: outputs.DatatransferEndpointSettingsYdsSourceParserJsonParser;
    /**
     * Parse data if tskv format.
     */
    tskvParser?: outputs.DatatransferEndpointSettingsYdsSourceParserTskvParser;
}

export interface DatatransferEndpointSettingsYdsSourceParserAuditTrailsV1Parser {
}

export interface DatatransferEndpointSettingsYdsSourceParserCloudLoggingParser {
}

export interface DatatransferEndpointSettingsYdsSourceParserJsonParser {
    /**
     * Add fields, that are not in the schema, into the _rest column.
     */
    addRestColumn: boolean;
    /**
     * Data parsing scheme.The structure is documented below.
     */
    dataSchema: outputs.DatatransferEndpointSettingsYdsSourceParserJsonParserDataSchema;
    /**
     * Allow null keys. If `false` - null keys will be putted to unparsed data
     */
    nullKeysAllowed: boolean;
    /**
     * Allow unescape string values.
     */
    unescapeStringValues: boolean;
}

export interface DatatransferEndpointSettingsYdsSourceParserJsonParserDataSchema {
    /**
     * Description of the data schema in the array of `fields` structure (documented below).
     */
    fields?: outputs.DatatransferEndpointSettingsYdsSourceParserJsonParserDataSchemaFields;
    /**
     * Description of the data schema as JSON specification.
     */
    jsonFields?: string;
}

export interface DatatransferEndpointSettingsYdsSourceParserJsonParserDataSchemaFields {
    /**
     * Description of the data schema in the array of `fields` structure (documented below).
     */
    fields: outputs.DatatransferEndpointSettingsYdsSourceParserJsonParserDataSchemaFieldsField[];
}

export interface DatatransferEndpointSettingsYdsSourceParserJsonParserDataSchemaFieldsField {
    /**
     * Mark field as Primary Key.
     */
    key: boolean;
    /**
     * Field name.
     */
    name: string;
    /**
     * Path to the field.
     */
    path: string;
    /**
     * Mark field as required.
     */
    required: boolean;
    /**
     * Field type, one of: `INT64`, `INT32`, `INT16`, `INT8`, `UINT64`, `UINT32`, `UINT16`, `UINT8`, `DOUBLE`, `BOOLEAN`, `STRING`, `UTF8`, `ANY`, `DATETIME`.
     */
    type: string;
}

export interface DatatransferEndpointSettingsYdsSourceParserTskvParser {
    /**
     * Add fields, that are not in the schema, into the _rest column.
     */
    addRestColumn: boolean;
    /**
     * Data parsing scheme.The structure is documented below.
     */
    dataSchema: outputs.DatatransferEndpointSettingsYdsSourceParserTskvParserDataSchema;
    /**
     * Allow null keys. If `false` - null keys will be putted to unparsed data
     */
    nullKeysAllowed: boolean;
    /**
     * Allow unescape string values.
     */
    unescapeStringValues: boolean;
}

export interface DatatransferEndpointSettingsYdsSourceParserTskvParserDataSchema {
    /**
     * Description of the data schema in the array of `fields` structure (documented below).
     */
    fields?: outputs.DatatransferEndpointSettingsYdsSourceParserTskvParserDataSchemaFields;
    /**
     * Description of the data schema as JSON specification.
     */
    jsonFields?: string;
}

export interface DatatransferEndpointSettingsYdsSourceParserTskvParserDataSchemaFields {
    /**
     * Description of the data schema in the array of `fields` structure (documented below).
     */
    fields: outputs.DatatransferEndpointSettingsYdsSourceParserTskvParserDataSchemaFieldsField[];
}

export interface DatatransferEndpointSettingsYdsSourceParserTskvParserDataSchemaFieldsField {
    /**
     * Mark field as Primary Key.
     */
    key: boolean;
    /**
     * Field name.
     */
    name: string;
    /**
     * Path to the field.
     */
    path: string;
    /**
     * Mark field as required.
     */
    required: boolean;
    /**
     * Field type, one of: `INT64`, `INT32`, `INT16`, `INT8`, `UINT64`, `UINT32`, `UINT16`, `UINT8`, `DOUBLE`, `BOOLEAN`, `STRING`, `UTF8`, `ANY`, `DATETIME`.
     */
    type: string;
}

export interface DatatransferEndpointSettingsYdsTarget {
    /**
     * Database name.
     */
    database: string;
    /**
     * - (Optional) YDS Endpoint.
     */
    endpoint: string;
    /**
     * Not to split events queue into separate per-table queues.
     */
    saveTxOrder: boolean;
    /**
     * List of security groups that the transfer associated with this endpoint should use.
     */
    securityGroups: string[];
    /**
     * Data serialization settings.
     */
    serializer: outputs.DatatransferEndpointSettingsYdsTargetSerializer;
    /**
     * - (Required) Service account ID for interaction with database.
     */
    serviceAccountId: string;
    /**
     * - (Optional) Stream.
     */
    stream: string;
    /**
     * Identifier of the Yandex Cloud VPC subnetwork to user for accessing the database. If omitted, the server has to be accessible via Internet.
     */
    subnetId: string;
}

export interface DatatransferEndpointSettingsYdsTargetSerializer {
    /**
     * Empty block. Select data serialization format automatically.
     */
    serializerAuto?: outputs.DatatransferEndpointSettingsYdsTargetSerializerSerializerAuto;
    /**
     * Serialize data in json format. The structure is documented below.
     */
    serializerDebezium?: outputs.DatatransferEndpointSettingsYdsTargetSerializerSerializerDebezium;
    /**
     * Empty block. Serialize data in json format.
     */
    serializerJson?: outputs.DatatransferEndpointSettingsYdsTargetSerializerSerializerJson;
}

export interface DatatransferEndpointSettingsYdsTargetSerializerSerializerAuto {
}

export interface DatatransferEndpointSettingsYdsTargetSerializerSerializerDebezium {
    /**
     * A list of debezium parameters set by the structure of the `key` and `value` string fields.
     */
    serializerParameters: outputs.DatatransferEndpointSettingsYdsTargetSerializerSerializerDebeziumSerializerParameter[];
}

export interface DatatransferEndpointSettingsYdsTargetSerializerSerializerDebeziumSerializerParameter {
    /**
     * Mark field as Primary Key.
     */
    key: string;
    value: string;
}

export interface DatatransferEndpointSettingsYdsTargetSerializerSerializerJson {
}

export interface DatatransferTransferRuntime {
    /**
     * YC Runtime parameters for the transfer.
     */
    ycRuntime: outputs.DatatransferTransferRuntimeYcRuntime;
}

export interface DatatransferTransferRuntimeYcRuntime {
    /**
     * Number of workers in parallel replication.
     */
    jobCount: number;
    /**
     * Parallel snapshot parameters.
     */
    uploadShardParams: outputs.DatatransferTransferRuntimeYcRuntimeUploadShardParams;
}

export interface DatatransferTransferRuntimeYcRuntimeUploadShardParams {
    /**
     * Number of workers.
     */
    jobCount: number;
    /**
     * Number of threads.
     */
    processCount: number;
}

export interface DatatransferTransferTransformation {
    /**
     * A list of transformers. You can specify exactly 1 transformer in each element of list.
     */
    transformers?: outputs.DatatransferTransferTransformationTransformer[];
}

export interface DatatransferTransferTransformationTransformer {
    /**
     * Convert column values to strings.
     */
    convertToString?: outputs.DatatransferTransferTransformationTransformerConvertToString;
    /**
     * Set up a list of table columns to transfer.
     */
    filterColumns?: outputs.DatatransferTransferTransformationTransformerFilterColumns;
    /**
     * This filter only applies to transfers with queues (Apache Kafka®) as a data source. When running a transfer, only the strings meeting the specified criteria remain in a changefeed.
     */
    filterRows?: outputs.DatatransferTransferTransformationTransformerFilterRows;
    /**
     * Mask field transformer allows you to hash data.
     */
    maskField?: outputs.DatatransferTransferTransformationTransformerMaskField;
    /**
     * Set rules for renaming tables by specifying the current names of the tables in the source and new names for these tables in the target.
     */
    renameTables?: outputs.DatatransferTransferTransformationTransformerRenameTables;
    /**
     * Override primary keys.
     */
    replacePrimaryKey?: outputs.DatatransferTransferTransformationTransformerReplacePrimaryKey;
    /**
     * Set the number of shards for particular tables and a list of columns whose values will be used for calculating a hash to determine a shard.
     */
    sharderTransformer?: outputs.DatatransferTransferTransformationTransformerSharderTransformer;
    /**
     * Splits the X table into multiple tables (X_1, X_2, ..., X_n) based on data.
     */
    tableSplitterTransformer?: outputs.DatatransferTransferTransformationTransformerTableSplitterTransformer;
}

export interface DatatransferTransferTransformationTransformerConvertToString {
    /**
     * List of the columns to transfer to the target tables using lists of included and excluded columns (see block documentation below).
     */
    columns?: outputs.DatatransferTransferTransformationTransformerConvertToStringColumns;
    /**
     * Table filter (see block documentation below).
     */
    tables?: outputs.DatatransferTransferTransformationTransformerConvertToStringTables;
}

export interface DatatransferTransferTransformationTransformerConvertToStringColumns {
    /**
     * List of columns that will be excluded to transfer.
     */
    excludeColumns?: string[];
    /**
     * List of columns that will be included to transfer.
     */
    includeColumns?: string[];
}

export interface DatatransferTransferTransformationTransformerConvertToStringTables {
    /**
     * List of tables that will be excluded to transfer.
     */
    excludeTables?: string[];
    /**
     * List of tables that will be included to transfer.
     */
    includeTables?: string[];
}

export interface DatatransferTransferTransformationTransformerFilterColumns {
    /**
     * List of the columns to transfer to the target tables using lists of included and excluded columns (see block documentation below).
     */
    columns?: outputs.DatatransferTransferTransformationTransformerFilterColumnsColumns;
    /**
     * Table filter (see block documentation below).
     */
    tables?: outputs.DatatransferTransferTransformationTransformerFilterColumnsTables;
}

export interface DatatransferTransferTransformationTransformerFilterColumnsColumns {
    /**
     * List of columns that will be excluded to transfer.
     */
    excludeColumns?: string[];
    /**
     * List of columns that will be included to transfer.
     */
    includeColumns?: string[];
}

export interface DatatransferTransferTransformationTransformerFilterColumnsTables {
    /**
     * List of tables that will be excluded to transfer.
     */
    excludeTables?: string[];
    /**
     * List of tables that will be included to transfer.
     */
    includeTables?: string[];
}

export interface DatatransferTransferTransformationTransformerFilterRows {
    /**
     * Filtering criterion. This can be comparison operators for numeric, string, and Boolean values, comparison to NULL, and checking whether a substring is part of a string. Details here: https://cloud.yandex.com/en/docs/data-transfer/concepts/data-transformation#append-only-sources
     */
    filter?: string;
    /**
     * Table filter (see block documentation below).
     */
    tables?: outputs.DatatransferTransferTransformationTransformerFilterRowsTables;
}

export interface DatatransferTransferTransformationTransformerFilterRowsTables {
    /**
     * List of tables that will be excluded to transfer.
     */
    excludeTables?: string[];
    /**
     * List of tables that will be included to transfer.
     */
    includeTables?: string[];
}

export interface DatatransferTransferTransformationTransformerMaskField {
    /**
     * List of strings that specify the name of the column for data masking (a regular expression).
     */
    columns?: string[];
    /**
     * Mask function.
     */
    function?: outputs.DatatransferTransferTransformationTransformerMaskFieldFunction;
    /**
     * Table filter.
     */
    tables?: outputs.DatatransferTransferTransformationTransformerMaskFieldTables;
}

export interface DatatransferTransferTransformationTransformerMaskFieldFunction {
    /**
     * Hash mask function.
     */
    maskFunctionHash?: outputs.DatatransferTransferTransformationTransformerMaskFieldFunctionMaskFunctionHash;
}

export interface DatatransferTransferTransformationTransformerMaskFieldFunctionMaskFunctionHash {
    /**
     * This string will be used in the HMAC(sha256, salt) function applied to the column data.
     */
    userDefinedSalt?: string;
}

export interface DatatransferTransferTransformationTransformerMaskFieldTables {
    /**
     * List of tables that will be excluded to transfer.
     */
    excludeTables?: string[];
    /**
     * List of tables that will be included to transfer.
     */
    includeTables?: string[];
}

export interface DatatransferTransferTransformationTransformerRenameTables {
    /**
     * List of renaming rules.
     */
    renameTables?: outputs.DatatransferTransferTransformationTransformerRenameTablesRenameTable[];
}

export interface DatatransferTransferTransformationTransformerRenameTablesRenameTable {
    /**
     * Specify the new names for this table in the target.
     */
    newName?: outputs.DatatransferTransferTransformationTransformerRenameTablesRenameTableNewName;
    /**
     * Specify the current names of the table in the source.
     */
    originalName?: outputs.DatatransferTransferTransformationTransformerRenameTablesRenameTableOriginalName;
}

export interface DatatransferTransferTransformationTransformerRenameTablesRenameTableNewName {
    /**
     * Name of the transfer.
     */
    name?: string;
    nameSpace?: string;
}

export interface DatatransferTransferTransformationTransformerRenameTablesRenameTableOriginalName {
    /**
     * Name of the transfer.
     */
    name?: string;
    nameSpace?: string;
}

export interface DatatransferTransferTransformationTransformerReplacePrimaryKey {
    /**
     * List of columns to be used as primary keys.
     */
    keys?: string[];
    /**
     * Table filter (see block documentation below).
     */
    tables?: outputs.DatatransferTransferTransformationTransformerReplacePrimaryKeyTables;
}

export interface DatatransferTransferTransformationTransformerReplacePrimaryKeyTables {
    /**
     * List of tables that will be excluded to transfer.
     */
    excludeTables?: string[];
    /**
     * List of tables that will be included to transfer.
     */
    includeTables?: string[];
}

export interface DatatransferTransferTransformationTransformerSharderTransformer {
    /**
     * List of the columns to transfer to the target tables using lists of included and excluded columns (see block documentation below).
     */
    columns?: outputs.DatatransferTransferTransformationTransformerSharderTransformerColumns;
    /**
     * Number of shards.
     */
    shardsCount?: number;
    /**
     * Table filter (see block documentation below).
     */
    tables?: outputs.DatatransferTransferTransformationTransformerSharderTransformerTables;
}

export interface DatatransferTransferTransformationTransformerSharderTransformerColumns {
    /**
     * List of columns that will be excluded to transfer.
     */
    excludeColumns?: string[];
    /**
     * List of columns that will be included to transfer.
     */
    includeColumns?: string[];
}

export interface DatatransferTransferTransformationTransformerSharderTransformerTables {
    /**
     * List of tables that will be excluded to transfer.
     */
    excludeTables?: string[];
    /**
     * List of tables that will be included to transfer.
     */
    includeTables?: string[];
}

export interface DatatransferTransferTransformationTransformerTableSplitterTransformer {
    /**
     * List of strings that specify the columns in the tables to be partitioned.
     */
    columns?: string[];
    /**
     * Specify the split string to be used for merging components in a new table name.
     */
    splitter?: string;
    /**
     * Table filter (see block documentation below).
     */
    tables?: outputs.DatatransferTransferTransformationTransformerTableSplitterTransformerTables;
}

export interface DatatransferTransferTransformationTransformerTableSplitterTransformerTables {
    /**
     * List of tables that will be excluded to transfer.
     */
    excludeTables?: string[];
    /**
     * List of tables that will be included to transfer.
     */
    includeTables?: string[];
}

export interface FunctionAsyncInvocation {
    /**
     * Maximum number of retries for async invocation
     */
    retriesCount?: number;
    /**
     * Service account ID for Yandex Cloud Function
     */
    serviceAccountId?: string;
    /**
     * Target for unsuccessful async invocation
     */
    ymqFailureTarget?: outputs.FunctionAsyncInvocationYmqFailureTarget;
    /**
     * Target for successful async invocation
     */
    ymqSuccessTarget?: outputs.FunctionAsyncInvocationYmqSuccessTarget;
}

export interface FunctionAsyncInvocationYmqFailureTarget {
    /**
     * YMQ ARN
     */
    arn: string;
    /**
     * Service account ID for Yandex Cloud Function
     */
    serviceAccountId: string;
}

export interface FunctionAsyncInvocationYmqSuccessTarget {
    /**
     * YMQ ARN
     */
    arn: string;
    /**
     * Service account ID for Yandex Cloud Function
     */
    serviceAccountId: string;
}

export interface FunctionConnectivity {
    networkId: string;
}

export interface FunctionContent {
    zipFilename: string;
}

export interface FunctionLogOptions {
    /**
     * Is logging from function disabled
     */
    disabled?: boolean;
    /**
     * Folder ID for the Yandex Cloud Function
     */
    folderId?: string;
    /**
     * Log entries are written to specified log group
     */
    logGroupId?: string;
    /**
     * Minimum log entry level
     */
    minLevel?: string;
}

export interface FunctionPackage {
    bucketName: string;
    objectName: string;
    sha256?: string;
}

export interface FunctionScalingPolicyPolicy {
    tag: string;
    zoneInstancesLimit?: number;
    zoneRequestsLimit?: number;
}

export interface FunctionSecret {
    /**
     * (Required) Function's environment variable in which secret's value will be stored.
     */
    environmentVariable: string;
    /**
     * (Required) Secret's id.
     */
    id: string;
    /**
     * (Required) Secret's entries key which value will be stored in environment variable.
     */
    key: string;
    /**
     * (Required) Secret's version id.
     */
    versionId: string;
}

export interface FunctionStorageMount {
    /**
     * (Required) Name of the mounting bucket.
     */
    bucket: string;
    /**
     * (Required) Name of the mount point. The directory where the bucket is mounted will be accessible at the `/function/storage/<mount_point>` path.
     */
    mountPointName: string;
    /**
     * Prefix within the bucket. If you leave this field empty, the entire bucket will be mounted.
     */
    prefix?: string;
    /**
     * Mount the bucket in read-only mode.
     */
    readOnly?: boolean;
}

export interface FunctionTriggerContainer {
    id: string;
    path?: string;
    retryAttempts?: string;
    retryInterval?: string;
    serviceAccountId?: string;
}

export interface FunctionTriggerContainerRegistry {
    batchCutoff: string;
    batchSize?: string;
    createImage?: boolean;
    createImageTag?: boolean;
    deleteImage?: boolean;
    deleteImageTag?: boolean;
    imageName?: string;
    registryId: string;
    tag?: string;
}

export interface FunctionTriggerDataStreams {
    batchCutoff: string;
    batchSize?: string;
    database: string;
    serviceAccountId: string;
    streamName: string;
}

export interface FunctionTriggerDlq {
    queueId: string;
    serviceAccountId: string;
}

export interface FunctionTriggerFunction {
    id: string;
    retryAttempts?: string;
    retryInterval?: string;
    serviceAccountId?: string;
    tag?: string;
}

export interface FunctionTriggerIot {
    batchCutoff: string;
    batchSize?: string;
    deviceId?: string;
    registryId: string;
    topic?: string;
}

export interface FunctionTriggerLogGroup {
    batchCutoff: string;
    batchSize?: string;
    logGroupIds: string[];
}

export interface FunctionTriggerLogging {
    batchCutoff: string;
    batchSize?: string;
    groupId: string;
    levels?: string[];
    resourceIds?: string[];
    resourceTypes?: string[];
    streamNames?: string[];
}

export interface FunctionTriggerMail {
    attachmentsBucketId?: string;
    batchCutoff: string;
    batchSize?: string;
    serviceAccountId?: string;
}

export interface FunctionTriggerMessageQueue {
    batchCutoff: string;
    batchSize?: string;
    queueId: string;
    serviceAccountId: string;
    visibilityTimeout?: string;
}

export interface FunctionTriggerObjectStorage {
    batchCutoff: string;
    batchSize?: string;
    bucketId: string;
    create?: boolean;
    delete?: boolean;
    prefix?: string;
    suffix?: string;
    update?: boolean;
}

export interface FunctionTriggerTimer {
    cronExpression: string;
    payload?: string;
}

export interface GetAlbBackendGroupGrpcBackend {
    healthcheck: outputs.GetAlbBackendGroupGrpcBackendHealthcheck;
    loadBalancingConfig: outputs.GetAlbBackendGroupGrpcBackendLoadBalancingConfig;
    name: string;
    port: number;
    targetGroupIds: string[];
    tls: outputs.GetAlbBackendGroupGrpcBackendTls;
    weight: number;
}

export interface GetAlbBackendGroupGrpcBackendHealthcheck {
    grpcHealthcheck: outputs.GetAlbBackendGroupGrpcBackendHealthcheckGrpcHealthcheck;
    healthcheckPort: number;
    healthyThreshold: number;
    httpHealthcheck: outputs.GetAlbBackendGroupGrpcBackendHealthcheckHttpHealthcheck;
    interval: string;
    intervalJitterPercent: number;
    streamHealthcheck: outputs.GetAlbBackendGroupGrpcBackendHealthcheckStreamHealthcheck;
    timeout: string;
    unhealthyThreshold: number;
}

export interface GetAlbBackendGroupGrpcBackendHealthcheckGrpcHealthcheck {
    serviceName: string;
}

export interface GetAlbBackendGroupGrpcBackendHealthcheckHttpHealthcheck {
    host: string;
    http2: boolean;
    path: string;
}

export interface GetAlbBackendGroupGrpcBackendHealthcheckStreamHealthcheck {
    receive: string;
    send: string;
}

export interface GetAlbBackendGroupGrpcBackendLoadBalancingConfig {
    localityAwareRoutingPercent: number;
    mode: string;
    panicThreshold: number;
    strictLocality: boolean;
}

export interface GetAlbBackendGroupGrpcBackendTls {
    sni: string;
    validationContext: outputs.GetAlbBackendGroupGrpcBackendTlsValidationContext;
}

export interface GetAlbBackendGroupGrpcBackendTlsValidationContext {
    trustedCaBytes: string;
    trustedCaId: string;
}

export interface GetAlbBackendGroupHttpBackend {
    healthcheck: outputs.GetAlbBackendGroupHttpBackendHealthcheck;
    http2: boolean;
    loadBalancingConfig: outputs.GetAlbBackendGroupHttpBackendLoadBalancingConfig;
    name: string;
    port: number;
    storageBucket: string;
    targetGroupIds: string[];
    tls: outputs.GetAlbBackendGroupHttpBackendTls;
    weight: number;
}

export interface GetAlbBackendGroupHttpBackendHealthcheck {
    grpcHealthcheck: outputs.GetAlbBackendGroupHttpBackendHealthcheckGrpcHealthcheck;
    healthcheckPort: number;
    healthyThreshold: number;
    httpHealthcheck: outputs.GetAlbBackendGroupHttpBackendHealthcheckHttpHealthcheck;
    interval: string;
    intervalJitterPercent: number;
    streamHealthcheck: outputs.GetAlbBackendGroupHttpBackendHealthcheckStreamHealthcheck;
    timeout: string;
    unhealthyThreshold: number;
}

export interface GetAlbBackendGroupHttpBackendHealthcheckGrpcHealthcheck {
    serviceName: string;
}

export interface GetAlbBackendGroupHttpBackendHealthcheckHttpHealthcheck {
    host: string;
    http2: boolean;
    path: string;
}

export interface GetAlbBackendGroupHttpBackendHealthcheckStreamHealthcheck {
    receive: string;
    send: string;
}

export interface GetAlbBackendGroupHttpBackendLoadBalancingConfig {
    localityAwareRoutingPercent: number;
    mode: string;
    panicThreshold: number;
    strictLocality: boolean;
}

export interface GetAlbBackendGroupHttpBackendTls {
    sni: string;
    validationContext: outputs.GetAlbBackendGroupHttpBackendTlsValidationContext;
}

export interface GetAlbBackendGroupHttpBackendTlsValidationContext {
    trustedCaBytes: string;
    trustedCaId: string;
}

export interface GetAlbBackendGroupSessionAffinity {
    /**
     * IP address affinity
     */
    connection: outputs.GetAlbBackendGroupSessionAffinityConnection;
    /**
     * Cookie affinity
     */
    cookie: outputs.GetAlbBackendGroupSessionAffinityCookie;
    /**
     * Request header affinity
     */
    header: outputs.GetAlbBackendGroupSessionAffinityHeader;
}

export interface GetAlbBackendGroupSessionAffinityConnection {
    /**
     * Use source IP address
     */
    sourceIp: boolean;
}

export interface GetAlbBackendGroupSessionAffinityCookie {
    /**
     * Name of the HTTP cookie
     */
    name: string;
    /**
     * TTL for the cookie (if not set, session cookie will be used)
     */
    ttl: string;
}

export interface GetAlbBackendGroupSessionAffinityHeader {
    /**
     * The name of the request header that will be used
     */
    headerName: string;
}

export interface GetAlbBackendGroupStreamBackend {
    enableProxyProtocol: boolean;
    healthcheck: outputs.GetAlbBackendGroupStreamBackendHealthcheck;
    loadBalancingConfig: outputs.GetAlbBackendGroupStreamBackendLoadBalancingConfig;
    name: string;
    port: number;
    targetGroupIds: string[];
    tls: outputs.GetAlbBackendGroupStreamBackendTls;
    weight: number;
}

export interface GetAlbBackendGroupStreamBackendHealthcheck {
    grpcHealthcheck: outputs.GetAlbBackendGroupStreamBackendHealthcheckGrpcHealthcheck;
    healthcheckPort: number;
    healthyThreshold: number;
    httpHealthcheck: outputs.GetAlbBackendGroupStreamBackendHealthcheckHttpHealthcheck;
    interval: string;
    intervalJitterPercent: number;
    streamHealthcheck: outputs.GetAlbBackendGroupStreamBackendHealthcheckStreamHealthcheck;
    timeout: string;
    unhealthyThreshold: number;
}

export interface GetAlbBackendGroupStreamBackendHealthcheckGrpcHealthcheck {
    serviceName: string;
}

export interface GetAlbBackendGroupStreamBackendHealthcheckHttpHealthcheck {
    host: string;
    http2: boolean;
    path: string;
}

export interface GetAlbBackendGroupStreamBackendHealthcheckStreamHealthcheck {
    receive: string;
    send: string;
}

export interface GetAlbBackendGroupStreamBackendLoadBalancingConfig {
    localityAwareRoutingPercent: number;
    mode: string;
    panicThreshold: number;
    strictLocality: boolean;
}

export interface GetAlbBackendGroupStreamBackendTls {
    sni: string;
    validationContext: outputs.GetAlbBackendGroupStreamBackendTlsValidationContext;
}

export interface GetAlbBackendGroupStreamBackendTlsValidationContext {
    trustedCaBytes: string;
    trustedCaId: string;
}

export interface GetAlbHttpRouterRouteOption {
    rbacs: outputs.GetAlbHttpRouterRouteOptionRbac[];
    securityProfileId?: string;
}

export interface GetAlbHttpRouterRouteOptionRbac {
    action: string;
    principals: outputs.GetAlbHttpRouterRouteOptionRbacPrincipal[];
}

export interface GetAlbHttpRouterRouteOptionRbacPrincipal {
    andPrincipals: outputs.GetAlbHttpRouterRouteOptionRbacPrincipalAndPrincipal[];
}

export interface GetAlbHttpRouterRouteOptionRbacPrincipalAndPrincipal {
    any: boolean;
    headers: outputs.GetAlbHttpRouterRouteOptionRbacPrincipalAndPrincipalHeader[];
    remoteIp: string;
}

export interface GetAlbHttpRouterRouteOptionRbacPrincipalAndPrincipalHeader {
    name: string;
    values: outputs.GetAlbHttpRouterRouteOptionRbacPrincipalAndPrincipalHeaderValue[];
}

export interface GetAlbHttpRouterRouteOptionRbacPrincipalAndPrincipalHeaderValue {
    exact: string;
    prefix: string;
    regex: string;
}

export interface GetAlbLoadBalancerAllocationPolicy {
    locations: outputs.GetAlbLoadBalancerAllocationPolicyLocation[];
}

export interface GetAlbLoadBalancerAllocationPolicyLocation {
    disableTraffic: boolean;
    subnetId: string;
    zoneId: string;
}

export interface GetAlbLoadBalancerListener {
    endpoints: outputs.GetAlbLoadBalancerListenerEndpoint[];
    https?: outputs.GetAlbLoadBalancerListenerHttp[];
    name: string;
    stream?: outputs.GetAlbLoadBalancerListenerStream;
    tls?: outputs.GetAlbLoadBalancerListenerTl[];
}

export interface GetAlbLoadBalancerListenerEndpoint {
    addresses: outputs.GetAlbLoadBalancerListenerEndpointAddress[];
    ports: number[];
}

export interface GetAlbLoadBalancerListenerEndpointAddress {
    externalIpv4Addresses: outputs.GetAlbLoadBalancerListenerEndpointAddressExternalIpv4Address[];
    externalIpv6Addresses: outputs.GetAlbLoadBalancerListenerEndpointAddressExternalIpv6Address[];
    internalIpv4Addresses: outputs.GetAlbLoadBalancerListenerEndpointAddressInternalIpv4Address[];
}

export interface GetAlbLoadBalancerListenerEndpointAddressExternalIpv4Address {
    address: string;
}

export interface GetAlbLoadBalancerListenerEndpointAddressExternalIpv6Address {
    address: string;
}

export interface GetAlbLoadBalancerListenerEndpointAddressInternalIpv4Address {
    address: string;
    subnetId: string;
}

export interface GetAlbLoadBalancerListenerHttp {
    handlers?: outputs.GetAlbLoadBalancerListenerHttpHandler[];
    redirects?: outputs.GetAlbLoadBalancerListenerHttpRedirect[];
}

export interface GetAlbLoadBalancerListenerHttpHandler {
    allowHttp10?: boolean;
    http2Options: outputs.GetAlbLoadBalancerListenerHttpHandlerHttp2Option[];
    httpRouterId: string;
    rewriteRequestId: boolean;
}

export interface GetAlbLoadBalancerListenerHttpHandlerHttp2Option {
    maxConcurrentStreams: number;
}

export interface GetAlbLoadBalancerListenerHttpRedirect {
    httpToHttps: boolean;
}

export interface GetAlbLoadBalancerListenerStream {
    handlers?: outputs.GetAlbLoadBalancerListenerStreamHandler[];
}

export interface GetAlbLoadBalancerListenerStreamHandler {
    backendGroupId: string;
}

export interface GetAlbLoadBalancerListenerTl {
    defaultHandlers: outputs.GetAlbLoadBalancerListenerTlDefaultHandler[];
    sniHandlers: outputs.GetAlbLoadBalancerListenerTlSniHandler[];
}

export interface GetAlbLoadBalancerListenerTlDefaultHandler {
    certificateIds: string[];
    httpHandlers?: outputs.GetAlbLoadBalancerListenerTlDefaultHandlerHttpHandler[];
    streamHandlers?: outputs.GetAlbLoadBalancerListenerTlDefaultHandlerStreamHandler[];
}

export interface GetAlbLoadBalancerListenerTlDefaultHandlerHttpHandler {
    allowHttp10?: boolean;
    http2Options: outputs.GetAlbLoadBalancerListenerTlDefaultHandlerHttpHandlerHttp2Option[];
    httpRouterId: string;
    rewriteRequestId: boolean;
}

export interface GetAlbLoadBalancerListenerTlDefaultHandlerHttpHandlerHttp2Option {
    maxConcurrentStreams: number;
}

export interface GetAlbLoadBalancerListenerTlDefaultHandlerStreamHandler {
    backendGroupId: string;
}

export interface GetAlbLoadBalancerListenerTlSniHandler {
    handlers: outputs.GetAlbLoadBalancerListenerTlSniHandlerHandler[];
    name: string;
    serverNames: string[];
}

export interface GetAlbLoadBalancerListenerTlSniHandlerHandler {
    certificateIds: string[];
    httpHandlers?: outputs.GetAlbLoadBalancerListenerTlSniHandlerHandlerHttpHandler[];
    streamHandlers?: outputs.GetAlbLoadBalancerListenerTlSniHandlerHandlerStreamHandler[];
}

export interface GetAlbLoadBalancerListenerTlSniHandlerHandlerHttpHandler {
    allowHttp10?: boolean;
    http2Options: outputs.GetAlbLoadBalancerListenerTlSniHandlerHandlerHttpHandlerHttp2Option[];
    httpRouterId: string;
    rewriteRequestId: boolean;
}

export interface GetAlbLoadBalancerListenerTlSniHandlerHandlerHttpHandlerHttp2Option {
    maxConcurrentStreams: number;
}

export interface GetAlbLoadBalancerListenerTlSniHandlerHandlerStreamHandler {
    backendGroupId: string;
}

export interface GetAlbLoadBalancerLogOption {
    disable: boolean;
    discardRules: outputs.GetAlbLoadBalancerLogOptionDiscardRule[];
    logGroupId: string;
}

export interface GetAlbLoadBalancerLogOptionDiscardRule {
    discardPercent: number;
    grpcCodes: string[];
    httpCodeIntervals: string[];
    httpCodes: number[];
}

export interface GetAlbTargetGroupTarget {
    ipAddress: string;
    privateIpv4Address?: boolean;
    subnetId?: string;
}

export interface GetAlbVirtualHostModifyRequestHeader {
    append: string;
    name: string;
    remove: boolean;
    replace: string;
}

export interface GetAlbVirtualHostModifyResponseHeader {
    append: string;
    name: string;
    remove: boolean;
    replace: string;
}

export interface GetAlbVirtualHostRoute {
    grpcRoutes: outputs.GetAlbVirtualHostRouteGrpcRoute[];
    httpRoutes: outputs.GetAlbVirtualHostRouteHttpRoute[];
    name: string;
    routeOptions: outputs.GetAlbVirtualHostRouteRouteOption[];
}

export interface GetAlbVirtualHostRouteGrpcRoute {
    grpcMatches: outputs.GetAlbVirtualHostRouteGrpcRouteGrpcMatch[];
    grpcRouteActions: outputs.GetAlbVirtualHostRouteGrpcRouteGrpcRouteAction[];
    grpcStatusResponseActions: outputs.GetAlbVirtualHostRouteGrpcRouteGrpcStatusResponseAction[];
}

export interface GetAlbVirtualHostRouteGrpcRouteGrpcMatch {
    fqmns: outputs.GetAlbVirtualHostRouteGrpcRouteGrpcMatchFqmn[];
}

export interface GetAlbVirtualHostRouteGrpcRouteGrpcMatchFqmn {
    exact: string;
    prefix: string;
    regex: string;
}

export interface GetAlbVirtualHostRouteGrpcRouteGrpcRouteAction {
    autoHostRewrite: boolean;
    backendGroupId: string;
    hostRewrite: string;
    idleTimeout: string;
    maxTimeout: string;
}

export interface GetAlbVirtualHostRouteGrpcRouteGrpcStatusResponseAction {
    status: string;
}

export interface GetAlbVirtualHostRouteHttpRoute {
    directResponseActions: outputs.GetAlbVirtualHostRouteHttpRouteDirectResponseAction[];
    httpMatches: outputs.GetAlbVirtualHostRouteHttpRouteHttpMatch[];
    httpRouteActions: outputs.GetAlbVirtualHostRouteHttpRouteHttpRouteAction[];
    redirectActions: outputs.GetAlbVirtualHostRouteHttpRouteRedirectAction[];
}

export interface GetAlbVirtualHostRouteHttpRouteDirectResponseAction {
    body: string;
    status: number;
}

export interface GetAlbVirtualHostRouteHttpRouteHttpMatch {
    httpMethods: string[];
    paths: outputs.GetAlbVirtualHostRouteHttpRouteHttpMatchPath[];
}

export interface GetAlbVirtualHostRouteHttpRouteHttpMatchPath {
    exact: string;
    prefix: string;
    regex: string;
}

export interface GetAlbVirtualHostRouteHttpRouteHttpRouteAction {
    autoHostRewrite: boolean;
    backendGroupId: string;
    hostRewrite: string;
    idleTimeout: string;
    prefixRewrite: string;
    timeout: string;
    upgradeTypes: string[];
}

export interface GetAlbVirtualHostRouteHttpRouteRedirectAction {
    removeQuery: boolean;
    replaceHost: string;
    replacePath: string;
    replacePort: number;
    replacePrefix: string;
    replaceScheme: string;
    responseCode: string;
}

export interface GetAlbVirtualHostRouteOption {
    rbacs: outputs.GetAlbVirtualHostRouteOptionRbac[];
    securityProfileId?: string;
}

export interface GetAlbVirtualHostRouteOptionRbac {
    action: string;
    principals: outputs.GetAlbVirtualHostRouteOptionRbacPrincipal[];
}

export interface GetAlbVirtualHostRouteOptionRbacPrincipal {
    andPrincipals: outputs.GetAlbVirtualHostRouteOptionRbacPrincipalAndPrincipal[];
}

export interface GetAlbVirtualHostRouteOptionRbacPrincipalAndPrincipal {
    any: boolean;
    headers: outputs.GetAlbVirtualHostRouteOptionRbacPrincipalAndPrincipalHeader[];
    remoteIp: string;
}

export interface GetAlbVirtualHostRouteOptionRbacPrincipalAndPrincipalHeader {
    name: string;
    values: outputs.GetAlbVirtualHostRouteOptionRbacPrincipalAndPrincipalHeaderValue[];
}

export interface GetAlbVirtualHostRouteOptionRbacPrincipalAndPrincipalHeaderValue {
    exact: string;
    prefix: string;
    regex: string;
}

export interface GetAlbVirtualHostRouteRouteOption {
    rbacs: outputs.GetAlbVirtualHostRouteRouteOptionRbac[];
    securityProfileId?: string;
}

export interface GetAlbVirtualHostRouteRouteOptionRbac {
    action: string;
    principals: outputs.GetAlbVirtualHostRouteRouteOptionRbacPrincipal[];
}

export interface GetAlbVirtualHostRouteRouteOptionRbacPrincipal {
    andPrincipals: outputs.GetAlbVirtualHostRouteRouteOptionRbacPrincipalAndPrincipal[];
}

export interface GetAlbVirtualHostRouteRouteOptionRbacPrincipalAndPrincipal {
    any: boolean;
    headers: outputs.GetAlbVirtualHostRouteRouteOptionRbacPrincipalAndPrincipalHeader[];
    remoteIp: string;
}

export interface GetAlbVirtualHostRouteRouteOptionRbacPrincipalAndPrincipalHeader {
    name: string;
    values: outputs.GetAlbVirtualHostRouteRouteOptionRbacPrincipalAndPrincipalHeaderValue[];
}

export interface GetAlbVirtualHostRouteRouteOptionRbacPrincipalAndPrincipalHeaderValue {
    exact: string;
    prefix: string;
    regex: string;
}

export interface GetApiGatewayCanary {
    variables?: {[key: string]: string};
    weight?: number;
}

export interface GetApiGatewayConnectivity {
    networkId: string;
}

export interface GetApiGatewayCustomDomain {
    certificateId: string;
    domainId: string;
    fqdn: string;
}

export interface GetApiGatewayLogOption {
    disabled: boolean;
    folderId: string;
    logGroupId: string;
    minLevel: string;
}

export interface GetAuditTrailsTrailDataStreamDestination {
    databaseId: string;
    streamName: string;
}

export interface GetAuditTrailsTrailFilter {
    eventFilters: outputs.GetAuditTrailsTrailFilterEventFilter[];
    pathFilters: outputs.GetAuditTrailsTrailFilterPathFilter[];
}

export interface GetAuditTrailsTrailFilterEventFilter {
    categories: outputs.GetAuditTrailsTrailFilterEventFilterCategory[];
    pathFilters: outputs.GetAuditTrailsTrailFilterEventFilterPathFilter[];
    service: string;
}

export interface GetAuditTrailsTrailFilterEventFilterCategory {
    plane: string;
    type: string;
}

export interface GetAuditTrailsTrailFilterEventFilterPathFilter {
    anyFilters: outputs.GetAuditTrailsTrailFilterEventFilterPathFilterAnyFilter[];
    someFilters: outputs.GetAuditTrailsTrailFilterEventFilterPathFilterSomeFilter[];
}

export interface GetAuditTrailsTrailFilterEventFilterPathFilterAnyFilter {
    resourceId: string;
    resourceType: string;
}

export interface GetAuditTrailsTrailFilterEventFilterPathFilterSomeFilter {
    anyFilters: outputs.GetAuditTrailsTrailFilterEventFilterPathFilterSomeFilterAnyFilter[];
    resourceId: string;
    resourceType: string;
}

export interface GetAuditTrailsTrailFilterEventFilterPathFilterSomeFilterAnyFilter {
    resourceId: string;
    resourceType: string;
}

export interface GetAuditTrailsTrailFilterPathFilter {
    anyFilters: outputs.GetAuditTrailsTrailFilterPathFilterAnyFilter[];
    someFilters: outputs.GetAuditTrailsTrailFilterPathFilterSomeFilter[];
}

export interface GetAuditTrailsTrailFilterPathFilterAnyFilter {
    resourceId: string;
    resourceType: string;
}

export interface GetAuditTrailsTrailFilterPathFilterSomeFilter {
    anyFilters: outputs.GetAuditTrailsTrailFilterPathFilterSomeFilterAnyFilter[];
    resourceId: string;
    resourceType: string;
}

export interface GetAuditTrailsTrailFilterPathFilterSomeFilterAnyFilter {
    resourceId: string;
    resourceType: string;
}

export interface GetAuditTrailsTrailLoggingDestination {
    logGroupId: string;
}

export interface GetAuditTrailsTrailStorageDestination {
    bucketName: string;
    objectPrefix: string;
}

export interface GetBackupPolicyReattempt {
    enabled: boolean;
    interval: string;
    maxAttempts: number;
}

export interface GetBackupPolicyRetention {
    afterBackup: boolean;
    rules: outputs.GetBackupPolicyRetentionRule[];
}

export interface GetBackupPolicyRetentionRule {
    maxAge?: string;
    maxCount?: number;
    repeatPeriods?: string[];
}

export interface GetBackupPolicyScheduling {
    backupSets?: outputs.GetBackupPolicySchedulingBackupSet[];
    enabled: boolean;
    maxParallelBackups: number;
    randomMaxDelay: string;
    scheme: string;
    weeklyBackupDay: string;
}

export interface GetBackupPolicySchedulingBackupSet {
    executeByInterval: number;
    executeByTimes: outputs.GetBackupPolicySchedulingBackupSetExecuteByTime[];
    type: string;
}

export interface GetBackupPolicySchedulingBackupSetExecuteByTime {
    includeLastDayOfMonth: boolean;
    monthdays: number[];
    months: number[];
    repeatAts: string[];
    repeatEvery: string;
    type: string;
    weekdays: string[];
}

export interface GetBackupPolicyVmSnapshotReattempt {
    enabled: boolean;
    interval: string;
    maxAttempts: number;
}

export interface GetCdnOriginGroupOrigin {
    backup?: boolean;
    enabled?: boolean;
    originGroupId: number;
    source: string;
}

export interface GetCdnResourceOptions {
    allowedHttpMethods: string[];
    browserCacheSettings: number;
    cacheHttpHeaders: string[];
    cors: string[];
    customHostHeader: string;
    customServerName: string;
    disableCache: boolean;
    disableProxyForceRanges: boolean;
    edgeCacheSettings: number;
    enableIpUrlSigning: boolean;
    fetchedCompressed: boolean;
    forwardHostHeader: boolean;
    gzipOn: boolean;
    ignoreCookie: boolean;
    ignoreQueryParams: boolean;
    ipAddressAcl: outputs.GetCdnResourceOptionsIpAddressAcl;
    proxyCacheMethodsSet: boolean;
    queryParamsBlacklists: string[];
    queryParamsWhitelists: string[];
    redirectHttpToHttps: boolean;
    redirectHttpsToHttp: boolean;
    secureKey: string;
    slice: boolean;
    staticRequestHeaders: {[key: string]: string};
    staticResponseHeaders: {[key: string]: string};
}

export interface GetCdnResourceOptionsIpAddressAcl {
    exceptedValues: string[];
    policyType: string;
}

export interface GetCdnResourceSslCertificate {
    certificateManagerId?: string;
    status: string;
    type: string;
}

export interface GetCmCertificateChallenge {
    createdAt: string;
    dnsName: string;
    dnsType: string;
    dnsValue: string;
    domain: string;
    httpContent: string;
    httpUrl: string;
    message: string;
    type: string;
    updatedAt: string;
}

export interface GetComputeDiskDiskPlacementPolicy {
    diskPlacementGroupId: string;
}

export interface GetComputeInstanceBootDisk {
    autoDelete: boolean;
    deviceName: string;
    diskId: string;
    initializeParams: outputs.GetComputeInstanceBootDiskInitializeParam[];
    mode: string;
}

export interface GetComputeInstanceBootDiskInitializeParam {
    blockSize: number;
    description: string;
    imageId: string;
    name: string;
    size: number;
    snapshotId: string;
    type: string;
}

export interface GetComputeInstanceFilesystem {
    deviceName: string;
    filesystemId: string;
    mode: string;
}

export interface GetComputeInstanceGroupAllocationPolicy {
    instanceTagsPools: outputs.GetComputeInstanceGroupAllocationPolicyInstanceTagsPool[];
    zones: string[];
}

export interface GetComputeInstanceGroupAllocationPolicyInstanceTagsPool {
    tags: string[];
    zone: string;
}

export interface GetComputeInstanceGroupApplicationBalancerState {
    statusMessage: string;
    targetGroupId: string;
}

export interface GetComputeInstanceGroupApplicationLoadBalancer {
    ignoreHealthChecks: boolean;
    maxOpeningTrafficDuration: number;
    statusMessage: string;
    targetGroupDescription: string;
    targetGroupId: string;
    targetGroupLabels: {[key: string]: string};
    targetGroupName: string;
}

export interface GetComputeInstanceGroupDeployPolicy {
    maxCreating: number;
    maxDeleting: number;
    maxExpansion: number;
    maxUnavailable: number;
    startupDuration: number;
    strategy: string;
}

export interface GetComputeInstanceGroupHealthCheck {
    healthyThreshold: number;
    httpOptions: outputs.GetComputeInstanceGroupHealthCheckHttpOption[];
    interval: number;
    tcpOptions: outputs.GetComputeInstanceGroupHealthCheckTcpOption[];
    timeout: number;
    unhealthyThreshold: number;
}

export interface GetComputeInstanceGroupHealthCheckHttpOption {
    path: string;
    port: number;
}

export interface GetComputeInstanceGroupHealthCheckTcpOption {
    port: number;
}

export interface GetComputeInstanceGroupInstance {
    fqdn: string;
    instanceId: string;
    instanceTag: string;
    name: string;
    networkInterfaces: outputs.GetComputeInstanceGroupInstanceNetworkInterface[];
    status: string;
    statusChangedAt: string;
    statusMessage: string;
    zoneId: string;
}

export interface GetComputeInstanceGroupInstanceNetworkInterface {
    index: number;
    ipAddress: string;
    ipv4: boolean;
    ipv6: boolean;
    ipv6Address: string;
    macAddress: string;
    nat: boolean;
    natIpAddress: string;
    natIpVersion: string;
    subnetId: string;
}

export interface GetComputeInstanceGroupInstanceTemplate {
    bootDisks: outputs.GetComputeInstanceGroupInstanceTemplateBootDisk[];
    description: string;
    filesystems?: outputs.GetComputeInstanceGroupInstanceTemplateFilesystem[];
    hostname: string;
    labels: {[key: string]: string};
    metadata: {[key: string]: string};
    metadataOptions: outputs.GetComputeInstanceGroupInstanceTemplateMetadataOptions;
    name: string;
    networkInterfaces: outputs.GetComputeInstanceGroupInstanceTemplateNetworkInterface[];
    networkSettings: outputs.GetComputeInstanceGroupInstanceTemplateNetworkSetting[];
    placementPolicy?: outputs.GetComputeInstanceGroupInstanceTemplatePlacementPolicy;
    platformId: string;
    resources: outputs.GetComputeInstanceGroupInstanceTemplateResource[];
    schedulingPolicies: outputs.GetComputeInstanceGroupInstanceTemplateSchedulingPolicy[];
    secondaryDisks: outputs.GetComputeInstanceGroupInstanceTemplateSecondaryDisk[];
    serviceAccountId: string;
}

export interface GetComputeInstanceGroupInstanceTemplateBootDisk {
    deviceName: string;
    diskId: string;
    initializeParams: outputs.GetComputeInstanceGroupInstanceTemplateBootDiskInitializeParam[];
    mode: string;
    name: string;
}

export interface GetComputeInstanceGroupInstanceTemplateBootDiskInitializeParam {
    description: string;
    imageId: string;
    size: number;
    snapshotId: string;
    type: string;
}

export interface GetComputeInstanceGroupInstanceTemplateFilesystem {
    deviceName: string;
    filesystemId: string;
    mode: string;
}

export interface GetComputeInstanceGroupInstanceTemplateMetadataOptions {
    awsV1HttpEndpoint: number;
    awsV1HttpToken: number;
    gceHttpEndpoint: number;
    gceHttpToken: number;
}

export interface GetComputeInstanceGroupInstanceTemplateNetworkInterface {
    dnsRecords: outputs.GetComputeInstanceGroupInstanceTemplateNetworkInterfaceDnsRecord[];
    ipAddress: string;
    ipv4: boolean;
    ipv6: boolean;
    ipv6Address: string;
    ipv6DnsRecords: outputs.GetComputeInstanceGroupInstanceTemplateNetworkInterfaceIpv6DnsRecord[];
    nat: boolean;
    natDnsRecords: outputs.GetComputeInstanceGroupInstanceTemplateNetworkInterfaceNatDnsRecord[];
    natIpAddress: string;
    networkId: string;
    securityGroupIds: string[];
    subnetIds: string[];
}

export interface GetComputeInstanceGroupInstanceTemplateNetworkInterfaceDnsRecord {
    dnsZoneId: string;
    fqdn: string;
    ptr: boolean;
    ttl: number;
}

export interface GetComputeInstanceGroupInstanceTemplateNetworkInterfaceIpv6DnsRecord {
    dnsZoneId: string;
    fqdn: string;
    ptr: boolean;
    ttl: number;
}

export interface GetComputeInstanceGroupInstanceTemplateNetworkInterfaceNatDnsRecord {
    dnsZoneId: string;
    fqdn: string;
    ptr: boolean;
    ttl: number;
}

export interface GetComputeInstanceGroupInstanceTemplateNetworkSetting {
    type: string;
}

export interface GetComputeInstanceGroupInstanceTemplatePlacementPolicy {
    placementGroupId: string;
}

export interface GetComputeInstanceGroupInstanceTemplateResource {
    coreFraction: number;
    cores: number;
    gpus: number;
    memory: number;
}

export interface GetComputeInstanceGroupInstanceTemplateSchedulingPolicy {
    preemptible: boolean;
}

export interface GetComputeInstanceGroupInstanceTemplateSecondaryDisk {
    deviceName: string;
    diskId: string;
    initializeParams: outputs.GetComputeInstanceGroupInstanceTemplateSecondaryDiskInitializeParam[];
    mode: string;
    name: string;
}

export interface GetComputeInstanceGroupInstanceTemplateSecondaryDiskInitializeParam {
    description: string;
    imageId: string;
    size: number;
    snapshotId: string;
    type: string;
}

export interface GetComputeInstanceGroupLoadBalancer {
    ignoreHealthChecks: boolean;
    maxOpeningTrafficDuration: number;
    statusMessage: string;
    targetGroupDescription: string;
    targetGroupId: string;
    targetGroupLabels: {[key: string]: string};
    targetGroupName: string;
}

export interface GetComputeInstanceGroupLoadBalancerState {
    statusMessage: string;
    targetGroupId: string;
}

export interface GetComputeInstanceGroupScalePolicy {
    autoScales: outputs.GetComputeInstanceGroupScalePolicyAutoScale[];
    fixedScales: outputs.GetComputeInstanceGroupScalePolicyFixedScale[];
    testAutoScales: outputs.GetComputeInstanceGroupScalePolicyTestAutoScale[];
}

export interface GetComputeInstanceGroupScalePolicyAutoScale {
    autoScaleType: string;
    cpuUtilizationTarget: number;
    customRules: outputs.GetComputeInstanceGroupScalePolicyAutoScaleCustomRule[];
    initialSize: number;
    maxSize: number;
    measurementDuration: number;
    minZoneSize: number;
    stabilizationDuration: number;
    warmupDuration: number;
}

export interface GetComputeInstanceGroupScalePolicyAutoScaleCustomRule {
    folderId: string;
    labels: {[key: string]: string};
    metricName: string;
    metricType: string;
    ruleType: string;
    service: string;
    target: number;
}

export interface GetComputeInstanceGroupScalePolicyFixedScale {
    size: number;
}

export interface GetComputeInstanceGroupScalePolicyTestAutoScale {
    autoScaleType: string;
    cpuUtilizationTarget: number;
    customRules: outputs.GetComputeInstanceGroupScalePolicyTestAutoScaleCustomRule[];
    initialSize: number;
    maxSize: number;
    measurementDuration: number;
    minZoneSize: number;
    stabilizationDuration: number;
    warmupDuration: number;
}

export interface GetComputeInstanceGroupScalePolicyTestAutoScaleCustomRule {
    folderId: string;
    labels: {[key: string]: string};
    metricName: string;
    metricType: string;
    ruleType: string;
    service: string;
    target: number;
}

export interface GetComputeInstanceLocalDisk {
    deviceName: string;
    sizeBytes: number;
}

export interface GetComputeInstanceMetadataOptions {
    awsV1HttpEndpoint: number;
    awsV1HttpToken: number;
    gceHttpEndpoint: number;
    gceHttpToken: number;
}

export interface GetComputeInstanceNetworkInterface {
    dnsRecords: outputs.GetComputeInstanceNetworkInterfaceDnsRecord[];
    index: number;
    ipAddress: string;
    ipv4: boolean;
    ipv6: boolean;
    ipv6Address: string;
    ipv6DnsRecords: outputs.GetComputeInstanceNetworkInterfaceIpv6DnsRecord[];
    macAddress: string;
    nat: boolean;
    natDnsRecords: outputs.GetComputeInstanceNetworkInterfaceNatDnsRecord[];
    natIpAddress: string;
    natIpVersion: string;
    securityGroupIds: string[];
    subnetId: string;
}

export interface GetComputeInstanceNetworkInterfaceDnsRecord {
    dnsZoneId: string;
    fqdn: string;
    ptr: boolean;
    ttl: number;
}

export interface GetComputeInstanceNetworkInterfaceIpv6DnsRecord {
    dnsZoneId: string;
    fqdn: string;
    ptr: boolean;
    ttl: number;
}

export interface GetComputeInstanceNetworkInterfaceNatDnsRecord {
    dnsZoneId: string;
    fqdn: string;
    ptr: boolean;
    ttl: number;
}

export interface GetComputeInstancePlacementPolicy {
    hostAffinityRules: outputs.GetComputeInstancePlacementPolicyHostAffinityRule[];
    placementGroupId?: string;
    placementGroupPartition?: number;
}

export interface GetComputeInstancePlacementPolicyHostAffinityRule {
    key: string;
    op: string;
    values: string[];
}

export interface GetComputeInstanceResource {
    coreFraction: number;
    cores: number;
    gpus: number;
    memory: number;
}

export interface GetComputeInstanceSchedulingPolicy {
    preemptible?: boolean;
}

export interface GetComputeInstanceSecondaryDisk {
    autoDelete: boolean;
    deviceName: string;
    diskId: string;
    mode: string;
}

export interface GetComputeSnapshotScheduleSchedulePolicy {
    expression: string;
    startAt: string;
}

export interface GetComputeSnapshotScheduleSnapshotSpec {
    description: string;
    labels: {[key: string]: string};
}

export interface GetContainerRepositoryLifecyclePolicyRule {
    description: string;
    expirePeriod: string;
    retainedTop: number;
    tagRegexp: string;
    untagged: boolean;
}

export interface GetDataprocClusterClusterConfig {
    hadoops: outputs.GetDataprocClusterClusterConfigHadoop[];
    subclusterSpecs: outputs.GetDataprocClusterClusterConfigSubclusterSpec[];
    versionId: string;
}

export interface GetDataprocClusterClusterConfigHadoop {
    initializationActions: outputs.GetDataprocClusterClusterConfigHadoopInitializationAction[];
    properties: {[key: string]: string};
    services: string[];
    sshPublicKeys: string[];
}

export interface GetDataprocClusterClusterConfigHadoopInitializationAction {
    args: string[];
    timeout: string;
    uri: string;
}

export interface GetDataprocClusterClusterConfigSubclusterSpec {
    assignPublicIp: boolean;
    autoscalingConfigs: outputs.GetDataprocClusterClusterConfigSubclusterSpecAutoscalingConfig[];
    hostsCount: number;
    id: string;
    name: string;
    resources: outputs.GetDataprocClusterClusterConfigSubclusterSpecResource[];
    role: string;
    subnetId: string;
}

export interface GetDataprocClusterClusterConfigSubclusterSpecAutoscalingConfig {
    cpuUtilizationTarget: string;
    decommissionTimeout: string;
    maxHostsCount: number;
    measurementDuration: string;
    preemptible: boolean;
    stabilizationDuration: string;
    warmupDuration: string;
}

export interface GetDataprocClusterClusterConfigSubclusterSpecResource {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetFunctionAsyncInvocation {
    retriesCount: number;
    serviceAccountId: string;
    ymqFailureTargets: outputs.GetFunctionAsyncInvocationYmqFailureTarget[];
    ymqSuccessTargets: outputs.GetFunctionAsyncInvocationYmqSuccessTarget[];
}

export interface GetFunctionAsyncInvocationYmqFailureTarget {
    arn: string;
    serviceAccountId: string;
}

export interface GetFunctionAsyncInvocationYmqSuccessTarget {
    arn: string;
    serviceAccountId: string;
}

export interface GetFunctionConnectivity {
    networkId: string;
}

export interface GetFunctionLogOption {
    disabled: boolean;
    folderId: string;
    logGroupId: string;
    minLevel: string;
}

export interface GetFunctionScalingPolicyPolicy {
    tag: string;
    zoneInstancesLimit?: number;
    zoneRequestsLimit?: number;
}

export interface GetFunctionSecret {
    environmentVariable: string;
    id: string;
    key: string;
    versionId: string;
}

export interface GetFunctionStorageMount {
    bucket: string;
    mountPointName: string;
    prefix?: string;
    readOnly?: boolean;
}

export interface GetFunctionTriggerContainer {
    id: string;
    path: string;
    retryAttempts: string;
    retryInterval: string;
    serviceAccountId: string;
}

export interface GetFunctionTriggerContainerRegistry {
    batchCutoff: string;
    batchSize: string;
    createImage: boolean;
    createImageTag: boolean;
    deleteImage: boolean;
    deleteImageTag: boolean;
    imageName: string;
    registryId: string;
    tag: string;
}

export interface GetFunctionTriggerDataStream {
    batchCutoff: string;
    batchSize: string;
    database: string;
    serviceAccountId: boolean;
    streamName: string;
    suffix: string;
}

export interface GetFunctionTriggerDlq {
    queueId: string;
    serviceAccountId: string;
}

export interface GetFunctionTriggerFunction {
    id: string;
    retryAttempts: string;
    retryInterval: string;
    serviceAccountId: string;
    tag: string;
}

export interface GetFunctionTriggerIot {
    batchCutoff: string;
    batchSize: string;
    deviceId: string;
    registryId: string;
    topic: string;
}

export interface GetFunctionTriggerLogGroup {
    batchCutoff: string;
    batchSize: string;
    logGroupIds: string[];
}

export interface GetFunctionTriggerLogging {
    batchCutoff: string;
    batchSize: string;
    groupId: string;
    levels: string[];
    resourceIds: string[];
    resourceTypes: string[];
    streamNames: string[];
}

export interface GetFunctionTriggerMail {
    attachmentsBucketId: string;
    batchCutoff: string;
    batchSize: string;
    serviceAccountId: string;
}

export interface GetFunctionTriggerMessageQueue {
    batchCutoff: string;
    batchSize: string;
    queueId: string;
    serviceAccountId: string;
    visibilityTimeout: string;
}

export interface GetFunctionTriggerObjectStorage {
    batchCutoff: string;
    batchSize: string;
    bucketId: string;
    create: boolean;
    delete: boolean;
    prefix: string;
    suffix: string;
    update: boolean;
}

export interface GetFunctionTriggerTimer {
    cronExpression: string;
    payload: string;
}

export interface GetIamPolicyBinding {
    members: string[];
    role: string;
}

export interface GetIotCoreBrokerLogOption {
    disabled: boolean;
    folderId: string;
    logGroupId: string;
    minLevel: string;
}

export interface GetIotCoreRegistryLogOption {
    disabled: boolean;
    folderId: string;
    logGroupId: string;
    minLevel: string;
}

export interface GetKubernetesClusterKmsProvider {
    keyId: string;
}

export interface GetKubernetesClusterMaster {
    clusterCaCertificate: string;
    etcdClusterSize: number;
    externalV4Address: string;
    externalV4Endpoint: string;
    externalV6Address: string;
    externalV6Endpoint: string;
    internalV4Address: string;
    internalV4Endpoint: string;
    maintenancePolicies: outputs.GetKubernetesClusterMasterMaintenancePolicy[];
    masterLocations: outputs.GetKubernetesClusterMasterMasterLocation[];
    masterLoggings: outputs.GetKubernetesClusterMasterMasterLogging[];
    publicIp: boolean;
    regionals: outputs.GetKubernetesClusterMasterRegional[];
    securityGroupIds: string[];
    version: string;
    versionInfos: outputs.GetKubernetesClusterMasterVersionInfo[];
    zonals: outputs.GetKubernetesClusterMasterZonal[];
}

export interface GetKubernetesClusterMasterMaintenancePolicy {
    autoUpgrade: boolean;
    maintenanceWindows: outputs.GetKubernetesClusterMasterMaintenancePolicyMaintenanceWindow[];
}

export interface GetKubernetesClusterMasterMaintenancePolicyMaintenanceWindow {
    day: string;
    duration: string;
    startTime: string;
}

export interface GetKubernetesClusterMasterMasterLocation {
    subnetId: string;
    zone: string;
}

export interface GetKubernetesClusterMasterMasterLogging {
    auditEnabled: boolean;
    clusterAutoscalerEnabled: boolean;
    enabled: boolean;
    eventsEnabled: boolean;
    folderId: string;
    kubeApiserverEnabled: boolean;
    logGroupId: string;
}

export interface GetKubernetesClusterMasterRegional {
    region: string;
}

export interface GetKubernetesClusterMasterVersionInfo {
    currentVersion: string;
    newRevisionAvailable: boolean;
    newRevisionSummary: string;
    versionDeprecated: boolean;
}

export interface GetKubernetesClusterMasterZonal {
    zone: string;
}

export interface GetKubernetesClusterNetworkImplementation {
    cilia: outputs.GetKubernetesClusterNetworkImplementationCilium[];
}

export interface GetKubernetesClusterNetworkImplementationCilium {
    routingMode: string;
}

export interface GetKubernetesNodeGroupAllocationPolicy {
    locations: outputs.GetKubernetesNodeGroupAllocationPolicyLocation[];
}

export interface GetKubernetesNodeGroupAllocationPolicyLocation {
    subnetId: string;
    zone: string;
}

export interface GetKubernetesNodeGroupDeployPolicy {
    maxExpansion: number;
    maxUnavailable: number;
}

export interface GetKubernetesNodeGroupInstanceTemplate {
    bootDisks: outputs.GetKubernetesNodeGroupInstanceTemplateBootDisk[];
    containerNetworks: outputs.GetKubernetesNodeGroupInstanceTemplateContainerNetwork[];
    containerRuntime: outputs.GetKubernetesNodeGroupInstanceTemplateContainerRuntime;
    gpuSettings: outputs.GetKubernetesNodeGroupInstanceTemplateGpuSetting[];
    labels: {[key: string]: string};
    metadata: {[key: string]: string};
    name: string;
    nat: boolean;
    networkAccelerationType: string;
    networkInterfaces: outputs.GetKubernetesNodeGroupInstanceTemplateNetworkInterface[];
    placementPolicies?: outputs.GetKubernetesNodeGroupInstanceTemplatePlacementPolicy[];
    platformId: string;
    resources: outputs.GetKubernetesNodeGroupInstanceTemplateResource[];
    schedulingPolicies: outputs.GetKubernetesNodeGroupInstanceTemplateSchedulingPolicy[];
}

export interface GetKubernetesNodeGroupInstanceTemplateBootDisk {
    size: number;
    type: string;
}

export interface GetKubernetesNodeGroupInstanceTemplateContainerNetwork {
    podMtu: number;
}

export interface GetKubernetesNodeGroupInstanceTemplateContainerRuntime {
    type: string;
}

export interface GetKubernetesNodeGroupInstanceTemplateGpuSetting {
    gpuClusterId: string;
    gpuEnvironment: string;
}

export interface GetKubernetesNodeGroupInstanceTemplateNetworkInterface {
    ipv4: boolean;
    ipv4DnsRecords: outputs.GetKubernetesNodeGroupInstanceTemplateNetworkInterfaceIpv4DnsRecord[];
    ipv6: boolean;
    ipv6DnsRecords: outputs.GetKubernetesNodeGroupInstanceTemplateNetworkInterfaceIpv6DnsRecord[];
    nat: boolean;
    securityGroupIds: string[];
    subnetIds: string[];
}

export interface GetKubernetesNodeGroupInstanceTemplateNetworkInterfaceIpv4DnsRecord {
    dnsZoneId: string;
    fqdn: string;
    ptr: boolean;
    ttl: number;
}

export interface GetKubernetesNodeGroupInstanceTemplateNetworkInterfaceIpv6DnsRecord {
    dnsZoneId: string;
    fqdn: string;
    ptr: boolean;
    ttl: number;
}

export interface GetKubernetesNodeGroupInstanceTemplatePlacementPolicy {
    placementGroupId: string;
}

export interface GetKubernetesNodeGroupInstanceTemplateResource {
    coreFraction: number;
    cores: number;
    gpus: number;
    memory: number;
}

export interface GetKubernetesNodeGroupInstanceTemplateSchedulingPolicy {
    preemptible: boolean;
}

export interface GetKubernetesNodeGroupMaintenancePolicy {
    autoRepair: boolean;
    autoUpgrade: boolean;
    maintenanceWindows: outputs.GetKubernetesNodeGroupMaintenancePolicyMaintenanceWindow[];
}

export interface GetKubernetesNodeGroupMaintenancePolicyMaintenanceWindow {
    day: string;
    duration: string;
    startTime: string;
}

export interface GetKubernetesNodeGroupScalePolicy {
    autoScales: outputs.GetKubernetesNodeGroupScalePolicyAutoScale[];
    fixedScales: outputs.GetKubernetesNodeGroupScalePolicyFixedScale[];
}

export interface GetKubernetesNodeGroupScalePolicyAutoScale {
    initial: number;
    max: number;
    min: number;
}

export interface GetKubernetesNodeGroupScalePolicyFixedScale {
    size: number;
}

export interface GetKubernetesNodeGroupVersionInfo {
    currentVersion: string;
    newRevisionAvailable: boolean;
    newRevisionSummary: string;
    versionDeprecated: boolean;
}

export interface GetLbNetworkLoadBalancerAttachedTargetGroup {
    healthchecks: outputs.GetLbNetworkLoadBalancerAttachedTargetGroupHealthcheck[];
    targetGroupId: string;
}

export interface GetLbNetworkLoadBalancerAttachedTargetGroupHealthcheck {
    healthyThreshold: number;
    httpOptions: outputs.GetLbNetworkLoadBalancerAttachedTargetGroupHealthcheckHttpOption[];
    interval: number;
    name: string;
    tcpOptions: outputs.GetLbNetworkLoadBalancerAttachedTargetGroupHealthcheckTcpOption[];
    timeout: number;
    unhealthyThreshold: number;
}

export interface GetLbNetworkLoadBalancerAttachedTargetGroupHealthcheckHttpOption {
    path: string;
    port: number;
}

export interface GetLbNetworkLoadBalancerAttachedTargetGroupHealthcheckTcpOption {
    port: number;
}

export interface GetLbNetworkLoadBalancerListener {
    externalAddressSpecs: outputs.GetLbNetworkLoadBalancerListenerExternalAddressSpec[];
    internalAddressSpecs: outputs.GetLbNetworkLoadBalancerListenerInternalAddressSpec[];
    name: string;
    port: number;
    protocol: string;
    targetPort: number;
}

export interface GetLbNetworkLoadBalancerListenerExternalAddressSpec {
    address: string;
    ipVersion: string;
}

export interface GetLbNetworkLoadBalancerListenerInternalAddressSpec {
    address: string;
    ipVersion: string;
    subnetId: string;
}

export interface GetLbTargetGroupTarget {
    address: string;
    subnetId: string;
}

export interface GetLoadtestingAgentComputeInstance {
    bootDisks: outputs.GetLoadtestingAgentComputeInstanceBootDisk[];
    computedMetadata: {[key: string]: string};
    labels: {[key: string]: string};
    metadata: {[key: string]: string};
    networkInterfaces: outputs.GetLoadtestingAgentComputeInstanceNetworkInterface[];
    platformId: string;
    resources: outputs.GetLoadtestingAgentComputeInstanceResource[];
    serviceAccountId: string;
    zoneId: string;
}

export interface GetLoadtestingAgentComputeInstanceBootDisk {
    autoDelete: boolean;
    deviceName: string;
    diskId: string;
    initializeParams: outputs.GetLoadtestingAgentComputeInstanceBootDiskInitializeParam[];
}

export interface GetLoadtestingAgentComputeInstanceBootDiskInitializeParam {
    blockSize: number;
    description: string;
    name: string;
    size: number;
    type: string;
}

export interface GetLoadtestingAgentComputeInstanceNetworkInterface {
    index: number;
    ipAddress: string;
    ipv4: boolean;
    ipv6: boolean;
    ipv6Address: string;
    macAddress: string;
    nat: boolean;
    natIpAddress: string;
    natIpVersion: string;
    securityGroupIds: string[];
    subnetId: string;
}

export interface GetLoadtestingAgentComputeInstanceResource {
    coreFraction: number;
    cores: number;
    memory: number;
}

export interface GetLockboxSecretCurrentVersion {
    createdAt: string;
    description: string;
    destroyAt: string;
    id: string;
    payloadEntryKeys: string[];
    secretId: string;
    status: string;
}

export interface GetLockboxSecretVersionEntry {
    key: string;
    textValue: string;
}

export interface GetMdbClickhouseClusterAccess {
    dataLens?: boolean;
    dataTransfer?: boolean;
    metrika?: boolean;
    serverless?: boolean;
    webSql?: boolean;
    yandexQuery?: boolean;
}

export interface GetMdbClickhouseClusterBackupWindowStart {
    hours?: number;
    minutes?: number;
}

export interface GetMdbClickhouseClusterClickhouse {
    config: outputs.GetMdbClickhouseClusterClickhouseConfig;
    resources: outputs.GetMdbClickhouseClusterClickhouseResources;
}

export interface GetMdbClickhouseClusterClickhouseConfig {
    backgroundFetchesPoolSize: number;
    backgroundMergesMutationsConcurrencyRatio: number;
    backgroundMessageBrokerSchedulePoolSize: number;
    backgroundPoolSize: number;
    backgroundSchedulePoolSize: number;
    compressions?: outputs.GetMdbClickhouseClusterClickhouseConfigCompression[];
    defaultDatabase: string;
    dictionariesLazyLoad: boolean;
    geobaseEnabled: boolean;
    geobaseUri: string;
    graphiteRollups?: outputs.GetMdbClickhouseClusterClickhouseConfigGraphiteRollup[];
    kafka: outputs.GetMdbClickhouseClusterClickhouseConfigKafka;
    kafkaTopics?: outputs.GetMdbClickhouseClusterClickhouseConfigKafkaTopic[];
    keepAliveTimeout: number;
    logLevel: string;
    markCacheSize: number;
    maxConcurrentQueries: number;
    maxConnections: number;
    maxPartitionSizeToDrop: number;
    maxTableSizeToDrop: number;
    mergeTree: outputs.GetMdbClickhouseClusterClickhouseConfigMergeTree;
    metricLogEnabled: boolean;
    metricLogRetentionSize: number;
    metricLogRetentionTime: number;
    partLogRetentionSize: number;
    partLogRetentionTime: number;
    queryCache: outputs.GetMdbClickhouseClusterClickhouseConfigQueryCache;
    queryLogRetentionSize: number;
    queryLogRetentionTime: number;
    queryMaskingRules?: outputs.GetMdbClickhouseClusterClickhouseConfigQueryMaskingRule[];
    queryThreadLogEnabled: boolean;
    queryThreadLogRetentionSize: number;
    queryThreadLogRetentionTime: number;
    rabbitmq: outputs.GetMdbClickhouseClusterClickhouseConfigRabbitmq;
    textLogEnabled: boolean;
    textLogLevel: string;
    textLogRetentionSize: number;
    textLogRetentionTime: number;
    timezone: string;
    totalMemoryProfilerStep: number;
    traceLogEnabled: boolean;
    traceLogRetentionSize: number;
    traceLogRetentionTime: number;
    uncompressedCacheSize: number;
}

export interface GetMdbClickhouseClusterClickhouseConfigCompression {
    level?: number;
    method?: string;
    minPartSize?: number;
    minPartSizeRatio?: number;
}

export interface GetMdbClickhouseClusterClickhouseConfigGraphiteRollup {
    name?: string;
    pathColumnName: string;
    patterns?: outputs.GetMdbClickhouseClusterClickhouseConfigGraphiteRollupPattern[];
    timeColumnName: string;
    valueColumnName: string;
    versionColumnName: string;
}

export interface GetMdbClickhouseClusterClickhouseConfigGraphiteRollupPattern {
    function?: string;
    regexp: string;
    retentions?: outputs.GetMdbClickhouseClusterClickhouseConfigGraphiteRollupPatternRetention[];
}

export interface GetMdbClickhouseClusterClickhouseConfigGraphiteRollupPatternRetention {
    age?: number;
    precision?: number;
}

export interface GetMdbClickhouseClusterClickhouseConfigKafka {
    autoOffsetReset: string;
    debug: string;
    enableSslCertificateVerification: boolean;
    maxPollIntervalMs: number;
    saslMechanism: string;
    saslPassword: string;
    saslUsername: string;
    securityProtocol: string;
    sessionTimeoutMs: number;
}

export interface GetMdbClickhouseClusterClickhouseConfigKafkaTopic {
    name?: string;
    settings?: outputs.GetMdbClickhouseClusterClickhouseConfigKafkaTopicSettings;
}

export interface GetMdbClickhouseClusterClickhouseConfigKafkaTopicSettings {
    autoOffsetReset: string;
    debug: string;
    enableSslCertificateVerification: boolean;
    maxPollIntervalMs: number;
    saslMechanism?: string;
    saslPassword?: string;
    saslUsername?: string;
    securityProtocol?: string;
    sessionTimeoutMs: number;
}

export interface GetMdbClickhouseClusterClickhouseConfigMergeTree {
    checkSampleColumnIsCorrect: boolean;
    cleanupDelayPeriod: number;
    maxAvgPartSizeForTooManyParts: number;
    maxBytesToMergeAtMinSpaceInPool: number;
    maxCleanupDelayPeriod: number;
    maxMergeSelectingSleepMs: number;
    maxNumberOfMergesWithTtlInPool: number;
    maxPartsInTotal: number;
    maxReplicatedMergesInQueue: number;
    mergeMaxBlockSize: number;
    mergeSelectingSleepMs: number;
    mergeWithRecompressionTtlTimeout: number;
    mergeWithTtlTimeout: number;
    minAgeToForceMergeOnPartitionOnly: boolean;
    minAgeToForceMergeSeconds: number;
    minBytesForWidePart: number;
    minRowsForWidePart: number;
    numberOfFreeEntriesInPoolToLowerMaxSizeOfMerge: number;
    partsToDelayInsert: number;
    partsToThrowInsert: number;
    replicatedDeduplicationWindow: number;
    replicatedDeduplicationWindowSeconds: number;
    ttlOnlyDropParts: boolean;
}

export interface GetMdbClickhouseClusterClickhouseConfigQueryCache {
    maxEntries: number;
    maxEntrySizeInBytes: number;
    maxEntrySizeInRows: number;
    maxSizeInBytes: number;
}

export interface GetMdbClickhouseClusterClickhouseConfigQueryMaskingRule {
    name: string;
    regexp?: string;
    replace: string;
}

export interface GetMdbClickhouseClusterClickhouseConfigRabbitmq {
    password: string;
    username: string;
    vhost: string;
}

export interface GetMdbClickhouseClusterClickhouseResources {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbClickhouseClusterCloudStorage {
    dataCacheEnabled: boolean;
    dataCacheMaxSize: number;
    enabled?: boolean;
    moveFactor: number;
    preferNotToMerge: boolean;
}

export interface GetMdbClickhouseClusterDatabase {
    name?: string;
}

export interface GetMdbClickhouseClusterFormatSchema {
    name?: string;
    type?: string;
    uri?: string;
}

export interface GetMdbClickhouseClusterHost {
    assignPublicIp?: boolean;
    fqdn: string;
    shardName: string;
    subnetId: string;
    type?: string;
    zone?: string;
}

export interface GetMdbClickhouseClusterMaintenanceWindow {
    day?: string;
    hour?: number;
    type?: string;
}

export interface GetMdbClickhouseClusterMlModel {
    name?: string;
    type?: string;
    uri?: string;
}

export interface GetMdbClickhouseClusterShard {
    name?: string;
    resources: outputs.GetMdbClickhouseClusterShardResources;
    weight: number;
}

export interface GetMdbClickhouseClusterShardGroup {
    description?: string;
    name?: string;
    shardNames?: string[];
}

export interface GetMdbClickhouseClusterShardResources {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbClickhouseClusterUser {
    name?: string;
    password?: string;
    permissions: outputs.GetMdbClickhouseClusterUserPermission[];
    quotas: outputs.GetMdbClickhouseClusterUserQuota[];
    settings: outputs.GetMdbClickhouseClusterUserSettings;
}

export interface GetMdbClickhouseClusterUserPermission {
    databaseName?: string;
}

export interface GetMdbClickhouseClusterUserQuota {
    errors: number;
    executionTime: number;
    intervalDuration?: number;
    queries: number;
    readRows: number;
    resultRows: number;
}

export interface GetMdbClickhouseClusterUserSettings {
    addHttpCorsHeader: boolean;
    allowDdl: boolean;
    allowIntrospectionFunctions: boolean;
    allowSuspiciousLowCardinalityTypes: boolean;
    asyncInsert: boolean;
    asyncInsertBusyTimeout: number;
    asyncInsertMaxDataSize: number;
    asyncInsertStaleTimeout: number;
    asyncInsertThreads: number;
    cancelHttpReadonlyQueriesOnClientClose: boolean;
    compile: boolean;
    compileExpressions: boolean;
    connectTimeout: number;
    connectTimeoutWithFailover: number;
    countDistinctImplementation: string;
    distinctOverflowMode: string;
    distributedAggregationMemoryEfficient: boolean;
    distributedDdlTaskTimeout: number;
    distributedProductMode: string;
    emptyResultForAggregationByEmptySet: boolean;
    enableHttpCompression: boolean;
    fallbackToStaleReplicasForDistributedQueries: boolean;
    flattenNested: boolean;
    forceIndexByDate: boolean;
    forcePrimaryKey: boolean;
    groupByOverflowMode: string;
    groupByTwoLevelThreshold: number;
    groupByTwoLevelThresholdBytes: number;
    hedgedConnectionTimeoutMs: number;
    httpConnectionTimeout: number;
    httpHeadersProgressInterval: number;
    httpReceiveTimeout: number;
    httpSendTimeout: number;
    idleConnectionTimeout: number;
    inputFormatDefaultsForOmittedFields: boolean;
    inputFormatImportNestedJson: boolean;
    inputFormatParallelParsing: boolean;
    inputFormatValuesInterpretExpressions: boolean;
    insertKeeperMaxRetries: number;
    insertNullAsDefault: boolean;
    insertQuorum: number;
    insertQuorumTimeout: number;
    joinOverflowMode: string;
    joinUseNulls: boolean;
    joinedSubqueryRequiresAlias: boolean;
    loadBalancing: string;
    localFilesystemReadMethod: string;
    logQueryThreads: boolean;
    lowCardinalityAllowInNativeFormat: boolean;
    maxAstDepth: number;
    maxAstElements: number;
    maxBlockSize: number;
    maxBytesBeforeExternalGroupBy: number;
    maxBytesBeforeExternalSort: number;
    maxBytesInDistinct: number;
    maxBytesInJoin: number;
    maxBytesInSet: number;
    maxBytesToRead: number;
    maxBytesToSort: number;
    maxBytesToTransfer: number;
    maxColumnsToRead: number;
    maxConcurrentQueriesForUser: number;
    maxExecutionTime: number;
    maxExpandedAstElements: number;
    maxFinalThreads: number;
    maxHttpGetRedirects: number;
    maxInsertBlockSize: number;
    maxInsertThreads: number;
    maxMemoryUsage: number;
    maxMemoryUsageForUser: number;
    maxNetworkBandwidth: number;
    maxNetworkBandwidthForUser: number;
    maxParserDepth: number;
    maxQuerySize: number;
    maxReadBufferSize: number;
    maxReplicaDelayForDistributedQueries: number;
    maxResultBytes: number;
    maxResultRows: number;
    maxRowsInDistinct: number;
    maxRowsInJoin: number;
    maxRowsInSet: number;
    maxRowsToGroupBy: number;
    maxRowsToRead: number;
    maxRowsToSort: number;
    maxRowsToTransfer: number;
    maxTemporaryColumns: number;
    maxTemporaryDataOnDiskSizeForQuery: number;
    maxTemporaryDataOnDiskSizeForUser: number;
    maxTemporaryNonConstColumns: number;
    maxThreads: number;
    memoryOvercommitRatioDenominator: number;
    memoryOvercommitRatioDenominatorForUser: number;
    memoryProfilerSampleProbability: number;
    memoryProfilerStep: number;
    memoryUsageOvercommitMaxWaitMicroseconds: number;
    mergeTreeMaxBytesToUseCache: number;
    mergeTreeMaxRowsToUseCache: number;
    mergeTreeMinBytesForConcurrentRead: number;
    mergeTreeMinRowsForConcurrentRead: number;
    minBytesToUseDirectIo: number;
    minCountToCompile: number;
    minCountToCompileExpression: number;
    minExecutionSpeed: number;
    minExecutionSpeedBytes: number;
    minInsertBlockSizeBytes: number;
    minInsertBlockSizeRows: number;
    outputFormatJsonQuote64bitIntegers: boolean;
    outputFormatJsonQuoteDenormals: boolean;
    preferLocalhostReplica: boolean;
    priority: number;
    quotaMode: string;
    readOverflowMode: string;
    readonly: number;
    receiveTimeout: number;
    remoteFilesystemReadMethod: string;
    replicationAlterPartitionsSync: number;
    resultOverflowMode: string;
    selectSequentialConsistency: boolean;
    sendProgressInHttpHeaders: boolean;
    sendTimeout: number;
    setOverflowMode: string;
    skipUnavailableShards: boolean;
    sortOverflowMode: string;
    timeoutBeforeCheckingExecutionSpeed: number;
    timeoutOverflowMode: string;
    transferOverflowMode: string;
    transformNullIn: boolean;
    useHedgedRequests: boolean;
    useUncompressedCache: boolean;
    waitForAsyncInsert: boolean;
    waitForAsyncInsertTimeout: number;
}

export interface GetMdbClickhouseClusterZookeeper {
    resources: outputs.GetMdbClickhouseClusterZookeeperResources;
}

export interface GetMdbClickhouseClusterZookeeperResources {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbElasticsearchClusterConfig {
    adminPassword: string;
    dataNodes: outputs.GetMdbElasticsearchClusterConfigDataNode[];
    edition: string;
    masterNode: outputs.GetMdbElasticsearchClusterConfigMasterNode;
    plugins: string[];
    version: string;
}

export interface GetMdbElasticsearchClusterConfigDataNode {
    resources: outputs.GetMdbElasticsearchClusterConfigDataNodeResource[];
}

export interface GetMdbElasticsearchClusterConfigDataNodeResource {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbElasticsearchClusterConfigMasterNode {
    resources: outputs.GetMdbElasticsearchClusterConfigMasterNodeResource[];
}

export interface GetMdbElasticsearchClusterConfigMasterNodeResource {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbElasticsearchClusterHost {
    assignPublicIp: boolean;
    fqdn: string;
    name: string;
    subnetId: string;
    type: string;
    zone: string;
}

export interface GetMdbElasticsearchClusterMaintenanceWindow {
    day: string;
    hour: number;
    type: string;
}

export interface GetMdbGreenplumClusterAccess {
    dataLens: boolean;
    dataTransfer: boolean;
    webSql: boolean;
}

export interface GetMdbGreenplumClusterBackupWindowStart {
    hours: number;
    minutes: number;
}

export interface GetMdbGreenplumClusterCloudStorage {
    enable: boolean;
}

export interface GetMdbGreenplumClusterMaintenanceWindow {
    day: string;
    hour: number;
    type: string;
}

export interface GetMdbGreenplumClusterMasterHost {
    assignPublicIp: boolean;
    fqdn: string;
}

export interface GetMdbGreenplumClusterMasterSubcluster {
    resources: outputs.GetMdbGreenplumClusterMasterSubclusterResource[];
}

export interface GetMdbGreenplumClusterMasterSubclusterResource {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbGreenplumClusterPoolerConfig {
    poolClientIdleTimeout?: number;
    poolSize?: number;
    poolingMode?: string;
}

export interface GetMdbGreenplumClusterPxfConfig {
    connectionTimeout?: number;
    maxThreads?: number;
    poolAllowCoreThreadTimeout?: boolean;
    poolCoreSize?: number;
    poolMaxSize?: number;
    poolQueueCapacity?: number;
    uploadTimeout?: number;
    xms?: number;
    xmx?: number;
}

export interface GetMdbGreenplumClusterSegmentHost {
    fqdn: string;
}

export interface GetMdbGreenplumClusterSegmentSubcluster {
    resources: outputs.GetMdbGreenplumClusterSegmentSubclusterResource[];
}

export interface GetMdbGreenplumClusterSegmentSubclusterResource {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbKafkaClusterAccess {
    dataTransfer: boolean;
}

export interface GetMdbKafkaClusterConfig {
    access: outputs.GetMdbKafkaClusterConfigAccess;
    assignPublicIp?: boolean;
    brokersCount?: number;
    diskSizeAutoscaling: outputs.GetMdbKafkaClusterConfigDiskSizeAutoscaling;
    kafka: outputs.GetMdbKafkaClusterConfigKafka;
    schemaRegistry?: boolean;
    /**
     * @deprecated The 'unmanaged_topics' field has been deprecated, because feature enabled permanently and can't be disabled.
     */
    unmanagedTopics?: boolean;
    version: string;
    zones: string[];
    zookeeper: outputs.GetMdbKafkaClusterConfigZookeeper;
}

export interface GetMdbKafkaClusterConfigAccess {
    dataTransfer?: boolean;
}

export interface GetMdbKafkaClusterConfigDiskSizeAutoscaling {
    diskSizeLimit: number;
    emergencyUsageThreshold?: number;
    plannedUsageThreshold?: number;
}

export interface GetMdbKafkaClusterConfigKafka {
    kafkaConfig?: outputs.GetMdbKafkaClusterConfigKafkaKafkaConfig;
    resources: outputs.GetMdbKafkaClusterConfigKafkaResources;
}

export interface GetMdbKafkaClusterConfigKafkaKafkaConfig {
    autoCreateTopicsEnable?: boolean;
    compressionType?: string;
    defaultReplicationFactor?: string;
    logFlushIntervalMessages?: string;
    logFlushIntervalMs?: string;
    logFlushSchedulerIntervalMs?: string;
    logPreallocate?: boolean;
    logRetentionBytes?: string;
    logRetentionHours?: string;
    logRetentionMinutes?: string;
    logRetentionMs?: string;
    logSegmentBytes?: string;
    messageMaxBytes?: string;
    numPartitions?: string;
    offsetsRetentionMinutes?: string;
    replicaFetchMaxBytes?: string;
    saslEnabledMechanisms?: string[];
    socketReceiveBufferBytes?: string;
    socketSendBufferBytes?: string;
    sslCipherSuites?: string[];
}

export interface GetMdbKafkaClusterConfigKafkaResources {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbKafkaClusterConfigZookeeper {
    resources: outputs.GetMdbKafkaClusterConfigZookeeperResources;
}

export interface GetMdbKafkaClusterConfigZookeeperResources {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbKafkaClusterDiskSizeAutoscaling {
    diskSizeLimit: number;
    emergencyUsageThreshold: number;
    plannedUsageThreshold: number;
}

export interface GetMdbKafkaClusterHost {
    assignPublicIp: boolean;
    health: string;
    name: string;
    role: string;
    subnetId: string;
    zoneId: string;
}

export interface GetMdbKafkaClusterMaintenanceWindow {
    day: string;
    hour: number;
    type: string;
}

export interface GetMdbKafkaClusterTopic {
    name: string;
    partitions: number;
    replicationFactor: number;
    topicConfig?: outputs.GetMdbKafkaClusterTopicTopicConfig;
}

export interface GetMdbKafkaClusterTopicTopicConfig {
    cleanupPolicy?: string;
    compressionType?: string;
    deleteRetentionMs?: string;
    fileDeleteDelayMs?: string;
    flushMessages?: string;
    flushMs?: string;
    maxMessageBytes?: string;
    minCompactionLagMs?: string;
    minInsyncReplicas?: string;
    preallocate?: boolean;
    retentionBytes?: string;
    retentionMs?: string;
    segmentBytes?: string;
}

export interface GetMdbKafkaClusterUser {
    name: string;
    password: string;
    permissions?: outputs.GetMdbKafkaClusterUserPermission[];
}

export interface GetMdbKafkaClusterUserPermission {
    allowHosts?: string[];
    role: string;
    topicName: string;
}

export interface GetMdbKafkaConnectorConnectorConfigMirrormaker {
    replicationFactor: number;
    sourceClusters: outputs.GetMdbKafkaConnectorConnectorConfigMirrormakerSourceCluster[];
    targetClusters: outputs.GetMdbKafkaConnectorConnectorConfigMirrormakerTargetCluster[];
    topics: string;
}

export interface GetMdbKafkaConnectorConnectorConfigMirrormakerSourceCluster {
    alias: string;
    externalClusters: outputs.GetMdbKafkaConnectorConnectorConfigMirrormakerSourceClusterExternalCluster[];
    thisClusters: outputs.GetMdbKafkaConnectorConnectorConfigMirrormakerSourceClusterThisCluster[];
}

export interface GetMdbKafkaConnectorConnectorConfigMirrormakerSourceClusterExternalCluster {
    bootstrapServers: string;
    saslMechanism: string;
    saslPassword: string;
    saslUsername: string;
    securityProtocol: string;
}

export interface GetMdbKafkaConnectorConnectorConfigMirrormakerSourceClusterThisCluster {
}

export interface GetMdbKafkaConnectorConnectorConfigMirrormakerTargetCluster {
    alias: string;
    externalClusters: outputs.GetMdbKafkaConnectorConnectorConfigMirrormakerTargetClusterExternalCluster[];
    thisClusters: outputs.GetMdbKafkaConnectorConnectorConfigMirrormakerTargetClusterThisCluster[];
}

export interface GetMdbKafkaConnectorConnectorConfigMirrormakerTargetClusterExternalCluster {
    bootstrapServers: string;
    saslMechanism: string;
    saslPassword: string;
    saslUsername: string;
    securityProtocol: string;
}

export interface GetMdbKafkaConnectorConnectorConfigMirrormakerTargetClusterThisCluster {
}

export interface GetMdbKafkaConnectorConnectorConfigS3Sink {
    fileCompressionType: string;
    fileMaxRecords: number;
    s3Connections: outputs.GetMdbKafkaConnectorConnectorConfigS3SinkS3Connection[];
    topics: string;
}

export interface GetMdbKafkaConnectorConnectorConfigS3SinkS3Connection {
    bucketName: string;
    externalS3s: outputs.GetMdbKafkaConnectorConnectorConfigS3SinkS3ConnectionExternalS3[];
}

export interface GetMdbKafkaConnectorConnectorConfigS3SinkS3ConnectionExternalS3 {
    accessKeyId: string;
    endpoint: string;
    region: string;
    secretAccessKey: string;
}

export interface GetMdbKafkaTopicTopicConfig {
    cleanupPolicy: string;
    compressionType: string;
    deleteRetentionMs: string;
    fileDeleteDelayMs: string;
    flushMessages: string;
    flushMs: string;
    maxMessageBytes: string;
    minCompactionLagMs: string;
    minInsyncReplicas: string;
    preallocate: boolean;
    retentionBytes: string;
    retentionMs: string;
    segmentBytes: string;
}

export interface GetMdbKafkaUserPermission {
    allowHosts: string[];
    role: string;
    topicName: string;
}

export interface GetMdbMongodbClusterClusterConfig {
    access: outputs.GetMdbMongodbClusterClusterConfigAccess;
    backupRetainPeriodDays: number;
    backupWindowStart: outputs.GetMdbMongodbClusterClusterConfigBackupWindowStart;
    featureCompatibilityVersion: string;
    mongocfg: outputs.GetMdbMongodbClusterClusterConfigMongocfg;
    mongod: outputs.GetMdbMongodbClusterClusterConfigMongod;
    mongos: outputs.GetMdbMongodbClusterClusterConfigMongos;
    performanceDiagnostics: outputs.GetMdbMongodbClusterClusterConfigPerformanceDiagnostics;
    version?: string;
}

export interface GetMdbMongodbClusterClusterConfigAccess {
    dataLens?: boolean;
    dataTransfer?: boolean;
}

export interface GetMdbMongodbClusterClusterConfigBackupWindowStart {
    hours?: number;
    minutes?: number;
}

export interface GetMdbMongodbClusterClusterConfigMongocfg {
    net?: outputs.GetMdbMongodbClusterClusterConfigMongocfgNet;
    operationProfiling?: outputs.GetMdbMongodbClusterClusterConfigMongocfgOperationProfiling;
    storage?: outputs.GetMdbMongodbClusterClusterConfigMongocfgStorage;
}

export interface GetMdbMongodbClusterClusterConfigMongocfgNet {
    maxIncomingConnections?: number;
}

export interface GetMdbMongodbClusterClusterConfigMongocfgOperationProfiling {
    mode?: string;
    slowOpThreshold?: number;
}

export interface GetMdbMongodbClusterClusterConfigMongocfgStorage {
    wiredTiger?: outputs.GetMdbMongodbClusterClusterConfigMongocfgStorageWiredTiger;
}

export interface GetMdbMongodbClusterClusterConfigMongocfgStorageWiredTiger {
    cacheSizeGb?: number;
}

export interface GetMdbMongodbClusterClusterConfigMongod {
    auditLog: outputs.GetMdbMongodbClusterClusterConfigMongodAuditLog;
    net?: outputs.GetMdbMongodbClusterClusterConfigMongodNet;
    operationProfiling?: outputs.GetMdbMongodbClusterClusterConfigMongodOperationProfiling;
    security: outputs.GetMdbMongodbClusterClusterConfigMongodSecurity;
    setParameter: outputs.GetMdbMongodbClusterClusterConfigMongodSetParameter;
    storage?: outputs.GetMdbMongodbClusterClusterConfigMongodStorage;
}

export interface GetMdbMongodbClusterClusterConfigMongodAuditLog {
    filter?: string;
    runtimeConfiguration?: boolean;
}

export interface GetMdbMongodbClusterClusterConfigMongodNet {
    compressors?: string[];
    maxIncomingConnections?: number;
}

export interface GetMdbMongodbClusterClusterConfigMongodOperationProfiling {
    mode?: string;
    slowOpSampleRate?: number;
    slowOpThreshold?: number;
}

export interface GetMdbMongodbClusterClusterConfigMongodSecurity {
    enableEncryption?: boolean;
    kmip: outputs.GetMdbMongodbClusterClusterConfigMongodSecurityKmip;
}

export interface GetMdbMongodbClusterClusterConfigMongodSecurityKmip {
    clientCertificate?: string;
    keyIdentifier?: string;
    port?: number;
    serverCa?: string;
    serverName?: string;
}

export interface GetMdbMongodbClusterClusterConfigMongodSetParameter {
    auditAuthorizationSuccess?: boolean;
    enableFlowControl?: boolean;
    minSnapshotHistoryWindowInSeconds?: number;
}

export interface GetMdbMongodbClusterClusterConfigMongodStorage {
    journal?: outputs.GetMdbMongodbClusterClusterConfigMongodStorageJournal;
    wiredTiger?: outputs.GetMdbMongodbClusterClusterConfigMongodStorageWiredTiger;
}

export interface GetMdbMongodbClusterClusterConfigMongodStorageJournal {
    commitInterval?: number;
}

export interface GetMdbMongodbClusterClusterConfigMongodStorageWiredTiger {
    blockCompressor?: string;
    cacheSizeGb?: number;
    prefixCompression?: boolean;
}

export interface GetMdbMongodbClusterClusterConfigMongos {
    net?: outputs.GetMdbMongodbClusterClusterConfigMongosNet;
}

export interface GetMdbMongodbClusterClusterConfigMongosNet {
    compressors?: string[];
    maxIncomingConnections?: number;
}

export interface GetMdbMongodbClusterClusterConfigPerformanceDiagnostics {
    enabled?: boolean;
}

export interface GetMdbMongodbClusterDatabase {
    name?: string;
}

export interface GetMdbMongodbClusterDiskSizeAutoscalingMongocfg {
    diskSizeLimit?: number;
    emergencyUsageThreshold?: number;
    plannedUsageThreshold?: number;
}

export interface GetMdbMongodbClusterDiskSizeAutoscalingMongod {
    diskSizeLimit?: number;
    emergencyUsageThreshold?: number;
    plannedUsageThreshold?: number;
}

export interface GetMdbMongodbClusterDiskSizeAutoscalingMongoinfra {
    diskSizeLimit?: number;
    emergencyUsageThreshold?: number;
    plannedUsageThreshold?: number;
}

export interface GetMdbMongodbClusterDiskSizeAutoscalingMongos {
    diskSizeLimit?: number;
    emergencyUsageThreshold?: number;
    plannedUsageThreshold?: number;
}

export interface GetMdbMongodbClusterHost {
    assignPublicIp?: boolean;
    health: string;
    hostParameters: outputs.GetMdbMongodbClusterHostHostParameters;
    name: string;
    role: string;
    shardName: string;
    subnetId?: string;
    type?: string;
    zoneId?: string;
}

export interface GetMdbMongodbClusterHostHostParameters {
    hidden?: boolean;
    priority?: number;
    secondaryDelaySecs?: number;
    tags?: {[key: string]: string};
}

export interface GetMdbMongodbClusterMaintenanceWindow {
    day?: string;
    hour?: number;
    type?: string;
}

export interface GetMdbMongodbClusterResources {
    diskSize?: number;
    diskTypeId?: string;
    resourcePresetId?: string;
}

export interface GetMdbMongodbClusterResourcesMongocfg {
    diskSize?: number;
    diskTypeId?: string;
    resourcePresetId?: string;
}

export interface GetMdbMongodbClusterResourcesMongod {
    diskSize?: number;
    diskTypeId?: string;
    resourcePresetId?: string;
}

export interface GetMdbMongodbClusterResourcesMongoinfra {
    diskSize?: number;
    diskTypeId?: string;
    resourcePresetId?: string;
}

export interface GetMdbMongodbClusterResourcesMongos {
    diskSize?: number;
    diskTypeId?: string;
    resourcePresetId?: string;
}

export interface GetMdbMongodbClusterRestore {
    backupId?: string;
    time?: string;
}

export interface GetMdbMongodbClusterUser {
    name?: string;
    password?: string;
    permissions: outputs.GetMdbMongodbClusterUserPermission[];
}

export interface GetMdbMongodbClusterUserPermission {
    databaseName?: string;
    roles?: string[];
}

export interface GetMdbMysqlClusterAccess {
    dataLens: boolean;
    dataTransfer: boolean;
    webSql: boolean;
}

export interface GetMdbMysqlClusterBackupWindowStart {
    hours?: number;
    minutes?: number;
}

export interface GetMdbMysqlClusterDatabase {
    name: string;
}

export interface GetMdbMysqlClusterHost {
    assignPublicIp?: boolean;
    backupPriority?: number;
    fqdn: string;
    priority?: number;
    replicationSource: string;
    subnetId: string;
    zone: string;
}

export interface GetMdbMysqlClusterMaintenanceWindow {
    day: string;
    hour: number;
    type: string;
}

export interface GetMdbMysqlClusterPerformanceDiagnostic {
    enabled: boolean;
    sessionsSamplingInterval: number;
    statementsSamplingInterval: number;
}

export interface GetMdbMysqlClusterResource {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbMysqlClusterUser {
    authenticationPlugin: string;
    connectionLimits: outputs.GetMdbMysqlClusterUserConnectionLimit[];
    globalPermissions: string[];
    name: string;
    password: string;
    permissions: outputs.GetMdbMysqlClusterUserPermission[];
}

export interface GetMdbMysqlClusterUserConnectionLimit {
    maxConnectionsPerHour: number;
    maxQuestionsPerHour: number;
    maxUpdatesPerHour: number;
    maxUserConnections: number;
}

export interface GetMdbMysqlClusterUserPermission {
    databaseName: string;
    roles?: string[];
}

export interface GetMdbMysqlUserConnectionLimit {
    maxConnectionsPerHour: number;
    maxQuestionsPerHour: number;
    maxUpdatesPerHour: number;
    maxUserConnections: number;
}

export interface GetMdbMysqlUserPermission {
    databaseName: string;
    roles?: string[];
}

export interface GetMdbPostgresqlClusterConfig {
    accesses: outputs.GetMdbPostgresqlClusterConfigAccess[];
    autofailover: boolean;
    backupRetainPeriodDays: number;
    backupWindowStarts: outputs.GetMdbPostgresqlClusterConfigBackupWindowStart[];
    diskSizeAutoscalings: outputs.GetMdbPostgresqlClusterConfigDiskSizeAutoscaling[];
    performanceDiagnostics: outputs.GetMdbPostgresqlClusterConfigPerformanceDiagnostic[];
    poolerConfigs: outputs.GetMdbPostgresqlClusterConfigPoolerConfig[];
    postgresqlConfig: {[key: string]: string};
    resources: outputs.GetMdbPostgresqlClusterConfigResource[];
    version: string;
}

export interface GetMdbPostgresqlClusterConfigAccess {
    dataLens: boolean;
    dataTransfer: boolean;
    serverless: boolean;
    webSql: boolean;
}

export interface GetMdbPostgresqlClusterConfigBackupWindowStart {
    hours: number;
    minutes: number;
}

export interface GetMdbPostgresqlClusterConfigDiskSizeAutoscaling {
    diskSizeLimit: number;
    emergencyUsageThreshold: number;
    plannedUsageThreshold: number;
}

export interface GetMdbPostgresqlClusterConfigPerformanceDiagnostic {
    enabled: boolean;
    sessionsSamplingInterval: number;
    statementsSamplingInterval: number;
}

export interface GetMdbPostgresqlClusterConfigPoolerConfig {
    poolDiscard: boolean;
    poolingMode: string;
}

export interface GetMdbPostgresqlClusterConfigResource {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbPostgresqlClusterDatabase {
    extensions?: outputs.GetMdbPostgresqlClusterDatabaseExtension[];
    lcCollate?: string;
    lcType?: string;
    name: string;
    owner: string;
    templateDb?: string;
}

export interface GetMdbPostgresqlClusterDatabaseExtension {
    name: string;
    version?: string;
}

export interface GetMdbPostgresqlClusterHost {
    assignPublicIp: boolean;
    fqdn: string;
    priority: number;
    replicationSource: string;
    role: string;
    subnetId: string;
    zone: string;
}

export interface GetMdbPostgresqlClusterMaintenanceWindow {
    day: string;
    hour: number;
    type: string;
}

export interface GetMdbPostgresqlClusterUser {
    connLimit: number;
    grants: string[];
    login?: boolean;
    name: string;
    permissions: outputs.GetMdbPostgresqlClusterUserPermission[];
    settings: {[key: string]: string};
}

export interface GetMdbPostgresqlClusterUserPermission {
    databaseName: string;
}

export interface GetMdbPostgresqlDatabaseExtension {
    name: string;
    version?: string;
}

export interface GetMdbPostgresqlUserPermission {
    databaseName: string;
}

export interface GetMdbRedisClusterConfig {
    clientOutputBufferLimitNormal: string;
    clientOutputBufferLimitPubsub: string;
    databases: number;
    maxmemoryPercent: number;
    maxmemoryPolicy: string;
    notifyKeyspaceEvents: string;
    slowlogLogSlowerThan: number;
    slowlogMaxLen: number;
    timeout: number;
    version: string;
}

export interface GetMdbRedisClusterDiskSizeAutoscaling {
    diskSizeLimit: number;
    emergencyUsageThreshold: number;
    plannedUsageThreshold: number;
}

export interface GetMdbRedisClusterHost {
    assignPublicIp?: boolean;
    fqdn: string;
    replicaPriority?: number;
    shardName: string;
    subnetId: string;
    zone: string;
}

export interface GetMdbRedisClusterMaintenanceWindow {
    day: string;
    hour: number;
    type: string;
}

export interface GetMdbRedisClusterResource {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbSqlserverClusterBackupWindowStart {
    hours: number;
    minutes: number;
}

export interface GetMdbSqlserverClusterDatabase {
    name: string;
}

export interface GetMdbSqlserverClusterHost {
    assignPublicIp: boolean;
    fqdn: string;
    subnetId: string;
    zone: string;
}

export interface GetMdbSqlserverClusterResource {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface GetMdbSqlserverClusterUser {
    name: string;
    password: string;
    permissions: outputs.GetMdbSqlserverClusterUserPermission[];
}

export interface GetMdbSqlserverClusterUserPermission {
    databaseName: string;
    roles: string[];
}

export interface GetMonitoringDashboardParametrization {
    /**
     * Dashboard parameter
     */
    parameters: outputs.GetMonitoringDashboardParametrizationParameter[];
    /**
     * Predefined selectors
     */
    selectors: string;
}

export interface GetMonitoringDashboardParametrizationParameter {
    /**
     * Custom parameter
     */
    customs: outputs.GetMonitoringDashboardParametrizationParameterCustom[];
    /**
     * Parameter description
     */
    description: string;
    /**
     * UI-visibility
     */
    hidden: boolean;
    /**
     * Parameter identifier
     */
    id: string;
    /**
     * Label values parameter
     */
    labelValues: outputs.GetMonitoringDashboardParametrizationParameterLabelValue[];
    /**
     * Text parameter
     */
    texts: outputs.GetMonitoringDashboardParametrizationParameterText[];
    /**
     * UI-visible title of the parameter
     */
    title: string;
}

export interface GetMonitoringDashboardParametrizationParameterCustom {
    /**
     * Default values from values
     */
    defaultValues: string[];
    /**
     * Specifies the multiselectable values of parameter
     */
    multiselectable: boolean;
    /**
     * Parameter values
     */
    values: string[];
}

export interface GetMonitoringDashboardParametrizationParameterLabelValue {
    /**
     * Default value
     */
    defaultValues: string[];
    /**
     * Folder ID
     */
    folderId: string;
    /**
     * Label key to list label values
     */
    labelKey: string;
    /**
     * Specifies the multiselectable values of parameter
     */
    multiselectable: boolean;
    /**
     * Selectors to select metric label values
     */
    selectors: string;
}

export interface GetMonitoringDashboardParametrizationParameterText {
    /**
     * Default value
     */
    defaultValue: string;
}

export interface GetMonitoringDashboardWidget {
    /**
     * Chart widget
     */
    charts: outputs.GetMonitoringDashboardWidgetChart[];
    /**
     * Widget layout position
     */
    positions: outputs.GetMonitoringDashboardWidgetPosition[];
    /**
     * Text widget
     */
    texts: outputs.GetMonitoringDashboardWidgetText[];
    /**
     * Title widget
     */
    titles: outputs.GetMonitoringDashboardWidgetTitle[];
}

export interface GetMonitoringDashboardWidgetChart {
    /**
     * Chart ID
     */
    chartId: string;
    /**
     * Chart description in dashboard (not enabled in UI)
     */
    description: string;
    /**
     * Enable legend under chart
     */
    displayLegend: boolean;
    /**
     * Fixed time interval for chart
     */
    freeze: string;
    /**
     * Name hiding settings
     */
    nameHidingSettings: outputs.GetMonitoringDashboardWidgetChartNameHidingSetting[];
    /**
     * Queries
     */
    queries: outputs.GetMonitoringDashboardWidgetChartQuery[];
    seriesOverrides: outputs.GetMonitoringDashboardWidgetChartSeriesOverride[];
    /**
     * Chart widget title
     */
    title: string;
    /**
     * Visualization settings
     */
    visualizationSettings: outputs.GetMonitoringDashboardWidgetChartVisualizationSetting[];
}

export interface GetMonitoringDashboardWidgetChartNameHidingSetting {
    names: string[];
    /**
     * True if we want to show concrete series names only, false if we want to hide concrete series names
     */
    positive: boolean;
}

export interface GetMonitoringDashboardWidgetChartQuery {
    /**
     * Downsampling settings
     */
    downsamplings: outputs.GetMonitoringDashboardWidgetChartQueryDownsampling[];
    /**
     * Downsampling settings
     */
    targets: outputs.GetMonitoringDashboardWidgetChartQueryTarget[];
}

export interface GetMonitoringDashboardWidgetChartQueryDownsampling {
    /**
     * Disable downsampling
     */
    disabled: boolean;
    /**
     * Parameters for filling gaps in data
     */
    gapFilling: string;
    /**
     * Function that is used for downsampling
     */
    gridAggregation: string;
    /**
     * Time interval (grid) for downsampling in milliseconds. Points in the specified range are aggregated into one time point
     */
    gridInterval: number;
    /**
     * Maximum number of points to be returned
     */
    maxPoints: number;
}

export interface GetMonitoringDashboardWidgetChartQueryTarget {
    /**
     * Checks that target is visible or invisible
     */
    hidden: boolean;
    /**
     * Query
     */
    query: string;
    /**
     * Text mode
     */
    textMode: boolean;
}

export interface GetMonitoringDashboardWidgetChartSeriesOverride {
    /**
     * Series name
     */
    name: string;
    /**
     * Override settings
     */
    settings: outputs.GetMonitoringDashboardWidgetChartSeriesOverrideSetting[];
    /**
     * Target index
     */
    targetIndex: string;
}

export interface GetMonitoringDashboardWidgetChartSeriesOverrideSetting {
    /**
     * Series color or empty
     */
    color: string;
    /**
     * Stack grow down
     */
    growDown: boolean;
    /**
     * Series name or empty
     */
    name: string;
    /**
     * Stack name or empty
     */
    stackName: string;
    /**
     * Type
     */
    type: string;
    /**
     * Yaxis position
     */
    yaxisPosition: string;
}

export interface GetMonitoringDashboardWidgetChartVisualizationSetting {
    /**
     * Aggregation
     */
    aggregation: string;
    /**
     * Color scheme settings
     */
    colorSchemeSettings: outputs.GetMonitoringDashboardWidgetChartVisualizationSettingColorSchemeSetting[];
    /**
     * Heatmap settings
     */
    heatmapSettings: outputs.GetMonitoringDashboardWidgetChartVisualizationSettingHeatmapSetting[];
    /**
     * Interpolate
     */
    interpolate: string;
    /**
     * Normalize
     */
    normalize: boolean;
    /**
     * Show chart labels
     */
    showLabels: boolean;
    /**
     * Inside chart title
     */
    title: string;
    /**
     * Visualization type
     */
    type: string;
    /**
     * Y axis settings
     */
    yaxisSettings: outputs.GetMonitoringDashboardWidgetChartVisualizationSettingYaxisSetting[];
}

export interface GetMonitoringDashboardWidgetChartVisualizationSettingColorSchemeSetting {
    /**
     * Automatic color scheme
     */
    automatics: outputs.GetMonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingAutomatic[];
    /**
     * Gradient color scheme
     */
    gradients: outputs.GetMonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingGradient[];
    /**
     * Standard color scheme
     */
    standards: outputs.GetMonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingStandard[];
}

export interface GetMonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingAutomatic {
}

export interface GetMonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingGradient {
    /**
     * Gradient green value
     */
    greenValue: string;
    /**
     * Gradient red value
     */
    redValue: string;
    /**
     * Gradient violet value
     */
    violetValue: string;
    /**
     * Gradient yellow value
     */
    yellowValue: string;
}

export interface GetMonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingStandard {
}

export interface GetMonitoringDashboardWidgetChartVisualizationSettingHeatmapSetting {
    /**
     * Heatmap green value
     */
    greenValue: string;
    /**
     * Heatmap red value
     */
    redValue: string;
    /**
     * Heatmap violet_value
     */
    violetValue: string;
    /**
     * Heatmap yellow value
     */
    yellowValue: string;
}

export interface GetMonitoringDashboardWidgetChartVisualizationSettingYaxisSetting {
    /**
     * Left Y axis settings
     */
    lefts: outputs.GetMonitoringDashboardWidgetChartVisualizationSettingYaxisSettingLeft[];
    /**
     * Right Y axis settings
     */
    rights: outputs.GetMonitoringDashboardWidgetChartVisualizationSettingYaxisSettingRight[];
}

export interface GetMonitoringDashboardWidgetChartVisualizationSettingYaxisSettingLeft {
    /**
     * Max value in extended number format or empty
     */
    max: string;
    /**
     * Min value in extended number format or empty
     */
    min: string;
    /**
     * Tick value precision (null as default, 0-7 in other cases)
     */
    precision: number;
    /**
     * Title or empty
     */
    title: string;
    /**
     * Type
     */
    type: string;
    /**
     * Unit format
     */
    unitFormat: string;
}

export interface GetMonitoringDashboardWidgetChartVisualizationSettingYaxisSettingRight {
    /**
     * Max value in extended number format or empty
     */
    max: string;
    /**
     * Min value in extended number format or empty
     */
    min: string;
    /**
     * Tick value precision (null as default, 0-7 in other cases)
     */
    precision: number;
    /**
     * Title or empty
     */
    title: string;
    /**
     * Type
     */
    type: string;
    /**
     * Unit format
     */
    unitFormat: string;
}

export interface GetMonitoringDashboardWidgetPosition {
    /**
     * Height
     */
    h: number;
    /**
     * Width
     */
    w: number;
    /**
     * X-axis top-left corner coordinate
     */
    x: number;
    /**
     * Y-axis top-left corner coordinate
     */
    y: number;
}

export interface GetMonitoringDashboardWidgetText {
    /**
     * Text
     */
    text: string;
}

export interface GetMonitoringDashboardWidgetTitle {
    /**
     * Title size
     */
    size: string;
    /**
     * Title text
     */
    text: string;
}

export interface GetOrganizationmanagerGroupMember {
    id: string;
    type: string;
}

export interface GetOrganizationmanagerOsLoginSettingsSshCertificateSettings {
    enabled?: boolean;
}

export interface GetOrganizationmanagerOsLoginSettingsUserSshKeySettings {
    allowManageOwnKeys?: boolean;
    enabled?: boolean;
}

export interface GetOrganizationmanagerSamlFederationSecuritySetting {
    encryptedAssertions: boolean;
}

export interface GetServerlessContainerConnectivity {
    networkId: string;
}

export interface GetServerlessContainerImage {
    args: string[];
    commands: string[];
    digest: string;
    environment: {[key: string]: string};
    url: string;
    workDir: string;
}

export interface GetServerlessContainerLogOption {
    disabled: boolean;
    folderId: string;
    logGroupId: string;
    minLevel: string;
}

export interface GetServerlessContainerSecret {
    environmentVariable: string;
    id: string;
    key: string;
    versionId: string;
}

export interface GetServerlessContainerStorageMount {
    bucket: string;
    mountPointPath: string;
    prefix?: string;
    readOnly?: boolean;
}

export interface GetSmartcaptchaCaptchaOverrideVariant {
    challengeType: string;
    complexity: string;
    description: string;
    preCheckType: string;
    uuid: string;
}

export interface GetSmartcaptchaCaptchaSecurityRule {
    conditions: outputs.GetSmartcaptchaCaptchaSecurityRuleCondition[];
    description: string;
    name: string;
    overrideVariantUuid: string;
    priority: number;
}

export interface GetSmartcaptchaCaptchaSecurityRuleCondition {
    headers: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionHeader[];
    hosts: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionHost[];
    sourceIps: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionSourceIp[];
    uris: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionUri[];
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionHeader {
    name: string;
    values: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionHeaderValue[];
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionHeaderValue {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionHost {
    hosts: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionHostHost[];
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionHostHost {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionSourceIp {
    geoIpMatches: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionSourceIpGeoIpMatch[];
    geoIpNotMatches: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionSourceIpGeoIpNotMatch[];
    ipRangesMatches: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionSourceIpIpRangesMatch[];
    ipRangesNotMatches: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionSourceIpIpRangesNotMatch[];
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionSourceIpGeoIpMatch {
    locations: string[];
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionSourceIpGeoIpNotMatch {
    locations: string[];
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionSourceIpIpRangesMatch {
    ipRanges: string[];
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionSourceIpIpRangesNotMatch {
    ipRanges: string[];
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionUri {
    paths: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionUriPath[];
    queries: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionUriQuery[];
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionUriPath {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionUriQuery {
    key: string;
    values: outputs.GetSmartcaptchaCaptchaSecurityRuleConditionUriQueryValue[];
}

export interface GetSmartcaptchaCaptchaSecurityRuleConditionUriQueryValue {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSwsSecurityProfileSecurityRule {
    description: string;
    dryRun: boolean;
    name: string;
    priority: number;
    ruleConditions: outputs.GetSwsSecurityProfileSecurityRuleRuleCondition[];
    smartProtections: outputs.GetSwsSecurityProfileSecurityRuleSmartProtection[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleCondition {
    action: string;
    conditions: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionCondition[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionCondition {
    authorities: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionAuthority[];
    headers: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionHeader[];
    httpMethods: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionHttpMethod[];
    requestUris: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionRequestUri[];
    sourceIps: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionSourceIp[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionAuthority {
    authorities: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionAuthorityAuthority[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionAuthorityAuthority {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionHeader {
    name: string;
    values: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionHeaderValue[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionHeaderValue {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionHttpMethod {
    httpMethods: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionHttpMethodHttpMethod[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionHttpMethodHttpMethod {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionRequestUri {
    paths: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriPath[];
    queries: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriQuery[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriPath {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriQuery {
    key: string;
    values: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriQueryValue[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriQueryValue {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionSourceIp {
    geoIpMatches: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpGeoIpMatch[];
    geoIpNotMatches: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpGeoIpNotMatch[];
    ipRangesMatches: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpIpRangesMatch[];
    ipRangesNotMatches: outputs.GetSwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpIpRangesNotMatch[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpGeoIpMatch {
    locations: string[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpGeoIpNotMatch {
    locations: string[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpIpRangesMatch {
    ipRanges: string[];
}

export interface GetSwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpIpRangesNotMatch {
    ipRanges: string[];
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtection {
    conditions: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionCondition[];
    mode: string;
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionCondition {
    authorities: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionAuthority[];
    headers: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionHeader[];
    httpMethods: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionHttpMethod[];
    requestUris: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUri[];
    sourceIps: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIp[];
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionAuthority {
    authorities: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionAuthorityAuthority[];
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionAuthorityAuthority {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionHeader {
    name: string;
    values: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionHeaderValue[];
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionHeaderValue {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionHttpMethod {
    httpMethods: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionHttpMethodHttpMethod[];
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionHttpMethodHttpMethod {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUri {
    paths: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriPath[];
    queries: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriQuery[];
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriPath {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriQuery {
    key: string;
    values: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriQueryValue[];
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriQueryValue {
    exactMatch: string;
    exactNotMatch: string;
    pireRegexMatch: string;
    pireRegexNotMatch: string;
    prefixMatch: string;
    prefixNotMatch: string;
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIp {
    geoIpMatches: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpGeoIpMatch[];
    geoIpNotMatches: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpGeoIpNotMatch[];
    ipRangesMatches: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpIpRangesMatch[];
    ipRangesNotMatches: outputs.GetSwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpIpRangesNotMatch[];
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpGeoIpMatch {
    locations: string[];
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpGeoIpNotMatch {
    locations: string[];
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpIpRangesMatch {
    ipRanges: string[];
}

export interface GetSwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpIpRangesNotMatch {
    ipRanges: string[];
}

export interface GetVpcAddressDnsRecord {
    dnsZoneId: string;
    fqdn: string;
    ptr: boolean;
    ttl: number;
}

export interface GetVpcAddressExternalIpv4Address {
    address: string;
    ddosProtectionProvider: string;
    outgoingSmtpCapability: string;
    zoneId: string;
}

export interface GetVpcGatewaySharedEgressGateway {
}

export interface GetVpcRouteTableStaticRoute {
    destinationPrefix: string;
    gatewayId: string;
    nextHopAddress: string;
}

export interface GetVpcSecurityGroupEgress {
    description: string;
    fromPort: number;
    id: string;
    labels: {[key: string]: string};
    port: number;
    predefinedTarget: string;
    protocol: string;
    securityGroupId: string;
    toPort: number;
    v4CidrBlocks: string[];
    v6CidrBlocks: string[];
}

export interface GetVpcSecurityGroupIngress {
    description: string;
    fromPort: number;
    id: string;
    labels: {[key: string]: string};
    port: number;
    predefinedTarget: string;
    protocol: string;
    securityGroupId: string;
    toPort: number;
    v4CidrBlocks: string[];
    v6CidrBlocks: string[];
}

export interface GetVpcSubnetDhcpOption {
    domainName: string;
    domainNameServers: string[];
    ntpServers: string[];
}

export interface GetYdbDatabaseDedicatedLocation {
    regions: outputs.GetYdbDatabaseDedicatedLocationRegion[];
    zones: outputs.GetYdbDatabaseDedicatedLocationZone[];
}

export interface GetYdbDatabaseDedicatedLocationRegion {
    id: string;
}

export interface GetYdbDatabaseDedicatedLocationZone {
    id: string;
}

export interface GetYdbDatabaseDedicatedScalePolicy {
    fixedScales: outputs.GetYdbDatabaseDedicatedScalePolicyFixedScale[];
}

export interface GetYdbDatabaseDedicatedScalePolicyFixedScale {
    size: number;
}

export interface GetYdbDatabaseDedicatedStorageConfig {
    groupCount: number;
    storageTypeId: string;
}

export interface GetYdbDatabaseServerlessServerlessDatabase {
    enableThrottlingRcuLimit: boolean;
    provisionedRcuLimit: number;
    storageSizeLimit: number;
    throttlingRcuLimit: number;
}

export interface IamServiceAccountApiKeyOutputToLockbox {
    /**
     * Entry where to store the value of `secretKey`.
     */
    entryForSecretKey: string;
    /**
     * ID of the Lockbox secret where to store the sensible values.
     */
    secretId: string;
}

export interface IamServiceAccountKeyOutputToLockbox {
    /**
     * Entry where to store the value of `privateKey`.
     */
    entryForPrivateKey: string;
    /**
     * ID of the Lockbox secret where to store the sensible values.
     */
    secretId: string;
}

export interface IamServiceAccountStaticAccessKeyOutputToLockbox {
    /**
     * Entry where to store the value of `secretKey`.
     */
    entryForSecretKey: string;
    /**
     * ID of the Lockbox secret where to store the sensible values.
     */
    secretId: string;
}

export interface IotCoreBrokerLogOptions {
    /**
     * Is logging for broker disabled
     */
    disabled?: boolean;
    /**
     * Log entries are written to default log group for specified folder
     */
    folderId?: string;
    /**
     * Log entries are written to specified log group
     */
    logGroupId?: string;
    /**
     * Minimum log entry level
     */
    minLevel?: string;
}

export interface IotCoreRegistryLogOptions {
    /**
     * Is logging for registry disabled
     */
    disabled?: boolean;
    /**
     * Log entries are written to default log group for specified folder
     */
    folderId?: string;
    /**
     * Log entries are written to specified log group
     */
    logGroupId?: string;
    /**
     * Minimum log entry level
     */
    minLevel?: string;
}

export interface KubernetesClusterKmsProvider {
    /**
     * KMS key ID.
     */
    keyId?: string;
}

export interface KubernetesClusterMaster {
    /**
     * (Computed) PEM-encoded public certificate that is the root of trust for the Kubernetes cluster.
     */
    clusterCaCertificate: string;
    etcdClusterSize: number;
    /**
     * (Computed) An IPv4 external network address that is assigned to the master.
     */
    externalV4Address: string;
    /**
     * (Computed) External endpoint that can be used to access Kubernetes cluster API from the internet (outside of the cloud).
     */
    externalV4Endpoint: string;
    externalV6Address?: string;
    externalV6Endpoint: string;
    /**
     * (Computed) An IPv4 internal network address that is assigned to the master.
     */
    internalV4Address: string;
    /**
     * (Computed) Internal endpoint that can be used to connect to the master from cloud networks.
     */
    internalV4Endpoint: string;
    /**
     * (Optional) (Computed) Maintenance policy for Kubernetes master.
     * If policy is omitted, automatic revision upgrades of the kubernetes master are enabled and could happen at any time.
     * Revision upgrades are performed only within the same minor version, e.g. 1.13.
     * Minor version upgrades (e.g. 1.13->1.14) should be performed manually. The structure is documented below.
     */
    maintenancePolicy: outputs.KubernetesClusterMasterMaintenancePolicy;
    /**
     * (Optional) Cluster master's instances locations array (zone and subnet).
     * Cannot be used together with `zonal` or `regional`. Currently, supports either one, for zonal master, or three instances of `masterLocation`.
     * Can be updated inplace. When creating regional cluster (three master instances), its `region` will be evaluated automatically by backend.
     * The structure is documented below.
     */
    masterLocations: outputs.KubernetesClusterMasterMasterLocation[];
    /**
     * (Optional) Master Logging options. The structure is documented below.
     */
    masterLogging?: outputs.KubernetesClusterMasterMasterLogging;
    /**
     * (Optional) (Computed) Boolean flag. When `true`, Kubernetes master will have visible ipv4 address.
     */
    publicIp: boolean;
    /**
     * (Optional) Initialize parameters for Regional Master (highly available master). The structure is documented below.
     */
    regional: outputs.KubernetesClusterMasterRegional;
    /**
     * (Optional) List of security group IDs to which the Kubernetes cluster belongs.
     */
    securityGroupIds?: string[];
    /**
     * (Optional) (Computed) Version of Kubernetes that will be used for master.
     */
    version: string;
    /**
     * (Computed) Information about cluster version. The structure is documented below.
     */
    versionInfos: outputs.KubernetesClusterMasterVersionInfo[];
    /**
     * (Optional) Initialize parameters for Zonal Master (single node master). The structure is documented below.
     */
    zonal: outputs.KubernetesClusterMasterZonal;
}

export interface KubernetesClusterMasterMaintenancePolicy {
    /**
     * (Required) Boolean flag that specifies if master can be upgraded automatically. When omitted, default value is TRUE.
     */
    autoUpgrade: boolean;
    /**
     * (Optional) (Computed) This structure specifies maintenance window, when update for master is allowed. When omitted, it defaults to any time.
     * To specify time of day interval, for all days, one element should be provided, with two fields set, `startTime` and `duration`.
     * Please see `zonalClusterResourceName` config example.
     */
    maintenanceWindows?: outputs.KubernetesClusterMasterMaintenancePolicyMaintenanceWindow[];
}

export interface KubernetesClusterMasterMaintenancePolicyMaintenanceWindow {
    day: string;
    duration: string;
    startTime: string;
}

export interface KubernetesClusterMasterMasterLocation {
    /**
     * (Optional) ID of the subnet.
     */
    subnetId: string;
    /**
     * (Optional) ID of the availability zone.
     */
    zone: string;
}

export interface KubernetesClusterMasterMasterLogging {
    /**
     * (Optional) Boolean flag that specifies if kube-apiserver audit logs should be sent to Yandex Cloud Logging.
     */
    auditEnabled?: boolean;
    /**
     * (Optional) Boolean flag that specifies if cluster-autoscaler logs should be sent to Yandex Cloud Logging.
     */
    clusterAutoscalerEnabled?: boolean;
    /**
     * (Optional) Boolean flag that specifies if master components logs should be sent to [Yandex Cloud Logging](https://cloud.yandex.com/docs/logging/). The exact components that will send their logs must be configured via the options described below.
     */
    enabled?: boolean;
    /**
     * (Optional) Boolean flag that specifies if kubernetes cluster events should be sent to Yandex Cloud Logging.
     */
    eventsEnabled?: boolean;
    /**
     * The ID of the folder that the Kubernetes cluster belongs to.
     * If it is not provided, the default provider folder is used.
     */
    folderId?: string;
    /**
     * (Optional) Boolean flag that specifies if kube-apiserver logs should be sent to Yandex Cloud Logging.
     */
    kubeApiserverEnabled?: boolean;
    /**
     * (Optional) ID of the Yandex Cloud Logging [Log group](https://cloud.yandex.com/docs/logging/concepts/log-group).
     */
    logGroupId?: string;
}

export interface KubernetesClusterMasterRegional {
    /**
     * Array of locations, where master instances will be allocated. The structure is documented below.
     */
    locations: outputs.KubernetesClusterMasterRegionalLocation[];
    /**
     * (Required) Name of availability region (e.g. "ru-central1"), where master instances will be allocated.
     */
    region: string;
}

export interface KubernetesClusterMasterRegionalLocation {
    /**
     * (Optional) ID of the subnet.
     */
    subnetId?: string;
    /**
     * (Optional) ID of the availability zone.
     */
    zone?: string;
}

export interface KubernetesClusterMasterVersionInfo {
    /**
     * Current Kubernetes version, major.minor (e.g. 1.15).
     */
    currentVersion: string;
    /**
     * Boolean flag.
     * Newer revisions may include Kubernetes patches (e.g 1.15.1 > 1.15.2) as well
     * as some internal component updates - new features or bug fixes in yandex-specific
     * components either on the master or nodes.
     */
    newRevisionAvailable: boolean;
    /**
     * Human readable description of the changes to be applied
     * when updating to the latest revision. Empty if newRevisionAvailable is false.
     */
    newRevisionSummary: string;
    /**
     * Boolean flag. The current version is on the deprecation schedule,
     * component (master or node group) should be upgraded.
     */
    versionDeprecated: boolean;
}

export interface KubernetesClusterMasterZonal {
    /**
     * (Optional) ID of the subnet.
     */
    subnetId?: string;
    /**
     * (Optional) ID of the availability zone.
     */
    zone: string;
}

export interface KubernetesClusterNetworkImplementation {
    /**
     * (Optional) Cilium network implementation configuration. No options exist.
     */
    cilium?: outputs.KubernetesClusterNetworkImplementationCilium;
}

export interface KubernetesClusterNetworkImplementationCilium {
}

export interface KubernetesNodeGroupAllocationPolicy {
    /**
     * Repeated field, that specify subnets (zones), that will be used by node group compute instances. The structure is documented below.
     */
    locations: outputs.KubernetesNodeGroupAllocationPolicyLocation[];
}

export interface KubernetesNodeGroupAllocationPolicyLocation {
    /**
     * ID of the subnet, that will be used by one compute instance in node group.
     *
     * Subnet specified by `subnetId` should be allocated in zone specified by 'zone' argument
     *
     * @deprecated The 'subnet_id' field has been deprecated. Please use 'subnet_ids under network_interface' instead.
     */
    subnetId: string;
    /**
     * ID of the availability zone where for one compute instance in node group.
     */
    zone: string;
}

export interface KubernetesNodeGroupDeployPolicy {
    /**
     * The maximum number of instances that can be temporarily allocated above the group's target size during the update.
     */
    maxExpansion: number;
    /**
     * The maximum number of running instances that can be taken offline during update.
     */
    maxUnavailable: number;
}

export interface KubernetesNodeGroupInstanceTemplate {
    /**
     * The specifications for boot disks that will be attached to the instance. The structure is documented below.
     */
    bootDisk: outputs.KubernetesNodeGroupInstanceTemplateBootDisk;
    /**
     * Container network configuration. The structure is documented below.
     */
    containerNetwork: outputs.KubernetesNodeGroupInstanceTemplateContainerNetwork;
    /**
     * Container runtime configuration. The structure is documented below.
     */
    containerRuntime: outputs.KubernetesNodeGroupInstanceTemplateContainerRuntime;
    /**
     * GPU settings. The structure is documented below.
     */
    gpuSettings: outputs.KubernetesNodeGroupInstanceTemplateGpuSettings;
    /**
     * Labels that will be assigned to compute nodes (instances), created by the Node Group.
     */
    labels?: {[key: string]: string};
    /**
     * The set of metadata `key:value` pairs assigned to this instance template. This includes custom metadata and predefined keys. **Note**: key "user-data" won't be provided into instances. It reserved for internal activity in `kubernetesNodeGroup` resource.
     *
     * * `resources.0.memory` - The memory size allocated to the instance.
     * * `resources.0.cores` - Number of CPU cores allocated to the instance.
     * * `resources.0.core_fraction` - Baseline core performance as a percent.
     * * `resources.0.gpus` - Number of GPU cores allocated to the instance.
     */
    metadata: {[key: string]: string};
    /**
     * Name template of the instance.
     * In order to be unique it must contain at least one of instance unique placeholders:
     * {instance.short_id}
     * {instance.index}
     * combination of {instance.zone_id} and {instance.index_in_zone}
     * Example: my-instance-{instance.index}
     * If not set, default is used: {instance_group.id}-{instance.short_id}
     * It may also contain another placeholders, see [Compute Instance group metadata doc](https://cloud.yandex.com/en-ru/docs/compute/api-ref/grpc/instance_group_service) for full list.
     */
    name?: string;
    /**
     * Boolean flag, enables NAT for node group compute instances.
     *
     * @deprecated The 'nat' field has been deprecated. Please use 'nat under network_interface' instead.
     */
    nat: boolean;
    /**
     * Type of network acceleration. Values: `standard`, `softwareAccelerated`.
     */
    networkAccelerationType: string;
    /**
     * An array with the network interfaces that will be attached to the instance. The structure is documented below.
     */
    networkInterfaces: outputs.KubernetesNodeGroupInstanceTemplateNetworkInterface[];
    /**
     * The placement policy configuration. The structure is documented below.
     */
    placementPolicy?: outputs.KubernetesNodeGroupInstanceTemplatePlacementPolicy;
    /**
     * The ID of the hardware platform configuration for the node group compute instances.
     */
    platformId: string;
    resources: outputs.KubernetesNodeGroupInstanceTemplateResources;
    /**
     * The scheduling policy for the instances in node group. The structure is documented below.
     */
    schedulingPolicy: outputs.KubernetesNodeGroupInstanceTemplateSchedulingPolicy;
}

export interface KubernetesNodeGroupInstanceTemplateBootDisk {
    /**
     * The size of the disk in GB. Allowed minimal size: 64 GB.
     */
    size: number;
    /**
     * The disk type.
     */
    type: string;
}

export interface KubernetesNodeGroupInstanceTemplateContainerNetwork {
    /**
     * MTU for pods.
     */
    podMtu: number;
}

export interface KubernetesNodeGroupInstanceTemplateContainerRuntime {
    /**
     * Type of container runtime. Values: `docker`, `containerd`.
     */
    type: string;
}

export interface KubernetesNodeGroupInstanceTemplateGpuSettings {
    /**
     * GPU cluster id.
     */
    gpuClusterId?: string;
    /**
     * GPU environment. Values: `runc`, `runcDriversCuda`.
     */
    gpuEnvironment: string;
}

export interface KubernetesNodeGroupInstanceTemplateNetworkInterface {
    /**
     * Allocate an IPv4 address for the interface. The default value is `true`.
     */
    ipv4?: boolean;
    /**
     * List of configurations for creating ipv4 DNS records. The structure is documented below.
     */
    ipv4DnsRecords?: outputs.KubernetesNodeGroupInstanceTemplateNetworkInterfaceIpv4DnsRecord[];
    /**
     * If true, allocate an IPv6 address for the interface. The address will be automatically assigned from the specified subnet.
     */
    ipv6: boolean;
    /**
     * List of configurations for creating ipv6 DNS records. The structure is documented below.
     */
    ipv6DnsRecords?: outputs.KubernetesNodeGroupInstanceTemplateNetworkInterfaceIpv6DnsRecord[];
    /**
     * A public address that can be used to access the internet over NAT.
     */
    nat: boolean;
    /**
     * Security group ids for network interface.
     */
    securityGroupIds?: string[];
    /**
     * The IDs of the subnets.
     */
    subnetIds: string[];
}

export interface KubernetesNodeGroupInstanceTemplateNetworkInterfaceIpv4DnsRecord {
    /**
     * DNS zone ID (if not set, private zone is used).
     */
    dnsZoneId?: string;
    /**
     * DNS record FQDN.
     */
    fqdn: string;
    /**
     * When set to true, also create a PTR DNS record.
     */
    ptr?: boolean;
    /**
     * DNS record TTL (in seconds).
     */
    ttl?: number;
}

export interface KubernetesNodeGroupInstanceTemplateNetworkInterfaceIpv6DnsRecord {
    /**
     * DNS zone ID (if not set, private zone is used).
     */
    dnsZoneId?: string;
    /**
     * DNS record FQDN.
     */
    fqdn: string;
    /**
     * When set to true, also create a PTR DNS record.
     */
    ptr?: boolean;
    /**
     * DNS record TTL (in seconds).
     */
    ttl?: number;
}

export interface KubernetesNodeGroupInstanceTemplatePlacementPolicy {
    /**
     * Specifies the id of the Placement Group to assign to the instances.
     */
    placementGroupId: string;
}

export interface KubernetesNodeGroupInstanceTemplateResources {
    coreFraction: number;
    cores: number;
    gpus?: number;
    memory: number;
}

export interface KubernetesNodeGroupInstanceTemplateSchedulingPolicy {
    /**
     * Specifies if the instance is preemptible. Defaults to false.
     */
    preemptible: boolean;
}

export interface KubernetesNodeGroupMaintenancePolicy {
    /**
     * Boolean flag that specifies if node group can be repaired automatically. When omitted, default value is TRUE.
     */
    autoRepair: boolean;
    /**
     * Boolean flag that specifies if node group can be upgraded automatically. When omitted, default value is TRUE.
     */
    autoUpgrade: boolean;
    /**
     * (Computed) Set of day intervals, when maintenance is allowed for this node group. When omitted, it defaults to any time. 
     *
     * To specify time of day interval, for all days, one element should be provided, with two fields set, `startTime` and `duration`.
     *
     * To allow maintenance only on specific days of week, please provide list of elements, with all fields set. Only one
     * time interval is allowed for each day of week. Please see `myNodeGroup` config example.
     */
    maintenanceWindows?: outputs.KubernetesNodeGroupMaintenancePolicyMaintenanceWindow[];
}

export interface KubernetesNodeGroupMaintenancePolicyMaintenanceWindow {
    day: string;
    duration: string;
    startTime: string;
}

export interface KubernetesNodeGroupScalePolicy {
    /**
     * Scale policy for an autoscaled node group. The structure is documented below.
     */
    autoScale?: outputs.KubernetesNodeGroupScalePolicyAutoScale;
    /**
     * Scale policy for a fixed scale node group. The structure is documented below.
     */
    fixedScale?: outputs.KubernetesNodeGroupScalePolicyFixedScale;
}

export interface KubernetesNodeGroupScalePolicyAutoScale {
    /**
     * Initial number of instances in the node group.
     */
    initial: number;
    /**
     * Maximum number of instances in the node group.
     */
    max: number;
    /**
     * Minimum number of instances in the node group.
     */
    min: number;
}

export interface KubernetesNodeGroupScalePolicyFixedScale {
    /**
     * The number of instances in the node group.
     */
    size: number;
}

export interface KubernetesNodeGroupVersionInfo {
    /**
     * Current Kubernetes version, major.minor (e.g. 1.15).
     */
    currentVersion: string;
    /**
     * True/false flag.
     * Newer revisions may include Kubernetes patches (e.g 1.15.1 > 1.15.2) as well
     * as some internal component updates - new features or bug fixes in yandex-specific
     * components either on the master or nodes.
     */
    newRevisionAvailable: boolean;
    /**
     * Human readable description of the changes to be applied
     * when updating to the latest revision. Empty if newRevisionAvailable is false.
     */
    newRevisionSummary: string;
    /**
     * True/false flag. The current version is on the deprecation schedule,
     * component (master or node group) should be upgraded.
     */
    versionDeprecated: boolean;
}

export interface LbNetworkLoadBalancerAttachedTargetGroup {
    /**
     * A HealthCheck resource. The structure is documented below.
     */
    healthchecks: outputs.LbNetworkLoadBalancerAttachedTargetGroupHealthcheck[];
    /**
     * ID of the target group.
     */
    targetGroupId: string;
}

export interface LbNetworkLoadBalancerAttachedTargetGroupHealthcheck {
    /**
     * Number of successful health checks required in order to set the `HEALTHY` status for the target.
     */
    healthyThreshold?: number;
    /**
     * Options for HTTP health check. The structure is documented below.
     */
    httpOptions?: outputs.LbNetworkLoadBalancerAttachedTargetGroupHealthcheckHttpOptions;
    /**
     * The interval between health checks. The default is 2 seconds.
     */
    interval?: number;
    /**
     * Name of the health check. The name must be unique for each target group that attached to a single load balancer.
     */
    name: string;
    /**
     * Options for TCP health check. The structure is documented below.
     *
     * > **NOTE:** One of `httpOptions` or `tcpOptions` should be specified.
     */
    tcpOptions?: outputs.LbNetworkLoadBalancerAttachedTargetGroupHealthcheckTcpOptions;
    /**
     * Timeout for a target to return a response for the health check. The default is 1 second.
     */
    timeout?: number;
    /**
     * Number of failed health checks before changing the status to `UNHEALTHY`. The default is 2.
     */
    unhealthyThreshold?: number;
}

export interface LbNetworkLoadBalancerAttachedTargetGroupHealthcheckHttpOptions {
    /**
     * URL path to set for health checking requests for every target in the target group. For example `/ping`. The default path is `/`.
     */
    path?: string;
    /**
     * Port to use for HTTP health checks.
     */
    port: number;
}

export interface LbNetworkLoadBalancerAttachedTargetGroupHealthcheckTcpOptions {
    /**
     * Port to use for TCP health checks.
     */
    port: number;
}

export interface LbNetworkLoadBalancerListener {
    /**
     * External IP address specification. The structure is documented below.
     */
    externalAddressSpec?: outputs.LbNetworkLoadBalancerListenerExternalAddressSpec;
    /**
     * Internal IP address specification. The structure is documented below.
     *
     * > **NOTE:** One of `externalAddressSpec` or `internalAddressSpec` should be specified.
     */
    internalAddressSpec?: outputs.LbNetworkLoadBalancerListenerInternalAddressSpec;
    /**
     * Name of the listener. The name must be unique for each listener on a single load balancer.
     */
    name: string;
    /**
     * Port for incoming traffic.
     */
    port: number;
    /**
     * Protocol for incoming traffic. TCP or UDP and the default is TCP.
     */
    protocol: string;
    /**
     * Port of a target. The default is the same as listener's port.
     */
    targetPort: number;
}

export interface LbNetworkLoadBalancerListenerExternalAddressSpec {
    /**
     * External IP address for a listener. IP address will be allocated if it wasn't been set.
     */
    address: string;
    /**
     * IP version of the external addresses that the load balancer works with. Must be one of ipv4 or ipv6. The default is ipv4.
     */
    ipVersion?: string;
}

export interface LbNetworkLoadBalancerListenerInternalAddressSpec {
    /**
     * Internal IP address for a listener. Must belong to the subnet that is referenced in subnet_id. IP address will be allocated if it wasn't been set.
     */
    address: string;
    /**
     * IP version of the internal addresses that the load balancer works with. Must be one of ipv4 or ipv6. The default is ipv4.
     */
    ipVersion?: string;
    /**
     * ID of the subnet to which the internal IP address belongs.
     */
    subnetId: string;
}

export interface LbTargetGroupTarget {
    /**
     * IP address of the target.
     */
    address: string;
    /**
     * ID of the subnet that targets are connected to. 
     * All targets in the target group must be connected to the same subnet within a single availability zone.
     */
    subnetId: string;
}

export interface LoadtestingAgentComputeInstance {
    /**
     * Boot disk specifications for the instance. The structure is documented below.
     */
    bootDisk: outputs.LoadtestingAgentComputeInstanceBootDisk;
    /**
     * The set of metadata `key:value` pairs assigned to this instance. This includes user custom `metadata`, and predefined items created by Yandex Cloud Load Testing.
     */
    computedMetadata: {[key: string]: string};
    /**
     * A set of key/value label pairs to assign to the instance.
     */
    labels?: {[key: string]: string};
    /**
     * A set of metadata key/value pairs to make available from within the instance.
     */
    metadata?: {[key: string]: string};
    /**
     * Network specifications for the instance. This can be used multiple times for adding multiple interfaces. The structure is documented below.
     */
    networkInterfaces: outputs.LoadtestingAgentComputeInstanceNetworkInterface[];
    /**
     * The Compute platform of virtual machine. If it is not provided, the standard-v2 platform will be used.
     */
    platformId: string;
    /**
     * Compute resource specifications for the instance. The structure is documented below.
     */
    resources: outputs.LoadtestingAgentComputeInstanceResources;
    /**
     * The ID of the service account authorized for this load testing agent. Service account should have `loadtesting.generatorClient` or `loadtesting.externalAgent` role in the folder.
     */
    serviceAccountId: string;
    /**
     * The availability zone where the virtual machine will be created. If it is not provided,
     * the default provider folder is used.
     */
    zoneId: string;
}

export interface LoadtestingAgentComputeInstanceBootDisk {
    /**
     * Whether the disk is auto-deleted when the instance is deleted. The default value is true.
     */
    autoDelete?: boolean;
    /**
     * This value can be used to reference the device under `/dev/disk/by-id/`.
     */
    deviceName: string;
    /**
     * The ID of created disk.
     */
    diskId: string;
    /**
     * Parameters for creating a disk alongside the instance. The structure is documented below.
     */
    initializeParams: outputs.LoadtestingAgentComputeInstanceBootDiskInitializeParams;
}

export interface LoadtestingAgentComputeInstanceBootDiskInitializeParams {
    /**
     * Block size of the disk, specified in bytes.
     */
    blockSize: number;
    /**
     * A description of the boot disk.
     */
    description: string;
    /**
     * A name of the boot disk.
     */
    name: string;
    /**
     * The size of the disk in GB. Defaults to 15 GB.
     */
    size?: number;
    /**
     * The disk type.
     */
    type?: string;
}

export interface LoadtestingAgentComputeInstanceNetworkInterface {
    index: number;
    /**
     * Manual set static IP address.
     */
    ipAddress: string;
    /**
     * Flag for allocating IPv4 address for the network interface.
     */
    ipv4?: boolean;
    /**
     * Flag for allocating IPv6 address for the network interface.
     */
    ipv6: boolean;
    /**
     * Manual set static IPv6 address.
     */
    ipv6Address: string;
    macAddress: string;
    /**
     * Flag for using NAT.
     */
    nat?: boolean;
    /**
     * A public address that can be used to access the internet over NAT.
     */
    natIpAddress: string;
    natIpVersion: string;
    /**
     * Security group ids for network interface.
     */
    securityGroupIds: string[];
    /**
     * The ID of the subnet to attach this interface to. The subnet must reside in the same zone where this instance was created.
     */
    subnetId: string;
}

export interface LoadtestingAgentComputeInstanceResources {
    /**
     * If provided, specifies baseline core performance as a percent.
     */
    coreFraction?: number;
    /**
     * The number of CPU cores for the instance. Defaults to 2 cores.
     */
    cores?: number;
    /**
     * The memory size in GB. Defaults to 2 GB.
     */
    memory?: number;
}

export interface LockboxSecretVersionEntry {
    /**
     * The command that generates the text value of the entry.
     *
     * Note that either `textValue` or `command` is required.
     *
     * The `command` block contains:
     */
    command?: outputs.LockboxSecretVersionEntryCommand;
    /**
     * The key of the entry.
     */
    key: string;
    /**
     * The text value of the entry.
     */
    textValue?: string;
}

export interface LockboxSecretVersionEntryCommand {
    /**
     * List of arguments to be passed to the script/command.
     */
    args?: string[];
    /**
     * Map of environment variables to set before calling the script/command.
     */
    env?: {[key: string]: string};
    /**
     * The path to the script or command to execute.
     */
    path: string;
}

export interface MdbClickhouseClusterAccess {
    /**
     * Allow access for DataLens. Can be either `true` or `false`.
     */
    dataLens?: boolean;
    /**
     * Allow access for DataTransfer. Can be either `true` or `false`.
     */
    dataTransfer?: boolean;
    /**
     * Allow access for Yandex.Metrika. Can be either `true` or `false`.
     */
    metrika?: boolean;
    /**
     * Allow access for Serverless. Can be either `true` or `false`.
     */
    serverless?: boolean;
    /**
     * Allow access for Web SQL. Can be either `true` or `false`.
     */
    webSql?: boolean;
    /**
     * Allow access for YandexQuery. Can be either `true` or `false`.
     */
    yandexQuery?: boolean;
}

export interface MdbClickhouseClusterBackupWindowStart {
    /**
     * The hour at which backup will be started.
     */
    hours?: number;
    /**
     * The minute at which backup will be started.
     */
    minutes?: number;
}

export interface MdbClickhouseClusterClickhouse {
    /**
     * Main ClickHouse cluster configuration.
     */
    config: outputs.MdbClickhouseClusterClickhouseConfig;
    /**
     * Resources allocated to hosts of the ClickHouse subcluster. The structure is documented below.
     */
    resources: outputs.MdbClickhouseClusterClickhouseResources;
}

export interface MdbClickhouseClusterClickhouseConfig {
    backgroundFetchesPoolSize: number;
    backgroundMergesMutationsConcurrencyRatio: number;
    backgroundMessageBrokerSchedulePoolSize: number;
    backgroundPoolSize: number;
    backgroundSchedulePoolSize: number;
    /**
     * Data compression configuration. The structure is documented below.
     */
    compressions?: outputs.MdbClickhouseClusterClickhouseConfigCompression[];
    defaultDatabase: string;
    dictionariesLazyLoad: boolean;
    geobaseEnabled: boolean;
    geobaseUri: string;
    /**
     * Graphite rollup configuration. The structure is documented below.
     */
    graphiteRollups?: outputs.MdbClickhouseClusterClickhouseConfigGraphiteRollup[];
    /**
     * Kafka connection configuration. The structure is documented below.
     */
    kafka: outputs.MdbClickhouseClusterClickhouseConfigKafka;
    /**
     * Kafka topic connection configuration. The structure is documented below.
     */
    kafkaTopics?: outputs.MdbClickhouseClusterClickhouseConfigKafkaTopic[];
    keepAliveTimeout: number;
    /**
     * , `maxConnections`, `maxConcurrentQueries`, `keepAliveTimeout`, `uncompressedCacheSize`, `markCacheSize`,
     * `maxTableSizeToDrop`, `maxPartitionSizeToDrop`, `timezone`, `geobaseUri`, `queryLogRetentionSize`,
     * `queryLogRetentionTime`, `queryThreadLogEnabled`, `queryThreadLogRetentionSize`, `queryThreadLogRetentionTime`,
     * `partLogRetentionSize`, `partLogRetentionTime`, `metricLogEnabled`, `metricLogRetentionSize`, `metricLogRetentionTime`,
     * `traceLogEnabled`, `traceLogRetentionSize`, `traceLogRetentionTime`, `textLogEnabled`, `textLogRetentionSize`,
     * `textLogRetentionTime`, `textLogLevel`,
     * `backgroundPoolSize`, `backgroundSchedulePoolSize`, `backgroundFetchesPoolSize`, `backgroundMessageBrokerSchedulePoolSize`,`backgroundMergesMutationsConcurrencyRatio`,
     * `defaultDatabase`,
     * `totalMemoryProfilerStep`, `dictionariesLazyLoad` - (Optional) ClickHouse server parameters. For more information, see
     * [the official documentation](https://cloud.yandex.com/docs/managed-clickhouse/concepts/settings-list).
     */
    logLevel: string;
    markCacheSize: number;
    maxConcurrentQueries: number;
    maxConnections: number;
    maxPartitionSizeToDrop: number;
    maxTableSizeToDrop: number;
    /**
     * MergeTree engine configuration. The structure is documented below.
     */
    mergeTree: outputs.MdbClickhouseClusterClickhouseConfigMergeTree;
    metricLogEnabled: boolean;
    metricLogRetentionSize: number;
    metricLogRetentionTime: number;
    partLogRetentionSize: number;
    partLogRetentionTime: number;
    /**
     * Query cache configuration. The structure is documented below.
     */
    queryCache: outputs.MdbClickhouseClusterClickhouseConfigQueryCache;
    queryLogRetentionSize: number;
    queryLogRetentionTime: number;
    /**
     * Query masking rules configuration. The structure is documented below.
     */
    queryMaskingRules?: outputs.MdbClickhouseClusterClickhouseConfigQueryMaskingRule[];
    queryThreadLogEnabled: boolean;
    queryThreadLogRetentionSize: number;
    queryThreadLogRetentionTime: number;
    /**
     * RabbitMQ connection configuration. The structure is documented below.
     */
    rabbitmq: outputs.MdbClickhouseClusterClickhouseConfigRabbitmq;
    textLogEnabled: boolean;
    textLogLevel: string;
    textLogRetentionSize: number;
    textLogRetentionTime: number;
    timezone: string;
    totalMemoryProfilerStep: number;
    traceLogEnabled: boolean;
    traceLogRetentionSize: number;
    traceLogRetentionTime: number;
    uncompressedCacheSize: number;
}

export interface MdbClickhouseClusterClickhouseConfigCompression {
    /**
     * Compression level for `ZSTD` method.
     */
    level?: number;
    /**
     * Method: Compression method. Two methods are available: LZ4 and zstd.
     */
    method: string;
    /**
     * Min part size: Minimum size (in bytes) of a data part in a table. ClickHouse only applies the rule to tables with data parts greater than or equal to the Min part size value.
     */
    minPartSize: number;
    /**
     * Min part size ratio: Minimum table part size to total table size ratio. ClickHouse only applies the rule to tables in which this ratio is greater than or equal to the Min part size ratio value.
     */
    minPartSizeRatio: number;
}

export interface MdbClickhouseClusterClickhouseConfigGraphiteRollup {
    /**
     * Graphite rollup configuration name.
     */
    name: string;
    /**
     * The name of the column storing the metric name (Graphite sensor). Default value: Path.
     */
    pathColumnName: string;
    /**
     * Set of thinning rules.
     */
    patterns?: outputs.MdbClickhouseClusterClickhouseConfigGraphiteRollupPattern[];
    /**
     * The name of the column storing the time of measuring the metric. Default value: Time.
     */
    timeColumnName: string;
    /**
     * The name of the column storing the value of the metric at the time set in time_column_name. Default value: Value.
     */
    valueColumnName: string;
    /**
     * The name of the column storing the version of the metric. Default value: Timestamp.
     */
    versionColumnName: string;
}

export interface MdbClickhouseClusterClickhouseConfigGraphiteRollupPattern {
    /**
     * Aggregation function name.
     */
    function: string;
    /**
     * Regular expression that the metric name must match.
     */
    regexp: string;
    /**
     * Retain parameters.
     */
    retentions?: outputs.MdbClickhouseClusterClickhouseConfigGraphiteRollupPatternRetention[];
}

export interface MdbClickhouseClusterClickhouseConfigGraphiteRollupPatternRetention {
    /**
     * Minimum data age in seconds.
     */
    age: number;
    /**
     * Accuracy of determining the age of the data in seconds.
     */
    precision: number;
}

export interface MdbClickhouseClusterClickhouseConfigKafka {
    /**
     * Action to take when there is no initial offset in offset store or the desired offset is out of range: 'smallest','earliest' - automatically reset the offset to the smallest offset, 'largest','latest' - automatically reset the offset to the largest offset, 'error' - trigger an error (ERR__AUTO_OFFSET_RESET) which is retrieved by consuming messages and checking 'message->err'.
     */
    autoOffsetReset: string;
    /**
     * A comma-separated list of debug contexts to enable.
     */
    debug: string;
    /**
     * enable verification of SSL certificates.
     */
    enableSslCertificateVerification: boolean;
    /**
     * Maximum allowed time between calls to consume messages (e.g., rd_kafka_consumer_poll()) for high-level consumers. If this interval is exceeded the consumer is considered failed and the group will rebalance in order to reassign the partitions to another consumer group member.
     */
    maxPollIntervalMs: number;
    /**
     * SASL mechanism used in kafka authentication.
     */
    saslMechanism: string;
    /**
     * User password on kafka server.
     */
    saslPassword: string;
    /**
     * Username on kafka server.
     */
    saslUsername: string;
    /**
     * Security protocol used to connect to kafka server.
     */
    securityProtocol: string;
    /**
     * Client group session and failure detection timeout. The consumer sends periodic heartbeats (heartbeat.interval.ms) to indicate its liveness to the broker. If no hearts are received by the broker for a group member within the session timeout, the broker will remove the consumer from the group and trigger a rebalance.
     */
    sessionTimeoutMs: number;
}

export interface MdbClickhouseClusterClickhouseConfigKafkaTopic {
    /**
     * Kafka topic name.
     */
    name: string;
    /**
     * Kafka connection settngs sanem as `kafka` block.
     */
    settings?: outputs.MdbClickhouseClusterClickhouseConfigKafkaTopicSettings;
}

export interface MdbClickhouseClusterClickhouseConfigKafkaTopicSettings {
    /**
     * Action to take when there is no initial offset in offset store or the desired offset is out of range: 'smallest','earliest' - automatically reset the offset to the smallest offset, 'largest','latest' - automatically reset the offset to the largest offset, 'error' - trigger an error (ERR__AUTO_OFFSET_RESET) which is retrieved by consuming messages and checking 'message->err'.
     */
    autoOffsetReset: string;
    /**
     * A comma-separated list of debug contexts to enable.
     */
    debug: string;
    /**
     * enable verification of SSL certificates.
     */
    enableSslCertificateVerification: boolean;
    /**
     * Maximum allowed time between calls to consume messages (e.g., rd_kafka_consumer_poll()) for high-level consumers. If this interval is exceeded the consumer is considered failed and the group will rebalance in order to reassign the partitions to another consumer group member.
     */
    maxPollIntervalMs: number;
    /**
     * SASL mechanism used in kafka authentication.
     */
    saslMechanism?: string;
    /**
     * User password on kafka server.
     */
    saslPassword?: string;
    /**
     * Username on kafka server.
     */
    saslUsername?: string;
    /**
     * Security protocol used to connect to kafka server.
     */
    securityProtocol?: string;
    /**
     * Client group session and failure detection timeout. The consumer sends periodic heartbeats (heartbeat.interval.ms) to indicate its liveness to the broker. If no hearts are received by the broker for a group member within the session timeout, the broker will remove the consumer from the group and trigger a rebalance.
     */
    sessionTimeoutMs: number;
}

export interface MdbClickhouseClusterClickhouseConfigMergeTree {
    /**
     * Enables the check at table creation, that the data type of a column for sampling or sampling expression is correct. The data type must be one of unsigned integer types: UInt8, UInt16, UInt32, UInt64. Default value: true.
     */
    checkSampleColumnIsCorrect: boolean;
    /**
     * Minimum period to clean old queue logs, blocks hashes and parts.
     */
    cleanupDelayPeriod: number;
    /**
     * The `too many parts` check according to `partsToDelayInsert` and `partsToThrowInsert` will be active only if the average part size (in the relevant partition) is not larger than the specified threshold. If it is larger than the specified threshold, the INSERTs will be neither delayed or rejected. This allows to have hundreds of terabytes in a single table on a single server if the parts are successfully merged to larger parts. This does not affect the thresholds on inactive parts or total parts.
     */
    maxAvgPartSizeForTooManyParts: number;
    /**
     * Max bytes to merge at min space in pool: Maximum total size of a data part to merge when the number of free threads in the background pool is minimum.
     */
    maxBytesToMergeAtMinSpaceInPool: number;
    /**
     * Maximum period to clean old queue logs, blocks hashes and parts. Default value: 300 seconds.
     */
    maxCleanupDelayPeriod: number;
    /**
     * Maximum sleep time for merge selecting, a lower setting will trigger selecting tasks in backgroundSchedulePool frequently which result in large amount of requests to zookeeper in large-scale clusters. Default value: 60000 milliseconds (60 seconds).
     */
    maxMergeSelectingSleepMs: number;
    /**
     * When there is more than specified number of merges with TTL entries in pool, do not assign new merge with TTL.
     */
    maxNumberOfMergesWithTtlInPool: number;
    /**
     * Maximum number of parts in all partitions.
     */
    maxPartsInTotal: number;
    /**
     * Max replicated merges in queue: Maximum number of merge tasks that can be in the ReplicatedMergeTree queue at the same time.
     */
    maxReplicatedMergesInQueue: number;
    /**
     * The number of rows that are read from the merged parts into memory. Default value: 8192.
     */
    mergeMaxBlockSize: number;
    /**
     * Sleep time for merge selecting when no part is selected. A lower setting triggers selecting tasks in backgroundSchedulePool frequently, which results in a large number of requests to ClickHouse Keeper in large-scale clusters.
     */
    mergeSelectingSleepMs: number;
    /**
     * Minimum delay in seconds before repeating a merge with recompression TTL. Default value: 14400 seconds (4 hours).
     */
    mergeWithRecompressionTtlTimeout: number;
    /**
     * Minimum delay in seconds before repeating a merge with delete TTL. Default value: 14400 seconds (4 hours).
     */
    mergeWithTtlTimeout: number;
    /**
     * Whether minAgeToForceMergeSeconds should be applied only on the entire partition and not on subset.
     */
    minAgeToForceMergeOnPartitionOnly: boolean;
    /**
     * Merge parts if every part in the range is older than the value of `minAgeToForceMergeSeconds`.
     */
    minAgeToForceMergeSeconds: number;
    /**
     * Minimum number of bytes in a data part that can be stored in Wide format. You can set one, both or none of these settings.
     */
    minBytesForWidePart: number;
    /**
     * Minimum number of rows in a data part that can be stored in Wide format. You can set one, both or none of these settings.
     */
    minRowsForWidePart: number;
    /**
     * Number of free entries in pool to lower max size of merge: Threshold value of free entries in the pool. If the number of entries in the pool falls below this value, ClickHouse reduces the maximum size of a data part to merge. This helps handle small merges faster, rather than filling the pool with lengthy merges.
     */
    numberOfFreeEntriesInPoolToLowerMaxSizeOfMerge: number;
    /**
     * Parts to delay insert: Number of active data parts in a table, on exceeding which ClickHouse starts artificially reduce the rate of inserting data into the table.
     */
    partsToDelayInsert: number;
    /**
     * Parts to throw insert: Threshold value of active data parts in a table, on exceeding which ClickHouse throws the 'Too many parts ...' exception.
     */
    partsToThrowInsert: number;
    /**
     * Replicated deduplication window: Number of recent hash blocks that ZooKeeper will store (the old ones will be deleted).
     */
    replicatedDeduplicationWindow: number;
    /**
     * Replicated deduplication window seconds: Time during which ZooKeeper stores the hash blocks (the old ones wil be deleted).
     */
    replicatedDeduplicationWindowSeconds: number;
    /**
     * Enables or disables complete dropping of data parts where all rows are expired in MergeTree tables.
     */
    ttlOnlyDropParts: boolean;
}

export interface MdbClickhouseClusterClickhouseConfigQueryCache {
    /**
     * The maximum number of SELECT query results stored in the cache. Default value: 1024.
     */
    maxEntries: number;
    /**
     * The maximum size in bytes SELECT query results may have to be saved in the cache. Default value: 1048576 (1 MiB).
     */
    maxEntrySizeInBytes: number;
    /**
     * The maximum number of rows SELECT query results may have to be saved in the cache. Default value: 30000000 (30 mil).
     */
    maxEntrySizeInRows: number;
    /**
     * The maximum cache size in bytes. 0 means the query cache is disabled. Default value: 1073741824 (1 GiB).
     */
    maxSizeInBytes: number;
}

export interface MdbClickhouseClusterClickhouseConfigQueryMaskingRule {
    /**
     * Name for the rule.
     */
    name: string;
    /**
     * RE2 compatible regular expression.
     */
    regexp: string;
    /**
     * Substitution string for sensitive data. Default value: six asterisks.
     */
    replace: string;
}

export interface MdbClickhouseClusterClickhouseConfigRabbitmq {
    /**
     * RabbitMQ user password.
     */
    password: string;
    /**
     * RabbitMQ username.
     */
    username: string;
    /**
     * RabbitMQ vhost. Default: '\'.
     */
    vhost: string;
}

export interface MdbClickhouseClusterClickhouseResources {
    /**
     * Volume of the storage available to a host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of hosts.
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbClickhouseClusterCloudStorage {
    /**
     * Enables temporary storage in the cluster repository of data requested from the object repository.
     */
    dataCacheEnabled: boolean;
    /**
     * Defines the maximum amount of memory (in bytes) allocated in the cluster storage for temporary storage of data requested from the object storage.
     */
    dataCacheMaxSize: number;
    /**
     * Whether to use Yandex Object Storage for storing ClickHouse data. Can be either `true` or `false`.
     */
    enabled: boolean;
    /**
     * Sets the minimum free space ratio in the cluster storage. If the free space is lower than this value, the data is transferred to Yandex Object Storage. Acceptable values are 0 to 1, inclusive.
     */
    moveFactor: number;
    /**
     * Disables merging of data parts in `Yandex Object Storage`.
     */
    preferNotToMerge: boolean;
}

export interface MdbClickhouseClusterDatabase {
    /**
     * The name of the database.
     */
    name: string;
}

export interface MdbClickhouseClusterFormatSchema {
    /**
     * The name of the format schema.
     */
    name: string;
    /**
     * Type of the format schema.
     */
    type: string;
    /**
     * Format schema file URL. You can only use format schemas stored in Yandex Object Storage.
     */
    uri: string;
}

export interface MdbClickhouseClusterHost {
    /**
     * Sets whether the host should get a public IP address on creation. Can be either `true` or `false`.
     */
    assignPublicIp?: boolean;
    /**
     * The fully qualified domain name of the host.
     */
    fqdn: string;
    /**
     * The name of the shard to which the host belongs.
     */
    shardName: string;
    /**
     * The ID of the subnet, to which the host belongs. The subnet must be a part of the network to which the cluster belongs.
     */
    subnetId: string;
    /**
     * The type of the host to be deployed. Can be either `CLICKHOUSE` or `ZOOKEEPER`.
     */
    type: string;
    /**
     * The availability zone where the ClickHouse host will be created.
     * For more information see [the official documentation](https://cloud.yandex.com/docs/overview/concepts/geo-scope).
     */
    zone: string;
}

export interface MdbClickhouseClusterMaintenanceWindow {
    /**
     * Day of week for maintenance window if window type is weekly. Possible values: `MON`, `TUE`, `WED`, `THU`, `FRI`, `SAT`, `SUN`.
     */
    day?: string;
    /**
     * Hour of day in UTC time zone (1-24) for maintenance window if window type is weekly.
     */
    hour?: number;
    /**
     * Type of maintenance window. Can be either `ANYTIME` or `WEEKLY`. A day and hour of window need to be specified with weekly window.
     */
    type: string;
}

export interface MdbClickhouseClusterMlModel {
    /**
     * The name of the ml model.
     */
    name: string;
    /**
     * Type of the model.
     */
    type: string;
    /**
     * Model file URL. You can only use models stored in Yandex Object Storage.
     */
    uri: string;
}

export interface MdbClickhouseClusterShard {
    /**
     * The name of shard.
     */
    name: string;
    /**
     * Resources allocated to host of the shard. The resources specified for the shard takes precedence over the resources specified for the cluster. The structure is documented below.
     */
    resources: outputs.MdbClickhouseClusterShardResources;
    /**
     * The weight of shard.
     */
    weight: number;
}

export interface MdbClickhouseClusterShardGroup {
    /**
     * Description of the shard group.
     */
    description?: string;
    /**
     * The name of the shard group, used as cluster name in Distributed tables.
     */
    name: string;
    /**
     * List of shards names that belong to the shard group.
     */
    shardNames: string[];
}

export interface MdbClickhouseClusterShardResources {
    /**
     * Volume of the storage available to a host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of hosts.
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbClickhouseClusterUser {
    /**
     * The name of the user.
     */
    name: string;
    /**
     * The password of the user.
     */
    password: string;
    /**
     * Set of permissions granted to the user. The structure is documented below.
     */
    permissions: outputs.MdbClickhouseClusterUserPermission[];
    /**
     * Set of user quotas. The structure is documented below.
     */
    quotas: outputs.MdbClickhouseClusterUserQuota[];
    /**
     * Custom settings for user. The list is documented below.
     */
    settings: outputs.MdbClickhouseClusterUserSettings;
}

export interface MdbClickhouseClusterUserPermission {
    /**
     * The name of the database that the permission grants access to.
     */
    databaseName: string;
}

export interface MdbClickhouseClusterUserQuota {
    /**
     * The number of queries that threw exception.
     */
    errors: number;
    /**
     * The total query execution time, in milliseconds (wall time).
     */
    executionTime: number;
    /**
     * Duration of interval for quota in milliseconds.
     */
    intervalDuration: number;
    /**
     * The total number of queries.
     */
    queries: number;
    /**
     * The total number of source rows read from tables for running the query, on all remote servers.
     */
    readRows: number;
    /**
     * The total number of rows given as the result.
     */
    resultRows: number;
}

export interface MdbClickhouseClusterUserSettings {
    /**
     * Include CORS headers in HTTP responces.
     */
    addHttpCorsHeader: boolean;
    /**
     * Allows or denies DDL queries.
     */
    allowDdl: boolean;
    /**
     * Enables introspections functions for query profiling.
     */
    allowIntrospectionFunctions: boolean;
    /**
     * Allows specifying LowCardinality modifier for types of small fixed size (8 or less) in CREATE TABLE statements. Enabling this may increase merge times and memory consumption.
     */
    allowSuspiciousLowCardinalityTypes: boolean;
    /**
     * Enables asynchronous inserts. Disabled by default.
     */
    asyncInsert: boolean;
    /**
     * The maximum timeout in milliseconds since the first INSERT query before inserting collected data. If the parameter is set to 0, the timeout is disabled. Default value: 200.
     */
    asyncInsertBusyTimeout: number;
    /**
     * The maximum size of the unparsed data in bytes collected per query before being inserted. If the parameter is set to 0, asynchronous insertions are disabled. Default value: 100000.
     */
    asyncInsertMaxDataSize: number;
    /**
     * The maximum timeout in milliseconds since the last INSERT query before dumping collected data. If enabled, the settings prolongs the asyncInsertBusyTimeout with every INSERT query as long as asyncInsertMaxDataSize is not exceeded.
     */
    asyncInsertStaleTimeout: number;
    /**
     * The maximum number of threads for background data parsing and insertion. If the parameter is set to 0, asynchronous insertions are disabled. Default value: 16.
     */
    asyncInsertThreads: number;
    /**
     * Cancels HTTP read-only queries (e.g. SELECT) when a client closes the connection without waiting for the response.
     * Default value: false.
     */
    cancelHttpReadonlyQueriesOnClientClose: boolean;
    /**
     * Enable compilation of queries.
     */
    compile: boolean;
    /**
     * Turn on expression compilation.
     */
    compileExpressions: boolean;
    /**
     * Connect timeout in milliseconds on the socket used for communicating with the client.
     */
    connectTimeout: number;
    /**
     * The timeout in milliseconds for connecting to a remote server for a Distributed table engine, if the ‘shard’ and ‘replica’ sections are used in the cluster definition. If unsuccessful, several attempts are made to connect to various replicas. Default value: 50.
     */
    connectTimeoutWithFailover: number;
    /**
     * Specifies which of the uniq* functions should be used to perform the COUNT(DISTINCT …) construction.
     */
    countDistinctImplementation: string;
    /**
     * Sets behaviour on overflow when using DISTINCT. Possible values:
     */
    distinctOverflowMode: string;
    /**
     * Determine the behavior of distributed subqueries.
     */
    distributedAggregationMemoryEfficient: boolean;
    /**
     * Timeout for DDL queries, in milliseconds.
     */
    distributedDdlTaskTimeout: number;
    /**
     * Changes the behaviour of distributed subqueries.
     */
    distributedProductMode: string;
    /**
     * Allows to retunr empty result.
     */
    emptyResultForAggregationByEmptySet: boolean;
    /**
     * Enables or disables data compression in the response to an HTTP request.
     */
    enableHttpCompression: boolean;
    /**
     * Forces a query to an out-of-date replica if updated data is not available.
     */
    fallbackToStaleReplicasForDistributedQueries: boolean;
    /**
     * Sets the data format of a nested columns.
     */
    flattenNested: boolean;
    /**
     * Disables query execution if the index can’t be used by date.
     */
    forceIndexByDate: boolean;
    /**
     * Disables query execution if indexing by the primary key is not possible.
     */
    forcePrimaryKey: boolean;
    /**
     * Sets behaviour on overflow while GROUP BY operation. Possible values:
     */
    groupByOverflowMode: string;
    /**
     * Sets the threshold of the number of keys, after that the two-level aggregation should be used.
     */
    groupByTwoLevelThreshold: number;
    /**
     * Sets the threshold of the number of bytes, after that the two-level aggregation should be used.
     */
    groupByTwoLevelThresholdBytes: number;
    /**
     * Connection timeout for establishing connection with replica for Hedged requests. Default value: 50 milliseconds.
     */
    hedgedConnectionTimeoutMs: number;
    /**
     * Timeout for HTTP connection in milliseconds.
     */
    httpConnectionTimeout: number;
    /**
     * Sets minimal interval between notifications about request process in HTTP header X-ClickHouse-Progress.
     */
    httpHeadersProgressInterval: number;
    /**
     * Timeout for HTTP connection in milliseconds.
     */
    httpReceiveTimeout: number;
    /**
     * Timeout for HTTP connection in milliseconds.
     */
    httpSendTimeout: number;
    /**
     * Timeout to close idle TCP connections after specified number of seconds. Default value: 3600 seconds.
     */
    idleConnectionTimeout: number;
    /**
     * When performing INSERT queries, replace omitted input column values with default values of the respective columns.
     */
    inputFormatDefaultsForOmittedFields: boolean;
    /**
     * Enables or disables the insertion of JSON data with nested objects.
     */
    inputFormatImportNestedJson: boolean;
    /**
     * Enables or disables order-preserving parallel parsing of data formats. Supported only for TSV, TKSV, CSV and JSONEachRow formats.
     */
    inputFormatParallelParsing: boolean;
    /**
     * Enables or disables the full SQL parser if the fast stream parser can’t parse the data.
     */
    inputFormatValuesInterpretExpressions: boolean;
    /**
     * The setting sets the maximum number of retries for ClickHouse Keeper (or ZooKeeper) requests during insert into replicated MergeTree. Only Keeper requests which failed due to network error, Keeper session timeout, or request timeout are considered for retries.
     */
    insertKeeperMaxRetries: number;
    /**
     * Enables the insertion of default values instead of NULL into columns with not nullable data type. Default value: true.
     */
    insertNullAsDefault: boolean;
    /**
     * Enables the quorum writes.
     */
    insertQuorum: number;
    /**
     * Write to a quorum timeout in milliseconds.
     */
    insertQuorumTimeout: number;
    /**
     * Sets behaviour on overflow in JOIN. Possible values:
     */
    joinOverflowMode: string;
    /**
     * Sets the type of JOIN behaviour. When merging tables, empty cells may appear. ClickHouse fills them differently based on this setting.
     */
    joinUseNulls: boolean;
    /**
     * Require aliases for subselects and table functions in FROM that more than one table is present.
     */
    joinedSubqueryRequiresAlias: boolean;
    /**
     * Specifies the algorithm of replicas selection that is used for distributed query processing, one of: random, nearest_hostname, in_order, first_or_random, round_robin. Default value: random.
     */
    loadBalancing: string;
    /**
     * Method of reading data from local filesystem. Possible values:
     */
    localFilesystemReadMethod: string;
    /**
     * Setting up query threads logging. Query threads log into the system.query_thread_log table. This setting has effect only when logQueries is true. Queries’ threads run by ClickHouse with this setup are logged according to the rules in the queryThreadLog server configuration parameter. Default value: true.
     */
    logQueryThreads: boolean;
    /**
     * Allows or restricts using the LowCardinality data type with the Native format.
     */
    lowCardinalityAllowInNativeFormat: boolean;
    /**
     * Maximum abstract syntax tree depth.
     */
    maxAstDepth: number;
    /**
     * Maximum abstract syntax tree elements.
     */
    maxAstElements: number;
    /**
     * A recommendation for what size of the block (in a count of rows) to load from tables.
     */
    maxBlockSize: number;
    /**
     * Limit in bytes for using memoru for GROUP BY before using swap on disk.
     */
    maxBytesBeforeExternalGroupBy: number;
    /**
     * This setting is equivalent of the maxBytesBeforeExternalGroupBy setting, except for it is for sort operation (ORDER BY), not aggregation.
     */
    maxBytesBeforeExternalSort: number;
    /**
     * Limits the maximum size of a hash table in bytes (uncompressed data) when using DISTINCT.
     */
    maxBytesInDistinct: number;
    /**
     * Limit on maximum size of the hash table for JOIN, in bytes.
     */
    maxBytesInJoin: number;
    /**
     * Limit on the number of bytes in the set resulting from the execution of the IN section.
     */
    maxBytesInSet: number;
    /**
     * Limits the maximum number of bytes (uncompressed data) that can be read from a table when running a query.
     */
    maxBytesToRead: number;
    /**
     * Limits the maximum number of bytes (uncompressed data) that can be read from a table for sorting.
     */
    maxBytesToSort: number;
    /**
     * Limits the maximum number of bytes (uncompressed data) that can be passed to a remote server or saved in a temporary table when using GLOBAL IN.
     */
    maxBytesToTransfer: number;
    /**
     * Limits the maximum number of columns that can be read from a table in a single query.
     */
    maxColumnsToRead: number;
    /**
     * The maximum number of concurrent requests per user. Default value: 0 (no limit).
     */
    maxConcurrentQueriesForUser: number;
    /**
     * Limits the maximum query execution time in milliseconds.
     */
    maxExecutionTime: number;
    /**
     * Maximum abstract syntax tree depth after after expansion of aliases.
     */
    maxExpandedAstElements: number;
    /**
     * Sets the maximum number of parallel threads for the SELECT query data read phase with the FINAL modifier.
     */
    maxFinalThreads: number;
    /**
     * Limits the maximum number of HTTP GET redirect hops for URL-engine tables.
     */
    maxHttpGetRedirects: number;
    /**
     * The size of blocks (in a count of rows) to form for insertion into a table.
     */
    maxInsertBlockSize: number;
    /**
     * The maximum number of threads to execute the INSERT SELECT query. Default value: 0.
     */
    maxInsertThreads: number;
    /**
     * Limits the maximum memory usage (in bytes) for processing queries on a single server.
     */
    maxMemoryUsage: number;
    /**
     * Limits the maximum memory usage (in bytes) for processing of user's queries on a single server.
     */
    maxMemoryUsageForUser: number;
    /**
     * Limits the speed of the data exchange over the network in bytes per second.
     */
    maxNetworkBandwidth: number;
    /**
     * Limits the speed of the data exchange over the network in bytes per second.
     */
    maxNetworkBandwidthForUser: number;
    /**
     * Limits maximum recursion depth in the recursive descent parser. Allows controlling the stack size. Zero means unlimited.
     */
    maxParserDepth: number;
    /**
     * The maximum part of a query that can be taken to RAM for parsing with the SQL parser.
     */
    maxQuerySize: number;
    /**
     * The maximum size of the buffer to read from the filesystem.
     */
    maxReadBufferSize: number;
    /**
     * Disables lagging replicas for distributed queries.
     */
    maxReplicaDelayForDistributedQueries: number;
    /**
     * Limits the number of bytes in the result.
     */
    maxResultBytes: number;
    /**
     * Limits the number of rows in the result.
     */
    maxResultRows: number;
    /**
     * Limits the maximum number of different rows when using DISTINCT.
     */
    maxRowsInDistinct: number;
    /**
     * Limit on maximum size of the hash table for JOIN, in rows.
     */
    maxRowsInJoin: number;
    /**
     * Limit on the number of rows in the set resulting from the execution of the IN section.
     */
    maxRowsInSet: number;
    /**
     * Limits the maximum number of unique keys received from aggregation function.
     */
    maxRowsToGroupBy: number;
    /**
     * Limits the maximum number of rows that can be read from a table when running a query.
     */
    maxRowsToRead: number;
    /**
     * Limits the maximum number of rows that can be read from a table for sorting.
     */
    maxRowsToSort: number;
    /**
     * Limits the maximum number of rows that can be passed to a remote server or saved in a temporary table when using GLOBAL IN.
     */
    maxRowsToTransfer: number;
    /**
     * Limits the maximum number of temporary columns that must be kept in RAM at the same time when running a query, including constant columns.
     */
    maxTemporaryColumns: number;
    /**
     * The maximum amount of data consumed by temporary files on disk in bytes for all concurrently running queries. Zero means unlimited.
     */
    maxTemporaryDataOnDiskSizeForQuery: number;
    /**
     * The maximum amount of data consumed by temporary files on disk in bytes for all concurrently running user queries. Zero means unlimited.
     */
    maxTemporaryDataOnDiskSizeForUser: number;
    /**
     * Limits the maximum number of temporary columns that must be kept in RAM at the same time when running a query, excluding constant columns.
     */
    maxTemporaryNonConstColumns: number;
    /**
     * The maximum number of query processing threads, excluding threads for retrieving data from remote servers.
     */
    maxThreads: number;
    /**
     * It represents soft memory limit in case when hard limit is reached on user level. This value is used to compute overcommit ratio for the query. Zero means skip the query.
     */
    memoryOvercommitRatioDenominator: number;
    /**
     * It represents soft memory limit in case when hard limit is reached on global level. This value is used to compute overcommit ratio for the query. Zero means skip the query.
     */
    memoryOvercommitRatioDenominatorForUser: number;
    /**
     * Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type. The probability is for every alloc/free regardless to the size of the allocation. Possible values: from 0 to 1. Default: 0.
     */
    memoryProfilerSampleProbability: number;
    /**
     * Memory profiler step (in bytes).  If the next query step requires more memory than this parameter specifies, the memory profiler collects the allocating stack trace. Values lower than a few megabytes slow down query processing. Default value: 4194304 (4 MB). Zero means disabled memory profiler.
     */
    memoryProfilerStep: number;
    /**
     * Maximum time thread will wait for memory to be freed in the case of memory overcommit on a user level. If the timeout is reached and memory is not freed, an exception is thrown.
     */
    memoryUsageOvercommitMaxWaitMicroseconds: number;
    /**
     * If ClickHouse should read more than mergeTreeMaxBytesToUseCache bytes in one query, it doesn’t use the cache of uncompressed blocks.
     */
    mergeTreeMaxBytesToUseCache: number;
    /**
     * If ClickHouse should read more than mergeTreeMaxRowsToUseCache rows in one query, it doesn’t use the cache of uncompressed blocks.
     */
    mergeTreeMaxRowsToUseCache: number;
    /**
     * If the number of bytes to read from one file of a MergeTree-engine table exceeds merge_tree_min_bytes_for_concurrent_read, then ClickHouse tries to concurrently read from this file in several threads.
     */
    mergeTreeMinBytesForConcurrentRead: number;
    /**
     * If the number of rows to be read from a file of a MergeTree table exceeds mergeTreeMinRowsForConcurrentRead then ClickHouse tries to perform a concurrent reading from this file on several threads.
     */
    mergeTreeMinRowsForConcurrentRead: number;
    /**
     * The minimum data volume required for using direct I/O access to the storage disk.
     */
    minBytesToUseDirectIo: number;
    /**
     * How many times to potentially use a compiled chunk of code before running compilation.
     */
    minCountToCompile: number;
    /**
     * A query waits for expression compilation process to complete prior to continuing execution.
     */
    minCountToCompileExpression: number;
    /**
     * Minimal execution speed in rows per second.
     */
    minExecutionSpeed: number;
    /**
     * Minimal execution speed in bytes per second.
     */
    minExecutionSpeedBytes: number;
    /**
     * Sets the minimum number of bytes in the block which can be inserted into a table by an INSERT query.
     */
    minInsertBlockSizeBytes: number;
    /**
     * Sets the minimum number of rows in the block which can be inserted into a table by an INSERT query.
     */
    minInsertBlockSizeRows: number;
    /**
     * If the value is true, integers appear in quotes when using JSON* Int64 and UInt64 formats (for compatibility with most JavaScript implementations); otherwise, integers are output without the quotes.
     */
    outputFormatJsonQuote64bitIntegers: boolean;
    /**
     * Enables +nan, -nan, +inf, -inf outputs in JSON output format.
     */
    outputFormatJsonQuoteDenormals: boolean;
    /**
     * Enables/disables preferable using the localhost replica when processing distributed queries. Default value: true.
     */
    preferLocalhostReplica: boolean;
    /**
     * Query priority.
     */
    priority: number;
    /**
     * Quota accounting mode.
     */
    quotaMode: string;
    /**
     * Sets behaviour on overflow while read. Possible values:
     */
    readOverflowMode: string;
    /**
     * Restricts permissions for reading data, write data and change settings queries.
     */
    readonly: number;
    /**
     * Receive timeout in milliseconds on the socket used for communicating with the client.
     */
    receiveTimeout: number;
    /**
     * Method of reading data from remote filesystem, one of: `read`, `threadpool`.
     */
    remoteFilesystemReadMethod: string;
    /**
     * For ALTER ... ATTACH|DETACH|DROP queries, you can use the replicationAlterPartitionsSync setting to set up waiting.
     */
    replicationAlterPartitionsSync: number;
    /**
     * Sets behaviour on overflow in result. Possible values:
     */
    resultOverflowMode: string;
    /**
     * Enables or disables sequential consistency for SELECT queries.
     */
    selectSequentialConsistency: boolean;
    /**
     * Enables or disables X-ClickHouse-Progress HTTP response headers in clickhouse-server responses.
     */
    sendProgressInHttpHeaders: boolean;
    /**
     * Send timeout in milliseconds on the socket used for communicating with the client.
     */
    sendTimeout: number;
    /**
     * Sets behaviour on overflow in the set resulting. Possible values:
     */
    setOverflowMode: string;
    /**
     * Enables or disables silently skipping of unavailable shards.
     */
    skipUnavailableShards: boolean;
    /**
     * Sets behaviour on overflow while sort. Possible values:
     */
    sortOverflowMode: string;
    /**
     * Timeout (in seconds) between checks of execution speed. It is checked that execution speed is not less that specified in minExecutionSpeed parameter.
     * Must be at least 1000.
     */
    timeoutBeforeCheckingExecutionSpeed: number;
    /**
     * Sets behaviour on overflow. Possible values:
     */
    timeoutOverflowMode: string;
    /**
     * Sets behaviour on overflow. Possible values:
     */
    transferOverflowMode: string;
    /**
     * Enables equality of NULL values for IN operator.
     */
    transformNullIn: boolean;
    /**
     * Enables hedged requests logic for remote queries. It allows to establish many connections with different replicas for query. New connection is enabled in case existent connection(s) with replica(s) were not established within hedgedConnectionTimeout or no data was received within receive_data_timeout. Query uses the first connection which send non empty progress packet (or data packet, if allow_changing_replica_until_first_data_packet); other connections are cancelled. Queries with maxParallelReplicas > 1 are supported. Default value: true.
     */
    useHedgedRequests: boolean;
    /**
     * Whether to use a cache of uncompressed blocks.
     */
    useUncompressedCache: boolean;
    /**
     * Enables waiting for processing of asynchronous insertion. If enabled, server returns OK only after the data is inserted.
     */
    waitForAsyncInsert: boolean;
    /**
     * The timeout (in seconds) for waiting for processing of asynchronous insertion. Value must be at least 1000 (1 second).
     */
    waitForAsyncInsertTimeout: number;
}

export interface MdbClickhouseClusterZookeeper {
    /**
     * Resources allocated to hosts of the ZooKeeper subcluster. The structure is documented below.
     */
    resources: outputs.MdbClickhouseClusterZookeeperResources;
}

export interface MdbClickhouseClusterZookeeperResources {
    /**
     * Volume of the storage available to a host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of hosts.
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbElasticsearchClusterConfig {
    /**
     * Password for admin user of Elasticsearch.
     */
    adminPassword: string;
    /**
     * Configuration for Elasticsearch data nodes subcluster. The structure is documented below.
     */
    dataNode: outputs.MdbElasticsearchClusterConfigDataNode;
    /**
     * Edition of Elasticsearch. For more information, see [the official documentation](https://cloud.yandex.com/en-ru/docs/managed-elasticsearch/concepts/es-editions).
     */
    edition: string;
    /**
     * Configuration for Elasticsearch master nodes subcluster. The structure is documented below.
     */
    masterNode?: outputs.MdbElasticsearchClusterConfigMasterNode;
    /**
     * A set of Elasticsearch plugins to install.
     */
    plugins?: string[];
    /**
     * Version of Elasticsearch.
     */
    version: string;
}

export interface MdbElasticsearchClusterConfigDataNode {
    /**
     * Resources allocated to hosts of the Elasticsearch data nodes subcluster. The structure is documented below.
     */
    resources: outputs.MdbElasticsearchClusterConfigDataNodeResources;
}

export interface MdbElasticsearchClusterConfigDataNodeResources {
    /**
     * Volume of the storage available to a host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of Elasticsearch hosts.
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbElasticsearchClusterConfigMasterNode {
    /**
     * Resources allocated to hosts of the Elasticsearch master nodes subcluster. The structure is documented below.
     */
    resources: outputs.MdbElasticsearchClusterConfigMasterNodeResources;
}

export interface MdbElasticsearchClusterConfigMasterNodeResources {
    /**
     * Volume of the storage available to a host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of Elasticsearch hosts.
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbElasticsearchClusterHost {
    /**
     * Sets whether the host should get a public IP address on creation. Can be either `true` or `false`.
     */
    assignPublicIp?: boolean;
    /**
     * The fully qualified domain name of the host.
     */
    fqdn: string;
    /**
     * User defined host name.
     */
    name: string;
    /**
     * The ID of the subnet, to which the host belongs. The subnet must
     * be a part of the network to which the cluster belongs.
     */
    subnetId: string;
    /**
     * The type of the host to be deployed. Can be either `DATA_NODE` or `MASTER_NODE`.
     */
    type: string;
    /**
     * The availability zone where the Elasticsearch host will be created.
     * For more information see [the official documentation](https://cloud.yandex.com/docs/overview/concepts/geo-scope).
     */
    zone: string;
}

export interface MdbElasticsearchClusterMaintenanceWindow {
    /**
     * Day of week for maintenance window if window type is weekly. Possible values: `MON`, `TUE`, `WED`, `THU`, `FRI`, `SAT`, `SUN`.
     */
    day?: string;
    /**
     * Hour of day in UTC time zone (1-24) for maintenance window if window type is weekly.
     */
    hour?: number;
    /**
     * Type of maintenance window. Can be either `ANYTIME` or `WEEKLY`. A day and hour of window need to be specified with weekly window.
     */
    type: string;
}

export interface MdbGreenplumClusterAccess {
    /**
     * Allow access for [Yandex DataLens](https://cloud.yandex.com/services/datalens).
     */
    dataLens?: boolean;
    /**
     * Allow access for [DataTransfer](https://cloud.yandex.com/services/data-transfer)
     */
    dataTransfer?: boolean;
    /**
     * Allows access for [SQL queries in the management console](https://cloud.yandex.com/docs/managed-mysql/operations/web-sql-query).
     */
    webSql?: boolean;
}

export interface MdbGreenplumClusterBackupWindowStart {
    /**
     * The hour at which backup will be started (UTC).
     */
    hours?: number;
    /**
     * The minute at which backup will be started (UTC).
     */
    minutes?: number;
}

export interface MdbGreenplumClusterCloudStorage {
    /**
     * Whether to use cloud storage or not.
     */
    enable?: boolean;
}

export interface MdbGreenplumClusterMaintenanceWindow {
    /**
     * Day of the week (in `DDD` format). Allowed values: "MON", "TUE", "WED", "THU", "FRI", "SAT", "SUN"
     */
    day?: string;
    /**
     * Hour of the day in UTC (in `HH` format). Allowed value is between 0 and 23.
     */
    hour?: number;
    /**
     * Type of maintenance window. Can be either `ANYTIME` or `WEEKLY`. A day and hour of window need to be specified with weekly window.
     */
    type: string;
}

export interface MdbGreenplumClusterMasterHost {
    /**
     * Sets whether the master hosts should get a public IP address on creation. Changing this parameter for an existing host is not supported at the moment.
     */
    assignPublicIp: boolean;
    /**
     * (Computed) The fully qualified domain name of the host.
     */
    fqdn: string;
}

export interface MdbGreenplumClusterMasterSubcluster {
    /**
     * Resources allocated to hosts for master subcluster of the Greenplum cluster. The structure is documented below.
     */
    resources: outputs.MdbGreenplumClusterMasterSubclusterResources;
}

export interface MdbGreenplumClusterMasterSubclusterResources {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbGreenplumClusterPoolerConfig {
    /**
     * Value for `poolClientIdleTimeout` [parameter in Odyssey](https://github.com/yandex/odyssey/blob/master/documentation/configuration.md#pool_ttl-integer).
     */
    poolClientIdleTimeout?: number;
    /**
     * Value for `poolSize` [parameter in Odyssey](https://github.com/yandex/odyssey/blob/master/documentation/configuration.md#pool_size-integer).
     */
    poolSize?: number;
    /**
     * Mode that the connection pooler is working in. See descriptions of all modes in the [documentation for Odyssey](https://github.com/yandex/odyssey/blob/master/documentation/configuration.md#pool-string.
     */
    poolingMode?: string;
}

export interface MdbGreenplumClusterPxfConfig {
    /**
     * The Tomcat server connection timeout for read operations in seconds. Value is between 5 and 600.
     */
    connectionTimeout?: number;
    /**
     * The maximum number of PXF tomcat threads. Value is between 1 and 1024.
     */
    maxThreads?: number;
    /**
     * Identifies whether or not core streaming threads are allowed to time out.
     */
    poolAllowCoreThreadTimeout?: boolean;
    /**
     * The number of core streaming threads. Value is between 1 and 1024.
     */
    poolCoreSize?: number;
    /**
     * The maximum allowed number of core streaming threads. Value is between 1 and 1024.
     */
    poolMaxSize?: number;
    /**
     * The capacity of the core streaming thread pool queue. Value is positive.
     */
    poolQueueCapacity?: number;
    /**
     * The Tomcat server connection timeout for write operations in seconds. Value is between 5 and 600.
     */
    uploadTimeout?: number;
    /**
     * Maximum JVM heap size for PXF daemon. Value is between 64 and 16384.
     */
    xms?: number;
    /**
     * Initial JVM heap size for PXF daemon. Value is between 64 and 16384.
     */
    xmx?: number;
}

export interface MdbGreenplumClusterSegmentHost {
    /**
     * (Computed) The fully qualified domain name of the host.
     */
    fqdn: string;
}

export interface MdbGreenplumClusterSegmentSubcluster {
    /**
     * Resources allocated to hosts for segment subcluster of the Greenplum cluster. The structure is documented below.
     */
    resources: outputs.MdbGreenplumClusterSegmentSubclusterResources;
}

export interface MdbGreenplumClusterSegmentSubclusterResources {
    diskSize: number;
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbKafkaClusterConfig {
    /**
     * Access policy to the Kafka cluster. The structure is documented below.
     *
     * - - -
     */
    access: outputs.MdbKafkaClusterConfigAccess;
    /**
     * Determines whether each broker will be assigned a public IP address. The default is `false`.
     */
    assignPublicIp?: boolean;
    /**
     * Count of brokers per availability zone. The default is `1`.
     */
    brokersCount?: number;
    diskSizeAutoscaling: outputs.MdbKafkaClusterConfigDiskSizeAutoscaling;
    /**
     * Configuration of the Kafka subcluster. The structure is documented below.
     */
    kafka: outputs.MdbKafkaClusterConfigKafka;
    /**
     * Enables managed schema registry on cluster. The default is `false`.
     */
    schemaRegistry?: boolean;
    /**
     * @deprecated The 'unmanaged_topics' field has been deprecated, because feature enabled permanently and can't be disabled.
     */
    unmanagedTopics?: boolean;
    /**
     * Version of the Kafka server software.
     */
    version: string;
    /**
     * List of availability zones.
     */
    zones: string[];
    /**
     * Configuration of the ZooKeeper subcluster. The structure is documented below.
     */
    zookeeper: outputs.MdbKafkaClusterConfigZookeeper;
}

export interface MdbKafkaClusterConfigAccess {
    /**
     * Allow access for [DataTransfer](https://cloud.yandex.com/services/data-transfer)
     *
     * The `user` block is deprecated. To manage users, please switch to using a separate resource type
     * `yandex.MdbKafkaUser`. The `user` block supports:
     */
    dataTransfer?: boolean;
}

export interface MdbKafkaClusterConfigDiskSizeAutoscaling {
    diskSizeLimit: number;
    emergencyUsageThreshold?: number;
    plannedUsageThreshold?: number;
}

export interface MdbKafkaClusterConfigKafka {
    /**
     * User-defined settings for the Kafka cluster. The structure is documented below.
     */
    kafkaConfig?: outputs.MdbKafkaClusterConfigKafkaKafkaConfig;
    /**
     * Resources allocated to hosts of the Kafka subcluster. The structure is documented below.
     */
    resources: outputs.MdbKafkaClusterConfigKafkaResources;
}

export interface MdbKafkaClusterConfigKafkaKafkaConfig {
    autoCreateTopicsEnable?: boolean;
    /**
     * , `logFlushIntervalMessages`, `logFlushIntervalMs`, `logFlushSchedulerIntervalMs`, `logRetentionBytes`, `logRetentionHours`,
     * `logRetentionMinutes`, `logRetentionMs`, `logSegmentBytes`, `logPreallocate`, `socketSendBufferBytes`, `socketReceiveBufferBytes`, `autoCreateTopicsEnable`,
     * `numPartitions`, `defaultReplicationFactor`, `messageMaxBytes`, `replicaFetchMaxBytes`, `sslCipherSuites`, `offsetsRetentionMinutes`, `saslEnabledMechanisms` - (Optional) Kafka server settings. For more information, see
     * [the official documentation](https://cloud.yandex.com/docs/managed-kafka/operations/cluster-update)
     * and [the Kafka documentation](https://kafka.apache.org/documentation/#configuration).
     */
    compressionType?: string;
    defaultReplicationFactor?: string;
    logFlushIntervalMessages?: string;
    logFlushIntervalMs?: string;
    logFlushSchedulerIntervalMs?: string;
    logPreallocate?: boolean;
    logRetentionBytes?: string;
    logRetentionHours?: string;
    logRetentionMinutes?: string;
    logRetentionMs?: string;
    logSegmentBytes?: string;
    messageMaxBytes?: string;
    numPartitions?: string;
    offsetsRetentionMinutes?: string;
    replicaFetchMaxBytes?: string;
    saslEnabledMechanisms?: string[];
    socketReceiveBufferBytes?: string;
    socketSendBufferBytes?: string;
    sslCipherSuites?: string[];
}

export interface MdbKafkaClusterConfigKafkaResources {
    /**
     * Volume of the storage available to a ZooKeeper host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of ZooKeeper hosts.
     * For more information see [the official documentation](https://cloud.yandex.com/docs/managed-kafka/concepts/storage).
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbKafkaClusterConfigZookeeper {
    /**
     * Resources allocated to hosts of the ZooKeeper subcluster. The structure is documented below.
     */
    resources: outputs.MdbKafkaClusterConfigZookeeperResources;
}

export interface MdbKafkaClusterConfigZookeeperResources {
    /**
     * Volume of the storage available to a ZooKeeper host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of ZooKeeper hosts.
     * For more information see [the official documentation](https://cloud.yandex.com/docs/managed-kafka/concepts/storage).
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbKafkaClusterHost {
    /**
     * Determines whether each broker will be assigned a public IP address. The default is `false`.
     */
    assignPublicIp: boolean;
    /**
     * Health of the host.
     */
    health: string;
    /**
     * Name of the Kafka cluster. Provided by the client when the cluster is created.
     */
    name: string;
    /**
     * The role type to grant to the topic.
     */
    role: string;
    /**
     * The ID of the subnet, to which the host belongs.
     */
    subnetId: string;
    /**
     * The availability zone where the Kafka host was created.
     */
    zoneId: string;
}

export interface MdbKafkaClusterMaintenanceWindow {
    /**
     * Day of the week (in `DDD` format). Allowed values: "MON", "TUE", "WED", "THU", "FRI", "SAT", "SUN"
     */
    day?: string;
    /**
     * Hour of the day in UTC (in `HH` format). Allowed value is between 1 and 24.
     *
     * - - -
     */
    hour?: number;
    /**
     * Type of maintenance window. Can be either `ANYTIME` or `WEEKLY`. A day and hour of window need to be specified with weekly window.
     */
    type: string;
}

export interface MdbKafkaClusterTopic {
    /**
     * Name of the Kafka cluster. Provided by the client when the cluster is created.
     */
    name: string;
    /**
     * The number of the topic's partitions.
     */
    partitions: number;
    /**
     * Amount of data copies (replicas) for the topic in the cluster.
     */
    replicationFactor: number;
    /**
     * User-defined settings for the topic. The structure is documented below.
     */
    topicConfig?: outputs.MdbKafkaClusterTopicTopicConfig;
}

export interface MdbKafkaClusterTopicTopicConfig {
    cleanupPolicy?: string;
    /**
     * , `deleteRetentionMs`, `fileDeleteDelayMs`, `flushMessages`, `flushMs`, `minCompactionLagMs`,
     * `retentionBytes`, `retentionMs`, `maxMessageBytes`, `minInsyncReplicas`, `segmentBytes`, `preallocate`, - (Optional) Kafka topic settings. For more information, see
     * [the official documentation](https://cloud.yandex.com/docs/managed-kafka/operations/cluster-topics#update-topic)
     * and [the Kafka documentation](https://kafka.apache.org/documentation/#configuration).
     */
    compressionType?: string;
    deleteRetentionMs?: string;
    fileDeleteDelayMs?: string;
    flushMessages?: string;
    flushMs?: string;
    maxMessageBytes?: string;
    minCompactionLagMs?: string;
    minInsyncReplicas?: string;
    preallocate?: boolean;
    retentionBytes?: string;
    retentionMs?: string;
    segmentBytes?: string;
}

export interface MdbKafkaClusterUser {
    /**
     * Name of the Kafka cluster. Provided by the client when the cluster is created.
     */
    name: string;
    /**
     * The password of the user.
     */
    password: string;
    /**
     * Set of permissions granted to the user. The structure is documented below.
     */
    permissions?: outputs.MdbKafkaClusterUserPermission[];
}

export interface MdbKafkaClusterUserPermission {
    /**
     * Set of hosts, to which this permission grants access to.
     *
     * The `topic` block is deprecated. To manage topics, please switch to using a separate resource type
     * `yandex.MdbKafkaTopic`. The `topic` block supports:
     */
    allowHosts?: string[];
    /**
     * The role type to grant to the topic.
     */
    role: string;
    /**
     * The name of the topic that the permission grants access to.
     */
    topicName: string;
}

export interface MdbKafkaConnectorConnectorConfigMirrormaker {
    replicationFactor: number;
    sourceCluster: outputs.MdbKafkaConnectorConnectorConfigMirrormakerSourceCluster;
    targetCluster: outputs.MdbKafkaConnectorConnectorConfigMirrormakerTargetCluster;
    topics: string;
}

export interface MdbKafkaConnectorConnectorConfigMirrormakerSourceCluster {
    alias?: string;
    externalClusters?: outputs.MdbKafkaConnectorConnectorConfigMirrormakerSourceClusterExternalCluster[];
    thisClusters?: outputs.MdbKafkaConnectorConnectorConfigMirrormakerSourceClusterThisCluster[];
}

export interface MdbKafkaConnectorConnectorConfigMirrormakerSourceClusterExternalCluster {
    bootstrapServers: string;
    saslMechanism?: string;
    saslPassword?: string;
    saslUsername?: string;
    securityProtocol?: string;
}

export interface MdbKafkaConnectorConnectorConfigMirrormakerSourceClusterThisCluster {
}

export interface MdbKafkaConnectorConnectorConfigMirrormakerTargetCluster {
    alias?: string;
    externalClusters?: outputs.MdbKafkaConnectorConnectorConfigMirrormakerTargetClusterExternalCluster[];
    thisClusters?: outputs.MdbKafkaConnectorConnectorConfigMirrormakerTargetClusterThisCluster[];
}

export interface MdbKafkaConnectorConnectorConfigMirrormakerTargetClusterExternalCluster {
    bootstrapServers: string;
    saslMechanism?: string;
    saslPassword?: string;
    saslUsername?: string;
    securityProtocol?: string;
}

export interface MdbKafkaConnectorConnectorConfigMirrormakerTargetClusterThisCluster {
}

export interface MdbKafkaConnectorConnectorConfigS3Sink {
    fileCompressionType: string;
    fileMaxRecords?: number;
    s3Connection: outputs.MdbKafkaConnectorConnectorConfigS3SinkS3Connection;
    topics: string;
}

export interface MdbKafkaConnectorConnectorConfigS3SinkS3Connection {
    bucketName: string;
    externalS3s: outputs.MdbKafkaConnectorConnectorConfigS3SinkS3ConnectionExternalS3[];
}

export interface MdbKafkaConnectorConnectorConfigS3SinkS3ConnectionExternalS3 {
    accessKeyId?: string;
    endpoint: string;
    region?: string;
    secretAccessKey?: string;
}

export interface MdbKafkaTopicTopicConfig {
    /**
     * , `compressionType`, `deleteRetentionMs`, `fileDeleteDelayMs`, `flushMessages`, `flushMs`, 
     * `minCompactionLagMs`, `retentionBytes`, `retentionMs`, `maxMessageBytes`, `minInsyncReplicas`,
     * `segmentBytes`, `preallocate` - (Optional) Kafka topic settings. For more information, see
     * [the official documentation](https://cloud.yandex.com/en-ru/docs/managed-kafka/concepts/settings-list#topic-settings)
     * and [the Kafka documentation](https://kafka.apache.org/documentation/#topicconfigs).
     */
    cleanupPolicy?: string;
    compressionType?: string;
    deleteRetentionMs?: string;
    fileDeleteDelayMs?: string;
    flushMessages?: string;
    flushMs?: string;
    maxMessageBytes?: string;
    minCompactionLagMs?: string;
    minInsyncReplicas?: string;
    preallocate?: boolean;
    retentionBytes?: string;
    retentionMs?: string;
    segmentBytes?: string;
}

export interface MdbKafkaUserPermission {
    /**
     * Set of hosts, to which this permission grants access to.
     */
    allowHosts?: string[];
    /**
     * The role type to grant to the topic.
     */
    role: string;
    /**
     * The name of the topic that the permission grants access to.
     */
    topicName: string;
}

export interface MdbMongodbClusterClusterConfig {
    /**
     * Access policy to the MongoDB cluster. The structure is documented below.
     */
    access: outputs.MdbMongodbClusterClusterConfigAccess;
    /**
     * Retain period of automatically created backup in days.
     */
    backupRetainPeriodDays: number;
    /**
     * Time to start the daily backup, in the UTC timezone. The structure is documented below.
     */
    backupWindowStart: outputs.MdbMongodbClusterClusterConfigBackupWindowStart;
    /**
     * Feature compatibility version of MongoDB. If not provided version is taken. Can be either `6.0`, `5.0`, `4.4` and `4.2`.
     */
    featureCompatibilityVersion: string;
    /**
     * Configuration of the mongocfg service. The structure is documented below.
     */
    mongocfg: outputs.MdbMongodbClusterClusterConfigMongocfg;
    /**
     * Configuration of the mongod service. The structure is documented below.
     */
    mongod: outputs.MdbMongodbClusterClusterConfigMongod;
    /**
     * Configuration of the mongos service. The structure is documented below.
     */
    mongos: outputs.MdbMongodbClusterClusterConfigMongos;
    /**
     * Performance diagnostics to the MongoDB cluster. The structure is documented below.
     */
    performanceDiagnostics: outputs.MdbMongodbClusterClusterConfigPerformanceDiagnostics;
    /**
     * Version of the MongoDB server software. Can be either `4.2`, `4.4`, `4.4-enterprise`, `5.0`, `5.0-enterprise`, `6.0` and `6.0-enterprise`.
     */
    version: string;
}

export interface MdbMongodbClusterClusterConfigAccess {
    /**
     * Allow access for [Yandex DataLens](https://cloud.yandex.com/services/datalens).
     */
    dataLens?: boolean;
    /**
     * Allow access for [DataTransfer](https://cloud.yandex.com/services/data-transfer)
     */
    dataTransfer?: boolean;
}

export interface MdbMongodbClusterClusterConfigBackupWindowStart {
    /**
     * The hour at which backup will be started.
     */
    hours?: number;
    /**
     * The minute at which backup will be started.
     */
    minutes?: number;
}

export interface MdbMongodbClusterClusterConfigMongocfg {
    /**
     * A set of network settings
     * (see the [net](https://www.mongodb.com/docs/manual/reference/configuration-options/#net-options) option).
     * The structure is documented below.
     */
    net?: outputs.MdbMongodbClusterClusterConfigMongocfgNet;
    /**
     * A set of profiling settings
     * (see the [operationProfiling](https://www.mongodb.com/docs/manual/reference/configuration-options/#operationprofiling-options) option).
     * The structure is documented below.
     */
    operationProfiling?: outputs.MdbMongodbClusterClusterConfigMongocfgOperationProfiling;
    /**
     * A set of storage settings
     * (see the [storage](https://www.mongodb.com/docs/manual/reference/configuration-options/#storage-options) option).
     * The structure is documented below.
     */
    storage?: outputs.MdbMongodbClusterClusterConfigMongocfgStorage;
}

export interface MdbMongodbClusterClusterConfigMongocfgNet {
    /**
     * The maximum number of simultaneous connections that host will accept.
     * For more information, see the [net.maxIncomingConnections](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-net.maxIncomingConnections)
     * description in the official documentation.
     */
    maxIncomingConnections?: number;
}

export interface MdbMongodbClusterClusterConfigMongocfgOperationProfiling {
    /**
     * Specifies which operations should be profiled. The following profiler levels are available: off, slow_op, all.
     * For more information, see the [operationProfiling.mode](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-operationProfiling.mode)
     * description in the official documentation.
     */
    mode?: string;
    /**
     * The slow operation time threshold, in milliseconds. Operations that run for longer than this threshold are considered slow.
     * For more information, see the [operationProfiling.slowOpThresholdMs](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-operationProfiling.slowOpThresholdMs)
     * description in the official documentation.
     */
    slowOpThreshold?: number;
}

export interface MdbMongodbClusterClusterConfigMongocfgStorage {
    /**
     * The WiredTiger engine settings.
     * (see the [storage.wiredTiger](https://www.mongodb.com/docs/manual/reference/configuration-options/#storage.wiredtiger-options) option).
     * These settings available only on `mongod` hosts. The structure is documented below.
     */
    wiredTiger?: outputs.MdbMongodbClusterClusterConfigMongocfgStorageWiredTiger;
}

export interface MdbMongodbClusterClusterConfigMongocfgStorageWiredTiger {
    /**
     * Defines the maximum size of the internal cache that WiredTiger will use for all data.
     * For more information, see the [storage.wiredTiger.engineConfig.cacheSizeGB](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-storage.wiredTiger.engineConfig.cacheSizeGB)
     * description in the official documentation.
     */
    cacheSizeGb?: number;
}

export interface MdbMongodbClusterClusterConfigMongod {
    /**
     * A set of audit log settings 
     * (see the [auditLog](https://www.mongodb.com/docs/manual/reference/configuration-options/#auditlog-options) option).
     * The structure is documented below. Available only in enterprise edition.
     */
    auditLog: outputs.MdbMongodbClusterClusterConfigMongodAuditLog;
    /**
     * A set of network settings
     * (see the [net](https://www.mongodb.com/docs/manual/reference/configuration-options/#net-options) option).
     * The structure is documented below.
     */
    net?: outputs.MdbMongodbClusterClusterConfigMongodNet;
    /**
     * A set of profiling settings
     * (see the [operationProfiling](https://www.mongodb.com/docs/manual/reference/configuration-options/#operationprofiling-options) option).
     * The structure is documented below.
     */
    operationProfiling?: outputs.MdbMongodbClusterClusterConfigMongodOperationProfiling;
    /**
     * A set of MongoDB Security settings
     * (see the [security](https://www.mongodb.com/docs/manual/reference/configuration-options/#security-options) option).
     * The structure is documented below. Available only in enterprise edition.
     */
    security: outputs.MdbMongodbClusterClusterConfigMongodSecurity;
    /**
     * A set of MongoDB Server Parameters 
     * (see the [setParameter](https://www.mongodb.com/docs/manual/reference/configuration-options/#setparameter-option) option).
     * The structure is documented below.
     */
    setParameter: outputs.MdbMongodbClusterClusterConfigMongodSetParameter;
    /**
     * A set of storage settings
     * (see the [storage](https://www.mongodb.com/docs/manual/reference/configuration-options/#storage-options) option).
     * The structure is documented below.
     */
    storage?: outputs.MdbMongodbClusterClusterConfigMongodStorage;
}

export interface MdbMongodbClusterClusterConfigMongodAuditLog {
    /**
     * Configuration of the audit log filter in JSON format.
     * For more information see [auditLog.filter](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-auditLog.filter)
     * description in the official documentation. Available only in enterprise edition.
     */
    filter?: string;
    /**
     * Specifies if a node allows runtime configuration of audit filters and the auditAuthorizationSuccess variable.
     * For more information see [auditLog.runtimeConfiguration](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-auditLog.runtimeConfiguration)
     * description in the official documentation. Available only in enterprise edition.
     */
    runtimeConfiguration?: boolean;
}

export interface MdbMongodbClusterClusterConfigMongodNet {
    /**
     * Specifies the default compressor(s) to use for communication between this mongod or mongos. 
     * Accepts array of compressors. Order matters. Available compressors: snappy, zlib, zstd, disabled. To disable network compression, make "disabled" the only value.
     * For more information, see the [net.Compression.Compressors](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-net.compression.compressors)
     * description in the official documentation.
     */
    compressors?: string[];
    /**
     * The maximum number of simultaneous connections that host will accept.
     * For more information, see the [net.maxIncomingConnections](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-net.maxIncomingConnections)
     * description in the official documentation.
     */
    maxIncomingConnections?: number;
}

export interface MdbMongodbClusterClusterConfigMongodOperationProfiling {
    /**
     * Specifies which operations should be profiled. The following profiler levels are available: off, slow_op, all.
     * For more information, see the [operationProfiling.mode](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-operationProfiling.mode)
     * description in the official documentation.
     */
    mode?: string;
    /**
     * The fraction of slow operations that should be profiled or logged. Accepts values between 0 and 1, inclusive.
     * For more information, see the [operationProfiling.slowOpSampleRate](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-operationProfiling.slowOpSampleRate)
     * description in the official documentation.
     */
    slowOpSampleRate?: number;
    /**
     * The slow operation time threshold, in milliseconds. Operations that run for longer than this threshold are considered slow.
     * For more information, see the [operationProfiling.slowOpThresholdMs](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-operationProfiling.slowOpThresholdMs)
     * description in the official documentation.
     */
    slowOpThreshold?: number;
}

export interface MdbMongodbClusterClusterConfigMongodSecurity {
    /**
     * Enables the encryption for the WiredTiger storage engine. Can be either true or false.
     * For more information see [security.enableEncryption](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-security.enableEncryption)
     * description in the official documentation. Available only in enterprise edition.
     */
    enableEncryption?: boolean;
    /**
     * Configuration of the third party key management appliance via the Key Management Interoperability Protocol (KMIP)
     * (see [Encryption tutorial](https://www.mongodb.com/docs/rapid/tutorial/configure-encryption) ). Requires `enableEncryption` to be true.
     * The structure is documented below. Available only in enterprise edition.
     */
    kmip: outputs.MdbMongodbClusterClusterConfigMongodSecurityKmip;
}

export interface MdbMongodbClusterClusterConfigMongodSecurityKmip {
    /**
     * String containing the client certificate used for authenticating MongoDB to the KMIP server.
     * For more information see [security.kmip.clientCertificateFile](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-security.kmip.clientCertificateFile)
     * description in the official documentation.
     */
    clientCertificate?: string;
    /**
     * Unique KMIP identifier for an existing key within the KMIP server.
     * For more information see [security.kmip.keyIdentifier](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-security.kmip.keyIdentifier)
     * description in the official documentation.
     */
    keyIdentifier?: string;
    /**
     * Port number to use to communicate with the KMIP server. Default: 5696
     * For more information see [security.kmip.port](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-security.kmip.port)
     * description in the official documentation.
     */
    port?: number;
    /**
     * Path to CA File. Used for validating secure client connection to KMIP server.
     * For more information see [security.kmip.serverCAFile](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-security.kmip.serverCAFile)
     * description in the official documentation.
     */
    serverCa?: string;
    /**
     * Hostname or IP address of the KMIP server to connect to.
     * For more information see [security.kmip.serverName](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-security.kmip.serverName)
     * description in the official documentation.
     */
    serverName?: string;
}

export interface MdbMongodbClusterClusterConfigMongodSetParameter {
    /**
     * Enables the auditing of authorization successes. Can be either true or false.
     * For more information, see the [auditAuthorizationSuccess](https://www.mongodb.com/docs/manual/reference/parameters/#mongodb-parameter-param.auditAuthorizationSuccess)
     * description in the official documentation. Available only in enterprise edition.
     */
    auditAuthorizationSuccess?: boolean;
    /**
     * Enables the flow control. Can be either true or false.
     * For more information, see the [enableFlowControl](https://www.mongodb.com/docs/rapid/reference/parameters/#mongodb-parameter-param.enableFlowControl)
     * description in the official documentation.
     */
    enableFlowControl?: boolean;
    /**
     * The minimum time window in seconds for which the storage engine keeps the snapshot history.
     * For more information, see the [minSnapshotHistoryWindowInSeconds](https://www.mongodb.com/docs/manual/reference/parameters/#mongodb-parameter-param.minSnapshotHistoryWindowInSeconds)
     * description in the official documentation.
     */
    minSnapshotHistoryWindowInSeconds?: number;
}

export interface MdbMongodbClusterClusterConfigMongodStorage {
    /**
     * The durability journal to ensure data files remain valid and recoverable.
     * The structure is documented below.
     */
    journal?: outputs.MdbMongodbClusterClusterConfigMongodStorageJournal;
    /**
     * The WiredTiger engine settings.
     * (see the [storage.wiredTiger](https://www.mongodb.com/docs/manual/reference/configuration-options/#storage.wiredtiger-options) option).
     * These settings available only on `mongod` hosts. The structure is documented below.
     */
    wiredTiger?: outputs.MdbMongodbClusterClusterConfigMongodStorageWiredTiger;
}

export interface MdbMongodbClusterClusterConfigMongodStorageJournal {
    /**
     * The maximum amount of time in milliseconds that the mongod process allows between journal operations.
     * For more information, see the [storage.journal.commitIntervalMs](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-storage.journal.commitIntervalMs)
     * description in the official documentation.
     */
    commitInterval?: number;
}

export interface MdbMongodbClusterClusterConfigMongodStorageWiredTiger {
    /**
     * Specifies the default compression for collection data. You can override this on a per-collection basis when creating collections.
     * Available compressors are: none, snappy, zlib, zstd. This setting available only on `mongod` hosts.
     * For more information, see the [storage.wiredTiger.collectionConfig.blockCompressor](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-storage.wiredTiger.collectionConfig.blockCompressor)
     * description in the official documentation.
     */
    blockCompressor?: string;
    /**
     * Defines the maximum size of the internal cache that WiredTiger will use for all data.
     * For more information, see the [storage.wiredTiger.engineConfig.cacheSizeGB](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-storage.wiredTiger.engineConfig.cacheSizeGB)
     * description in the official documentation.
     */
    cacheSizeGb?: number;
    /**
     * Enables or disables prefix compression for index data. Сan be either true or false.
     * For more information, see the [storage.wiredTiger.indexConfig.prefixCompression](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-storage.wiredTiger.indexConfig.prefixCompression)
     * description in the official documentation.
     */
    prefixCompression?: boolean;
}

export interface MdbMongodbClusterClusterConfigMongos {
    /**
     * A set of network settings
     * (see the [net](https://www.mongodb.com/docs/manual/reference/configuration-options/#net-options) option).
     * The structure is documented below.
     */
    net?: outputs.MdbMongodbClusterClusterConfigMongosNet;
}

export interface MdbMongodbClusterClusterConfigMongosNet {
    /**
     * Specifies the default compressor(s) to use for communication between this mongod or mongos. 
     * Accepts array of compressors. Order matters. Available compressors: snappy, zlib, zstd, disabled. To disable network compression, make "disabled" the only value.
     * For more information, see the [net.Compression.Compressors](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-net.compression.compressors)
     * description in the official documentation.
     */
    compressors?: string[];
    /**
     * The maximum number of simultaneous connections that host will accept.
     * For more information, see the [net.maxIncomingConnections](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-net.maxIncomingConnections)
     * description in the official documentation.
     */
    maxIncomingConnections?: number;
}

export interface MdbMongodbClusterClusterConfigPerformanceDiagnostics {
    /**
     * Enable or disable performance diagnostics.
     */
    enabled?: boolean;
}

export interface MdbMongodbClusterDatabase {
    /**
     * The name of the database.
     */
    name: string;
}

export interface MdbMongodbClusterDiskSizeAutoscalingMongocfg {
    /**
     * Limit of disk size after autoscaling (GiB).
     */
    diskSizeLimit: number;
    /**
     * Immediate autoscaling disk usage (percent).
     */
    emergencyUsageThreshold?: number;
    /**
     * Maintenance window autoscaling disk usage (percent).
     */
    plannedUsageThreshold?: number;
}

export interface MdbMongodbClusterDiskSizeAutoscalingMongod {
    /**
     * Limit of disk size after autoscaling (GiB).
     */
    diskSizeLimit: number;
    /**
     * Immediate autoscaling disk usage (percent).
     */
    emergencyUsageThreshold?: number;
    /**
     * Maintenance window autoscaling disk usage (percent).
     */
    plannedUsageThreshold?: number;
}

export interface MdbMongodbClusterDiskSizeAutoscalingMongoinfra {
    /**
     * Limit of disk size after autoscaling (GiB).
     */
    diskSizeLimit: number;
    /**
     * Immediate autoscaling disk usage (percent).
     */
    emergencyUsageThreshold?: number;
    /**
     * Maintenance window autoscaling disk usage (percent).
     */
    plannedUsageThreshold?: number;
}

export interface MdbMongodbClusterDiskSizeAutoscalingMongos {
    /**
     * Limit of disk size after autoscaling (GiB).
     */
    diskSizeLimit: number;
    /**
     * Immediate autoscaling disk usage (percent).
     */
    emergencyUsageThreshold?: number;
    /**
     * Maintenance window autoscaling disk usage (percent).
     */
    plannedUsageThreshold?: number;
}

export interface MdbMongodbClusterHost {
    /**
     * Should this host have assigned public IP assigned. Can be either `true` or `false`.
     */
    assignPublicIp?: boolean;
    /**
     * The health of the host.
     */
    health: string;
    /**
     * The parameters of mongod host in replicaset.
     */
    hostParameters: outputs.MdbMongodbClusterHostHostParameters;
    /**
     * The fully qualified domain name of the host. Computed on server side.
     */
    name: string;
    /**
     * The role of the cluster (either PRIMARY or SECONDARY).
     */
    role: string;
    /**
     * The name of the shard to which the host belongs. Only for sharded cluster.
     */
    shardName: string;
    /**
     * The ID of the subnet, to which the host belongs. The subnet must
     * be a part of the network to which the cluster belongs.
     */
    subnetId: string;
    /**
     * type of mongo daemon which runs on this host (mongod, mongos, mongocfg, mongoinfra). Defaults to mongod.
     */
    type?: string;
    /**
     * The availability zone where the MongoDB host will be created.
     * For more information see [the official documentation](https://cloud.yandex.com/docs/overview/concepts/geo-scope).
     */
    zoneId: string;
}

export interface MdbMongodbClusterHostHostParameters {
    /**
     * Should this host be hidden in replicaset. Can be either `true` of `false`. For more information see [the official documentation](https://www.mongodb.com/docs/current/reference/replica-configuration/#mongodb-rsconf-rsconf.members-n-.hidden)
     */
    hidden?: boolean;
    /**
     * A floating point number that indicates the relative likelihood of a replica set member to become the primary. For more information see [the official documentation](https://www.mongodb.com/docs/current/reference/replica-configuration/#mongodb-rsconf-rsconf.members-n-.priority)
     */
    priority?: number;
    /**
     * The number of seconds "behind" the primary that this replica set member should "lag". For more information see [the official documentation](https://www.mongodb.com/docs/current/reference/replica-configuration/#mongodb-rsconf-rsconf.members-n-.secondaryDelaySecs)
     */
    secondaryDelaySecs?: number;
    /**
     * A set of key/value pairs to assign for the replica set member. For more information see [the official documentation](https://www.mongodb.com/docs/current/reference/replica-configuration/#mongodb-rsconf-rsconf.members-n-.tags)
     */
    tags?: {[key: string]: string};
}

export interface MdbMongodbClusterMaintenanceWindow {
    /**
     * Day of week for maintenance window if window type is weekly. Possible values: `MON`, `TUE`, `WED`, `THU`, `FRI`, `SAT`, `SUN`.
     */
    day?: string;
    /**
     * Hour of day in UTC time zone (1-24) for maintenance window if window type is weekly.
     */
    hour?: number;
    /**
     * Type of maintenance window. Can be either `ANYTIME` or `WEEKLY`. A day and hour of window need to be specified with weekly window.
     */
    type: string;
}

export interface MdbMongodbClusterResources {
    /**
     * Volume of the storage available to a MongoDB host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of MongoDB hosts.
     * For more information see [the official documentation](https://cloud.yandex.com/docs/managed-clickhouse/concepts/storage).
     *
     * The `diskSizeAutoscalingMongod`, `diskSizeAutoscalingMongos`, `diskSizeAutoscalingMongoinfra`, `diskSizeAutoscalingMongocfg` blocks support:
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbMongodbClusterResourcesMongocfg {
    /**
     * Volume of the storage available to a MongoDB host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of MongoDB hosts.
     * For more information see [the official documentation](https://cloud.yandex.com/docs/managed-clickhouse/concepts/storage).
     *
     * The `diskSizeAutoscalingMongod`, `diskSizeAutoscalingMongos`, `diskSizeAutoscalingMongoinfra`, `diskSizeAutoscalingMongocfg` blocks support:
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbMongodbClusterResourcesMongod {
    /**
     * Volume of the storage available to a MongoDB host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of MongoDB hosts.
     * For more information see [the official documentation](https://cloud.yandex.com/docs/managed-clickhouse/concepts/storage).
     *
     * The `diskSizeAutoscalingMongod`, `diskSizeAutoscalingMongos`, `diskSizeAutoscalingMongoinfra`, `diskSizeAutoscalingMongocfg` blocks support:
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbMongodbClusterResourcesMongoinfra {
    /**
     * Volume of the storage available to a MongoDB host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of MongoDB hosts.
     * For more information see [the official documentation](https://cloud.yandex.com/docs/managed-clickhouse/concepts/storage).
     *
     * The `diskSizeAutoscalingMongod`, `diskSizeAutoscalingMongos`, `diskSizeAutoscalingMongoinfra`, `diskSizeAutoscalingMongocfg` blocks support:
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbMongodbClusterResourcesMongos {
    /**
     * Volume of the storage available to a MongoDB host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of MongoDB hosts.
     * For more information see [the official documentation](https://cloud.yandex.com/docs/managed-clickhouse/concepts/storage).
     *
     * The `diskSizeAutoscalingMongod`, `diskSizeAutoscalingMongos`, `diskSizeAutoscalingMongoinfra`, `diskSizeAutoscalingMongocfg` blocks support:
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbMongodbClusterRestore {
    /**
     * Backup ID. The cluster will be created from the specified backup. [How to get a list of PostgreSQL backups](https://cloud.yandex.com/en-ru/docs/managed-mongodb/operations/cluster-backups)
     */
    backupId: string;
    /**
     * Timestamp of the moment to which the MongoDB cluster should be restored. (Format: "2006-01-02T15:04:05" - UTC). When not set, current time is used.
     */
    time?: string;
}

export interface MdbMongodbClusterUser {
    /**
     * The name of the user.
     */
    name: string;
    /**
     * The password of the user.
     */
    password: string;
    /**
     * Set of permissions granted to the user. The structure is documented below.
     */
    permissions: outputs.MdbMongodbClusterUserPermission[];
}

export interface MdbMongodbClusterUserPermission {
    /**
     * The name of the database that the permission grants access to.
     */
    databaseName: string;
    /**
     * The roles of the user in this database. For more information see [the official documentation](https://cloud.yandex.com/docs/managed-mongodb/concepts/users-and-roles).
     */
    roles?: string[];
}

export interface MdbMysqlClusterAccess {
    /**
     * Allow access for [Yandex DataLens](https://cloud.yandex.com/services/datalens).
     */
    dataLens?: boolean;
    /**
     * Allow access for [DataTransfer](https://cloud.yandex.com/services/data-transfer)
     */
    dataTransfer?: boolean;
    /**
     * Allows access for [SQL queries in the management console](https://cloud.yandex.com/docs/managed-mysql/operations/web-sql-query).
     */
    webSql?: boolean;
}

export interface MdbMysqlClusterBackupWindowStart {
    /**
     * The hour at which backup will be started.
     */
    hours?: number;
    /**
     * The minute at which backup will be started.
     */
    minutes?: number;
}

export interface MdbMysqlClusterDatabase {
    /**
     * The name of the database.
     */
    name: string;
}

export interface MdbMysqlClusterHost {
    /**
     * Sets whether the host should get a public IP address. It can be changed on the fly only when `name` is set.
     */
    assignPublicIp?: boolean;
    /**
     * Host backup priority. Value is between 0 and 100, default is 0.
     */
    backupPriority?: number;
    /**
     * The fully qualified domain name of the host.
     */
    fqdn: string;
    /**
     * Host state name. It should be set for all hosts or unset for all hosts. This field can be used by another host, to select which host will be its replication source. Please refer to `replicationSourceName` parameter.
     */
    name?: string;
    /**
     * Host master promotion priority. Value is between 0 and 100, default is 0.
     */
    priority?: number;
    /**
     * Host replication source (fqdn), when replicationSource is empty then host is in HA group.
     */
    replicationSource: string;
    /**
     * Host replication source name points to host's `name` from which this host should replicate. When not set then host in HA group. It works only when `name` is set.
     */
    replicationSourceName?: string;
    /**
     * The ID of the subnet, to which the host belongs. The subnet must be a part of the network to which the cluster belongs.
     */
    subnetId: string;
    /**
     * The availability zone where the MySQL host will be created.
     */
    zone: string;
}

export interface MdbMysqlClusterMaintenanceWindow {
    /**
     * Day of the week (in `DDD` format). Allowed values: "MON", "TUE", "WED", "THU", "FRI", "SAT", "SUN"
     */
    day?: string;
    /**
     * Hour of the day in UTC (in `HH` format). Allowed value is between 0 and 23.
     */
    hour?: number;
    /**
     * Type of maintenance window. Can be either `ANYTIME` or `WEEKLY`. A day and hour of window need to be specified with weekly window.
     */
    type: string;
}

export interface MdbMysqlClusterPerformanceDiagnostics {
    /**
     * Enable performance diagnostics
     */
    enabled: boolean;
    /**
     * Interval (in seconds) for myStatActivity sampling Acceptable values are 1 to 86400, inclusive.
     */
    sessionsSamplingInterval: number;
    /**
     * Interval (in seconds) for myStatStatements sampling Acceptable values are 1 to 86400, inclusive.
     */
    statementsSamplingInterval: number;
}

export interface MdbMysqlClusterResources {
    /**
     * Volume of the storage available to a MySQL host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of MySQL hosts.
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbMysqlClusterRestore {
    /**
     * Backup ID. The cluster will be created from the specified backup. [How to get a list of MySQL backups](https://cloud.yandex.com/docs/managed-mysql/operations/cluster-backups).
     */
    backupId: string;
    /**
     * Timestamp of the moment to which the MySQL cluster should be restored. (Format: "2006-01-02T15:04:05" - UTC). When not set, current time is used.
     */
    time?: string;
}

export interface MdbMysqlClusterUser {
    /**
     * Authentication plugin. Allowed values: `MYSQL_NATIVE_PASSWORD`, `CACHING_SHA2_PASSWORD`, `SHA256_PASSWORD` (for version 5.7 `MYSQL_NATIVE_PASSWORD`, `SHA256_PASSWORD`)
     */
    authenticationPlugin: string;
    /**
     * User's connection limits. The structure is documented below.
     * If the attribute is not specified there will be no changes.
     */
    connectionLimits: outputs.MdbMysqlClusterUserConnectionLimits;
    /**
     * List user's global permissions     
     * Allowed permissions:  `REPLICATION_CLIENT`, `REPLICATION_SLAVE`, `PROCESS` for clear list use empty list.
     * If the attribute is not specified there will be no changes.
     */
    globalPermissions: string[];
    /**
     * The name of the user.
     */
    name: string;
    /**
     * The password of the user.
     */
    password: string;
    /**
     * Set of permissions granted to the user. The structure is documented below.
     */
    permissions: outputs.MdbMysqlClusterUserPermission[];
}

export interface MdbMysqlClusterUserConnectionLimits {
    /**
     * Max connections per hour.
     */
    maxConnectionsPerHour?: number;
    /**
     * Max questions per hour.
     */
    maxQuestionsPerHour?: number;
    /**
     * Max updates per hour.
     */
    maxUpdatesPerHour?: number;
    /**
     * Max user connections.
     */
    maxUserConnections?: number;
}

export interface MdbMysqlClusterUserPermission {
    /**
     * The name of the database that the permission grants access to.
     */
    databaseName: string;
    /**
     * List user's roles in the database.
     * Allowed roles: `ALL`,`ALTER`,`ALTER_ROUTINE`,`CREATE`,`CREATE_ROUTINE`,`CREATE_TEMPORARY_TABLES`,
     * `CREATE_VIEW`,`DELETE`,`DROP`,`EVENT`,`EXECUTE`,`INDEX`,`INSERT`,`LOCK_TABLES`,`SELECT`,`SHOW_VIEW`,`TRIGGER`,`UPDATE`.
     */
    roles?: string[];
}

export interface MdbMysqlUserConnectionLimits {
    /**
     * Max connections per hour.
     */
    maxConnectionsPerHour?: number;
    /**
     * Max questions per hour.
     */
    maxQuestionsPerHour?: number;
    /**
     * Max updates per hour.
     */
    maxUpdatesPerHour?: number;
    /**
     * Max user connections.
     */
    maxUserConnections?: number;
}

export interface MdbMysqlUserPermission {
    /**
     * The name of the database that the permission grants access to.
     */
    databaseName: string;
    /**
     * List user's roles in the database.
     * Allowed roles: `ALL`,`ALTER`,`ALTER_ROUTINE`,`CREATE`,`CREATE_ROUTINE`,`CREATE_TEMPORARY_TABLES`,
     * `CREATE_VIEW`,`DELETE`,`DROP`,`EVENT`,`EXECUTE`,`INDEX`,`INSERT`,`LOCK_TABLES`,`SELECT`,`SHOW_VIEW`,`TRIGGER`,`UPDATE`.
     */
    roles?: string[];
}

export interface MdbPostgresqlClusterConfig {
    /**
     * Access policy to the PostgreSQL cluster. The structure is documented below.
     */
    access: outputs.MdbPostgresqlClusterConfigAccess;
    /**
     * Configuration setting which enables/disables autofailover in cluster.
     */
    autofailover: boolean;
    /**
     * The period in days during which backups are stored.
     */
    backupRetainPeriodDays: number;
    /**
     * Time to start the daily backup, in the UTC timezone. The structure is documented below.
     */
    backupWindowStart: outputs.MdbPostgresqlClusterConfigBackupWindowStart;
    /**
     * Cluster disk size autoscaling settings. The structure is documented below.
     */
    diskSizeAutoscaling: outputs.MdbPostgresqlClusterConfigDiskSizeAutoscaling;
    /**
     * Cluster performance diagnostics settings. The structure is documented below. [YC Documentation](https://cloud.yandex.com/en-ru/docs/managed-postgresql/api-ref/grpc/cluster_service#PerformanceDiagnostics)
     */
    performanceDiagnostics: outputs.MdbPostgresqlClusterConfigPerformanceDiagnostics;
    /**
     * Configuration of the connection pooler. The structure is documented below.
     */
    poolerConfig?: outputs.MdbPostgresqlClusterConfigPoolerConfig;
    /**
     * PostgreSQL cluster config. Detail info in "postresql config" section (documented below).
     */
    postgresqlConfig: {[key: string]: string};
    /**
     * Resources allocated to hosts of the PostgreSQL cluster. The structure is documented below.
     */
    resources: outputs.MdbPostgresqlClusterConfigResources;
    /**
     * Version of the PostgreSQL cluster. (allowed versions are: 10, 10-1c, 11, 11-1c, 12, 12-1c, 13, 13-1c, 14, 14-1c, 15, 15-1c, 16)
     */
    version: string;
}

export interface MdbPostgresqlClusterConfigAccess {
    /**
     * Allow access for [Yandex DataLens](https://cloud.yandex.com/services/datalens).
     */
    dataLens?: boolean;
    /**
     * Allow access for [DataTransfer](https://cloud.yandex.com/services/data-transfer)
     */
    dataTransfer?: boolean;
    /**
     * Allow access for [connection to managed databases from functions](https://cloud.yandex.com/docs/functions/operations/database-connection)
     */
    serverless?: boolean;
    /**
     * Allow access for [SQL queries in the management console](https://cloud.yandex.com/docs/managed-postgresql/operations/web-sql-query)
     */
    webSql: boolean;
}

export interface MdbPostgresqlClusterConfigBackupWindowStart {
    /**
     * The hour at which backup will be started (UTC).
     */
    hours?: number;
    /**
     * The minute at which backup will be started (UTC).
     */
    minutes?: number;
}

export interface MdbPostgresqlClusterConfigDiskSizeAutoscaling {
    /**
     * Limit of disk size after autoscaling (GiB).
     */
    diskSizeLimit: number;
    /**
     * Immediate autoscaling disk usage (percent).
     */
    emergencyUsageThreshold?: number;
    /**
     * Maintenance window autoscaling disk usage (percent).
     */
    plannedUsageThreshold?: number;
}

export interface MdbPostgresqlClusterConfigPerformanceDiagnostics {
    /**
     * Enable performance diagnostics
     */
    enabled: boolean;
    /**
     * Interval (in seconds) for pgStatActivity sampling Acceptable values are 1 to 86400, inclusive.
     */
    sessionsSamplingInterval: number;
    /**
     * Interval (in seconds) for pgStatStatements sampling Acceptable values are 1 to 86400, inclusive.
     */
    statementsSamplingInterval: number;
}

export interface MdbPostgresqlClusterConfigPoolerConfig {
    /**
     * Setting `poolDiscard` [parameter in Odyssey](https://github.com/yandex/odyssey/blob/master/documentation/configuration.md#pool_discard-yesno).
     */
    poolDiscard?: boolean;
    /**
     * Mode that the connection pooler is working in. See descriptions of all modes in the [documentation for Odyssey](https://github.com/yandex/odyssey/blob/master/documentation/configuration.md#pool-string.
     */
    poolingMode?: string;
}

export interface MdbPostgresqlClusterConfigResources {
    /**
     * Volume of the storage available to a PostgreSQL host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of PostgreSQL hosts.
     */
    diskTypeId?: string;
    resourcePresetId: string;
}

export interface MdbPostgresqlClusterDatabase {
    extensions?: outputs.MdbPostgresqlClusterDatabaseExtension[];
    lcCollate?: string;
    lcType?: string;
    /**
     * Name of the PostgreSQL cluster. Provided by the client when the cluster is created.
     */
    name: string;
    owner: string;
    templateDb?: string;
}

export interface MdbPostgresqlClusterDatabaseExtension {
    /**
     * Name of the PostgreSQL cluster. Provided by the client when the cluster is created.
     */
    name: string;
    /**
     * Version of the PostgreSQL cluster. (allowed versions are: 10, 10-1c, 11, 11-1c, 12, 12-1c, 13, 13-1c, 14, 14-1c, 15, 15-1c, 16)
     */
    version?: string;
}

export interface MdbPostgresqlClusterHost {
    /**
     * Sets whether the host should get a public IP address on creation. It can be changed on the fly only when `name` is set.
     */
    assignPublicIp?: boolean;
    /**
     * The fully qualified domain name of the host.
     */
    fqdn: string;
    /**
     * Host state name. It should be set for all hosts or unset for all hosts. This field can be used by another host, to select which host will be its replication source. Please see `replicationSourceName` parameter.
     * Also, this field is used to select which host will be selected as a master host. Please see `hostMasterName` parameter.
     */
    name?: string;
    /**
     * Host priority in HA group. It works only when `name` is set.
     */
    priority?: number;
    /**
     * Host replication source (fqdn), when replicationSource is empty then host is in HA group.
     */
    replicationSource: string;
    /**
     * Host replication source name points to host's `name` from which this host should replicate. When not set then host in HA group. It works only when `name` is set.
     */
    replicationSourceName?: string;
    role: string;
    /**
     * The ID of the subnet, to which the host belongs. The subnet must be a part of the network to which the cluster belongs.
     */
    subnetId?: string;
    /**
     * The availability zone where the PostgreSQL host will be created.
     */
    zone: string;
}

export interface MdbPostgresqlClusterMaintenanceWindow {
    /**
     * Day of the week (in `DDD` format). Allowed values: "MON", "TUE", "WED", "THU", "FRI", "SAT", "SUN"
     */
    day?: string;
    /**
     * Hour of the day in UTC (in `HH` format). Allowed value is between 1 and 24.
     */
    hour?: number;
    /**
     * Type of maintenance window. Can be either `ANYTIME` or `WEEKLY`. A day and hour of window need to be specified with weekly window.
     */
    type: string;
}

export interface MdbPostgresqlClusterRestore {
    /**
     * Backup ID. The cluster will be created from the specified backup. [How to get a list of PostgreSQL backups](https://cloud.yandex.com/docs/managed-postgresql/operations/cluster-backups).
     */
    backupId: string;
    /**
     * Timestamp of the moment to which the PostgreSQL cluster should be restored. (Format: "2006-01-02T15:04:05" - UTC). When not set, current time is used.
     */
    time?: string;
    /**
     * Flag that indicates whether a database should be restored to the first backup point available just after the timestamp specified in the [time] field instead of just before.  
     * Possible values:
     * - false (default) — the restore point refers to the first backup moment before [time].
     * - true — the restore point refers to the first backup point after [time].
     */
    timeInclusive?: boolean;
}

export interface MdbPostgresqlClusterUser {
    connLimit: number;
    grants: string[];
    login?: boolean;
    /**
     * Name of the PostgreSQL cluster. Provided by the client when the cluster is created.
     */
    name: string;
    password: string;
    permissions: outputs.MdbPostgresqlClusterUserPermission[];
    settings: {[key: string]: string};
}

export interface MdbPostgresqlClusterUserPermission {
    databaseName: string;
}

export interface MdbPostgresqlDatabaseExtension {
    /**
     * Name of the database extension. For more information on available extensions see [the official documentation](https://cloud.yandex.com/docs/managed-postgresql/operations/cluster-extensions).
     */
    name: string;
    /**
     * Version of the extension.
     */
    version?: string;
}

export interface MdbPostgresqlUserPermission {
    /**
     * The name of the database that the permission grants access to.
     */
    databaseName: string;
}

export interface MdbRedisClusterConfig {
    /**
     * Normal clients output buffer limits.
     * See [redis config file](https://github.com/redis/redis/blob/6.2/redis.conf#L1841).
     */
    clientOutputBufferLimitNormal: string;
    /**
     * Pubsub clients output buffer limits.
     * See [redis config file](https://github.com/redis/redis/blob/6.2/redis.conf#L1843).
     */
    clientOutputBufferLimitPubsub: string;
    /**
     * Number of databases (changing requires redis-server restart).
     */
    databases: number;
    /**
     * Redis maxmemory usage in percent
     */
    maxmemoryPercent?: number;
    /**
     * Redis key eviction policy for a dataset that reaches maximum memory.
     * Can be any of the listed in [the official RedisDB documentation](https://docs.redislabs.com/latest/rs/administering/database-operations/eviction-policy/).
     */
    maxmemoryPolicy: string;
    /**
     * Select the events that Redis will notify among a set of classes.
     */
    notifyKeyspaceEvents: string;
    /**
     * Password for the Redis cluster.
     */
    password: string;
    /**
     * Log slow queries below this number in microseconds.
     */
    slowlogLogSlowerThan: number;
    /**
     * Slow queries log length.
     */
    slowlogMaxLen: number;
    /**
     * Close the connection after a client is idle for N seconds.
     */
    timeout: number;
    /**
     * Version of Redis (6.2).
     */
    version: string;
}

export interface MdbRedisClusterDiskSizeAutoscaling {
    /**
     * Limit of disk size after autoscaling (GiB).
     */
    diskSizeLimit: number;
    /**
     * Immediate autoscaling disk usage (percent).
     */
    emergencyUsageThreshold?: number;
    /**
     * Maintenance window autoscaling disk usage (percent).
     */
    plannedUsageThreshold?: number;
}

export interface MdbRedisClusterHost {
    /**
     * Sets whether the host should get a public IP address or not.
     */
    assignPublicIp?: boolean;
    /**
     * The fully qualified domain name of the host.
     */
    fqdn: string;
    /**
     * Replica priority of a current replica (usable for non-sharded only).
     */
    replicaPriority?: number;
    /**
     * The name of the shard to which the host belongs.
     */
    shardName: string;
    /**
     * The ID of the subnet, to which the host belongs. The subnet must
     * be a part of the network to which the cluster belongs.
     */
    subnetId: string;
    /**
     * The availability zone where the Redis host will be created.
     * For more information see [the official documentation](https://cloud.yandex.com/docs/overview/concepts/geo-scope).
     */
    zone: string;
}

export interface MdbRedisClusterMaintenanceWindow {
    /**
     * Day of week for maintenance window if window type is weekly. Possible values: `MON`, `TUE`, `WED`, `THU`, `FRI`, `SAT`, `SUN`.
     */
    day?: string;
    /**
     * Hour of day in UTC time zone (1-24) for maintenance window if window type is weekly.
     */
    hour?: number;
    /**
     * Type of maintenance window. Can be either `ANYTIME` or `WEEKLY`. A day and hour of window need to be specified with weekly window.
     */
    type: string;
}

export interface MdbRedisClusterResources {
    /**
     * Volume of the storage available to a host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of Redis hosts - environment default is used if missing.
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbSqlserverClusterBackupWindowStart {
    /**
     * The hour at which backup will be started.
     */
    hours?: number;
    /**
     * The minute at which backup will be started.
     */
    minutes?: number;
}

export interface MdbSqlserverClusterDatabase {
    /**
     * The name of the database.
     */
    name: string;
}

export interface MdbSqlserverClusterHost {
    /**
     * Sets whether the host should get a public IP address on creation. Changing this parameter for an existing host is not supported at the moment
     */
    assignPublicIp?: boolean;
    /**
     * The fully qualified domain name of the host.
     */
    fqdn: string;
    /**
     * The ID of the subnet, to which the host belongs. The subnet must be a part of the network to which the cluster belongs.
     */
    subnetId: string;
    /**
     * The availability zone where the SQLServer host will be created.
     */
    zone: string;
}

export interface MdbSqlserverClusterResources {
    /**
     * Volume of the storage available to a SQLServer host, in gigabytes.
     */
    diskSize: number;
    /**
     * Type of the storage of SQLServer hosts.
     */
    diskTypeId: string;
    resourcePresetId: string;
}

export interface MdbSqlserverClusterUser {
    /**
     * The name of the user.
     */
    name: string;
    /**
     * The password of the user.
     */
    password: string;
    /**
     * Set of permissions granted to the user. The structure is documented below.
     */
    permissions?: outputs.MdbSqlserverClusterUserPermission[];
}

export interface MdbSqlserverClusterUserPermission {
    /**
     * The name of the database that the permission grants access to.
     */
    databaseName: string;
    /**
     * List user's roles in the database.
     * Allowed roles: `OWNER`, `SECURITYADMIN`, `ACCESSADMIN`, `BACKUPOPERATOR`, `DDLADMIN`, `DATAWRITER`, `DATAREADER`, `DENYDATAWRITER`, `DENYDATAREADER`.
     */
    roles?: string[];
}

export interface MonitoringDashboardParametrization {
    /**
     * parameters list.
     */
    parameters?: outputs.MonitoringDashboardParametrizationParameter[];
    /**
     * dashboard predefined parameters selector.
     */
    selectors?: string;
}

export interface MonitoringDashboardParametrizationParameter {
    /**
     * Custom values parameter. Oneof: label_values, custom, text.
     */
    customs?: outputs.MonitoringDashboardParametrizationParameterCustom[];
    /**
     * Parameter description.
     */
    description?: string;
    /**
     * UI-visibility.
     */
    hidden?: boolean;
    /**
     * Parameter identifier
     */
    id: string;
    /**
     * Label values parameter. Oneof: label_values, custom, text.
     */
    labelValues?: outputs.MonitoringDashboardParametrizationParameterLabelValue[];
    /**
     * Text parameter. Oneof: label_values, custom, text.
     */
    texts?: outputs.MonitoringDashboardParametrizationParameterText[];
    /**
     * UI-visible title of the parameter.
     */
    title?: string;
}

export interface MonitoringDashboardParametrizationParameterCustom {
    /**
     * Default value.
     */
    defaultValues?: string[];
    /**
     * Specifies the multiselectable values of parameter.
     */
    multiselectable?: boolean;
    /**
     * Parameter values.
     */
    values?: string[];
}

export interface MonitoringDashboardParametrizationParameterLabelValue {
    /**
     * Default value.
     */
    defaultValues?: string[];
    /**
     * Labels folder ID.
     */
    folderId?: string;
    /**
     * Label key to list label values.
     */
    labelKey: string;
    /**
     * Specifies the multiselectable values of parameter.
     */
    multiselectable?: boolean;
    /**
     * Selectors to select metric label values.
     */
    selectors?: string;
}

export interface MonitoringDashboardParametrizationParameterText {
    /**
     * Default value.
     */
    defaultValue?: string;
}

export interface MonitoringDashboardWidget {
    /**
     * Chart widget settings. Oneof: text, title or chart.
     */
    charts?: outputs.MonitoringDashboardWidgetChart[];
    /**
     * Widget position.
     */
    positions?: outputs.MonitoringDashboardWidgetPosition[];
    /**
     * Text widget settings. Oneof: text, title or chart.
     */
    texts?: outputs.MonitoringDashboardWidgetText[];
    /**
     * Title widget settings. Oneof: text, title or chart.
     */
    titles?: outputs.MonitoringDashboardWidgetTitle[];
}

export interface MonitoringDashboardWidgetChart {
    /**
     * Chart ID.
     */
    chartId?: string;
    /**
     * Chart description in dashboard (not enabled in UI).
     */
    description?: string;
    /**
     * Enable legend under chart.
     */
    displayLegend?: boolean;
    /**
     * Fixed time interval for chart. Values:
     * - FREEZE_DURATION_HOUR: Last hour.
     * - FREEZE_DURATION_DAY: Last day = last 24 hours.
     * - FREEZE_DURATION_WEEK: Last 7 days.
     * - FREEZE_DURATION_MONTH: Last 31 days.
     */
    freeze: string;
    /**
     * Names settings.
     */
    nameHidingSettings?: outputs.MonitoringDashboardWidgetChartNameHidingSetting[];
    /**
     * Queries settings.
     */
    queries?: outputs.MonitoringDashboardWidgetChartQuery[];
    /**
     * Time series settings.
     */
    seriesOverrides?: outputs.MonitoringDashboardWidgetChartSeriesOverride[];
    /**
     * Chart widget title.
     */
    title?: string;
    /**
     * Visualization settings.
     */
    visualizationSettings?: outputs.MonitoringDashboardWidgetChartVisualizationSetting[];
}

export interface MonitoringDashboardWidgetChartNameHidingSetting {
    /**
     * Series name.
     */
    names?: string[];
    /**
     * True if we want to show concrete series names only, false if we want to hide concrete series names.
     */
    positive?: boolean;
}

export interface MonitoringDashboardWidgetChartQuery {
    /**
     * Downsamplang settings.
     */
    downsamplings?: outputs.MonitoringDashboardWidgetChartQueryDownsampling[];
    /**
     * Query targets.
     */
    targets?: outputs.MonitoringDashboardWidgetChartQueryTarget[];
}

export interface MonitoringDashboardWidgetChartQueryDownsampling {
    /**
     * Disable downsampling.
     */
    disabled?: boolean;
    /**
     * Parameters for filling gaps in data.
     */
    gapFilling?: string;
    /**
     * Function that is used for downsampling.
     */
    gridAggregation?: string;
    /**
     * Time interval (grid) for downsampling in milliseconds. Points in the specified range are aggregated into one time point
     */
    gridInterval?: number;
    /**
     * Maximum number of points to be returned.
     */
    maxPoints?: number;
}

export interface MonitoringDashboardWidgetChartQueryTarget {
    /**
     * Checks that target is visible or invisible.
     */
    hidden?: boolean;
    /**
     * Query.
     */
    query?: string;
    /**
     * Text mode enabled.
     */
    textMode?: boolean;
}

export interface MonitoringDashboardWidgetChartSeriesOverride {
    /**
     * Series name. Oneof: name or target_index.
     */
    name?: string;
    /**
     * Override settings.
     */
    settings?: outputs.MonitoringDashboardWidgetChartSeriesOverrideSetting[];
    /**
     * Series index. Oneof: name or target_index.
     */
    targetIndex?: string;
}

export interface MonitoringDashboardWidgetChartSeriesOverrideSetting {
    /**
     * Series color or empty.
     */
    color?: string;
    /**
     * Stack grow down.
     */
    growDown?: boolean;
    /**
     * Series name or empty.
     */
    name?: string;
    /**
     * Stack name or empty.
     */
    stackName?: string;
    /**
     * Type.
     */
    type: string;
    /**
     * Yaxis position.
     */
    yaxisPosition: string;
}

export interface MonitoringDashboardWidgetChartVisualizationSetting {
    /**
     * Aggregation. Values:
     * - SERIES_AGGREGATION_UNSPECIFIED: Not specified (avg by default).
     * - SERIES_AGGREGATION_AVG: Average.
     * - SERIES_AGGREGATION_MIN: Minimum.
     * - SERIES_AGGREGATION_MAX: Maximum.
     * - SERIES_AGGREGATION_LAST: Last non-NaN value.
     * - SERIES_AGGREGATION_SUM: Sum.
     */
    aggregation: string;
    /**
     * Color settings.
     */
    colorSchemeSettings?: outputs.MonitoringDashboardWidgetChartVisualizationSettingColorSchemeSetting[];
    /**
     * Heatmap settings.
     */
    heatmapSettings?: outputs.MonitoringDashboardWidgetChartVisualizationSettingHeatmapSetting[];
    /**
     * Interpolate values. Values:
     * - INTERPOLATE_UNSPECIFIED: Not specified (linear by default).
     * - INTERPOLATE_LINEAR: Linear.
     * - INTERPOLATE_LEFT: Left.
     * - INTERPOLATE_RIGHT: Right.
     */
    interpolate: string;
    /**
     * Normalize values.
     */
    normalize?: boolean;
    /**
     * Show chart labels.
     */
    showLabels?: boolean;
    /**
     * Inside chart title.
     */
    title?: string;
    /**
     * Visualization type. Values:
     * - VISUALIZATION_TYPE_UNSPECIFIED: Not specified (line by default).
     * - VISUALIZATION_TYPE_LINE: Line chart.
     * - VISUALIZATION_TYPE_STACK: Stack chart.
     * - VISUALIZATION_TYPE_COLUMN: Points as columns chart.
     * - VISUALIZATION_TYPE_POINTS: Points.
     * - VISUALIZATION_TYPE_PIE: Pie aggregation chart.
     * - VISUALIZATION_TYPE_BARS: Bars aggregation chart.
     * - VISUALIZATION_TYPE_DISTRIBUTION: Distribution aggregation chart.
     * - VISUALIZATION_TYPE_HEATMAP: Heatmap aggregation chart.
     */
    type: string;
    /**
     * Y axis settings.
     */
    yaxisSettings?: outputs.MonitoringDashboardWidgetChartVisualizationSettingYaxisSetting[];
}

export interface MonitoringDashboardWidgetChartVisualizationSettingColorSchemeSetting {
    /**
     * Automatic color scheme. Oneof: automatic, standard or gradient.
     */
    automatics?: outputs.MonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingAutomatic[];
    /**
     * Gradient color scheme. Oneof: automatic, standard or gradient.
     */
    gradients?: outputs.MonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingGradient[];
    /**
     * Standard color scheme. Oneof: automatic, standard or gradient.
     */
    standards?: outputs.MonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingStandard[];
}

export interface MonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingAutomatic {
}

export interface MonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingGradient {
    /**
     * Gradient green value.
     */
    greenValue?: string;
    /**
     * Gradient red value.
     */
    redValue?: string;
    /**
     * Gradient violet value.
     */
    violetValue?: string;
    /**
     * Gradient yellow value.
     */
    yellowValue?: string;
}

export interface MonitoringDashboardWidgetChartVisualizationSettingColorSchemeSettingStandard {
}

export interface MonitoringDashboardWidgetChartVisualizationSettingHeatmapSetting {
    /**
     * Heatmap green value.
     */
    greenValue?: string;
    /**
     * Heatmap red value.
     */
    redValue?: string;
    /**
     * Heatmap violet value.
     */
    violetValue?: string;
    /**
     * Heatmap yellow value.
     */
    yellowValue?: string;
}

export interface MonitoringDashboardWidgetChartVisualizationSettingYaxisSetting {
    /**
     * Left yaxis config.
     */
    lefts?: outputs.MonitoringDashboardWidgetChartVisualizationSettingYaxisSettingLeft[];
    /**
     * Right yaxis config.
     */
    rights?: outputs.MonitoringDashboardWidgetChartVisualizationSettingYaxisSettingRight[];
}

export interface MonitoringDashboardWidgetChartVisualizationSettingYaxisSettingLeft {
    /**
     * Max value in extended number format or empty.
     */
    max?: string;
    /**
     * Min value in extended number format or empty.
     */
    min?: string;
    /**
     * Tick value precision (null as default, 0-7 in other cases).
     */
    precision?: number;
    /**
     * Title or empty.
     */
    title?: string;
    /**
     * Type. Values:
     * - YAXIS_TYPE_UNSPECIFIED: Not specified (linear by default).
     * - YAXIS_TYPE_LINEAR: Linear.
     * - YAXIS_TYPE_LOGARITHMIC: Logarithmic.
     */
    type: string;
    /**
     * Unit format. Values:
     * - UNIT_NONE: Misc. None (show tick values as-is).
     * - UNIT_COUNT: Count.
     * - UNIT_PERCENT: Percent (0-100).
     * - UNIT_PERCENT_UNIT: Percent (0-1).
     * - UNIT_NANOSECONDS: Time. Nanoseconds (ns).
     * - UNIT_MICROSECONDS: Microseconds (µs).
     * - UNIT_MILLISECONDS: Milliseconds (ms).
     * - UNIT_SECONDS: Seconds (s).
     * - UNIT_MINUTES: Minutes (m).
     * - UNIT_HOURS: Hours (h).
     * - UNIT_DAYS: Days (d).
     * - UNIT_BITS_SI: Data (SI). Bits (SI).
     * - UNIT_BYTES_SI: Bytes (SI).
     * - UNIT_KILOBYTES: Kilobytes (KB).
     * - UNIT_MEGABYTES: Megabytes (MB).
     * - UNIT_GIGABYTES: Gigabytes (GB).
     * - UNIT_TERABYTES: Terabytes (TB)
     * - UNIT_PETABYTES: Petabytes (PB).
     * - UNIT_EXABYTES: Exabytes (EB).
     * - UNIT_BITS_IEC: Data (IEC). Bits (IEC).
     * - UNIT_BYTES_IEC: Bytes (IEC).
     * - UNIT_KIBIBYTES: Kibibytes (KiB).
     * - UNIT_MEBIBYTES: Mebibytes (MiB).
     * - UNIT_GIBIBYTES: Gigibytes (GiB).
     * - UNIT_TEBIBYTES: Tebibytes (TiB).
     * - UNIT_PEBIBYTES: Pebibytes (PiB).
     * - UNIT_EXBIBYTES: Exbibytes (EiB).
     * - UNIT_REQUESTS_PER_SECOND: Throughput. Requests per second (reqps).
     * - UNIT_OPERATIONS_PER_SECOND: Operations per second (ops).
     * - UNIT_WRITES_PER_SECOND: Writes per second (wps).
     * - UNIT_READS_PER_SECOND: Reads per second (rps).
     * - UNIT_PACKETS_PER_SECOND: Packets per second (pps).
     * - UNIT_IO_OPERATIONS_PER_SECOND: IO operations per second (iops).
     * - UNIT_COUNTS_PER_SECOND: Counts per second (counts/sec).
     * - UNIT_BITS_SI_PER_SECOND: Data Rate (SI). Bits (SI) per second (bits/sec).
     * - UNIT_BYTES_SI_PER_SECOND: Bytes (SI) per second (bytes/sec).
     * - UNIT_KILOBITS_PER_SECOND: Kilobits per second (KBits/sec).
     * - UNIT_KILOBYTES_PER_SECOND: Kilobytes per second (KB/sec).
     * - UNIT_MEGABITS_PER_SECOND: Megabits per second (MBits/sec).
     * - UNIT_MEGABYTES_PER_SECOND: Megabytes per second (MB/sec).
     * - UNIT_GIGABITS_PER_SECOND: Gigabits per second (GBits/sec).
     * - UNIT_GIGABYTES_PER_SECOND: Gigabytes per second (GB/sec).
     * - UNIT_TERABITS_PER_SECOND: Terabits per second (TBits/sec).
     * - UNIT_TERABYTES_PER_SECOND: Terabytes per second (TB/sec).
     * - UNIT_PETABITS_PER_SECOND: Petabits per second (Pbits/sec).
     * - UNIT_PETABYTES_PER_SECOND: Petabytes per second (PB/sec).
     * - UNIT_BITS_IEC_PER_SECOND: Data Rate (IEC). Bits (IEC) per second (bits/sec).
     * - UNIT_BYTES_IEC_PER_SECOND: Bytes (IEC) per second (bytes/sec).
     * - UNIT_KIBIBITS_PER_SECOND: Kibibits per second (KiBits/sec).
     * - UNIT_KIBIBYTES_PER_SECOND: Kibibytes per second (KiB/sec).
     * - UNIT_MEBIBITS_PER_SECOND: Mebibits per second (MiBits/sec).
     * - UNIT_MEBIBYTES_PER_SECOND: Mebibytes per second (MiB/sec).
     * - UNIT_GIBIBITS_PER_SECOND: Gibibits per second (GiBits/sec).
     * - UNIT_GIBIBYTES_PER_SECOND: Gibibytes per second (GiB/sec).
     * - UNIT_TEBIBITS_PER_SECOND: Tebibits per second (TiBits/sec).
     * - UNIT_TEBIBYTES_PER_SECOND: Tebibytes per second (TiB/sec).
     * - UNIT_PEBIBITS_PER_SECOND: Pebibits per second (PiBits/sec).
     * - UNIT_PEBIBYTES_PER_SECOND: Pebibytes per second (PiB/sec).
     * - UNIT_DATETIME_UTC: Date & time. Datetime (UTC).
     * - UNIT_DATETIME_LOCAL: Datetime (local).
     * - UNIT_HERTZ: Frequency. Hertz (Hz).
     * - UNIT_KILOHERTZ: Kilohertz (KHz).
     * - UNIT_MEGAHERTZ: Megahertz (MHz).
     * - UNIT_GIGAHERTZ: Gigahertz (GHz).
     * - UNIT_DOLLAR: Currency. Dollar.
     * - UNIT_EURO: Euro.
     * - UNIT_ROUBLE: Rouble.
     * - UNIT_CELSIUS: Temperature. Celsius (°C).
     * - UNIT_FAHRENHEIT: Fahrenheit (°F).
     * - UNIT_KELVIN: Kelvin (K).
     * - UNIT_FLOP_PER_SECOND: Computation. Flop per second (FLOP/sec).
     * - UNIT_KILOFLOP_PER_SECOND: Kiloflop per second (KFLOP/sec).
     * - UNIT_MEGAFLOP_PER_SECOND: Megaflop per second (MFLOP/sec).
     * - UNIT_GIGAFLOP_PER_SECOND: Gigaflop per second (GFLOP/sec).
     * - UNIT_PETAFLOP_PER_SECOND: Petaflop per second (PFLOP/sec).
     * - UNIT_EXAFLOP_PER_SECOND: Exaflop per second (EFLOP/sec).
     * - UNIT_METERS_PER_SECOND: Velocity. Meters per second (m/sec).
     * - UNIT_KILOMETERS_PER_HOUR: Kilometers per hour (km/h).
     * - UNIT_MILES_PER_HOUR: Miles per hour (mi/h).
     * - UNIT_MILLIMETER: Length. Millimeter.
     * - UNIT_CENTIMETER: Centimeter.
     * - UNIT_METER: Meter.
     * - UNIT_KILOMETER: Kilometer.
     * - UNIT_MILE: Mile.
     * - UNIT_PPM: Concentration. Parts per million (ppm).
     * - UNIT_EVENTS_PER_SECOND: Events per second
     * - UNIT_PACKETS: Packets
     * - UNIT_DBM: dBm (dbm)
     * - UNIT_VIRTUAL_CPU: Virtual CPU cores based on CPU time (vcpu)
     * - UNIT_MESSAGES_PER_SECOND: Messages per second (mps)
     */
    unitFormat: string;
}

export interface MonitoringDashboardWidgetChartVisualizationSettingYaxisSettingRight {
    /**
     * Max value in extended number format or empty.
     */
    max?: string;
    /**
     * Min value in extended number format or empty.
     */
    min?: string;
    /**
     * Tick value precision (null as default, 0-7 in other cases).
     */
    precision?: number;
    /**
     * Title or empty.
     */
    title?: string;
    /**
     * Type.
     */
    type?: string;
    /**
     * Unit format.
     */
    unitFormat?: string;
}

export interface MonitoringDashboardWidgetPosition {
    /**
     * Height.
     */
    h?: number;
    /**
     * Width.
     */
    w?: number;
    /**
     * X-axis top-left corner coordinate.
     */
    x?: number;
    /**
     * Y-axis top-left corner coordinate.
     */
    y?: number;
}

export interface MonitoringDashboardWidgetText {
    /**
     * Widget text.
     */
    text?: string;
}

export interface MonitoringDashboardWidgetTitle {
    /**
     * Title size. Values: 
     * - TITLE_SIZE_XS: Extra small size.
     * - TITLE_SIZE_S: Small size.
     * - TITLE_SIZE_M: Middle size.
     * - TITLE_SIZE_L: Large size.
     */
    size: string;
    /**
     * Title text.
     */
    text: string;
}

export interface OrganizationmanagerOsLoginSettingsSshCertificateSettings {
    /**
     * Enables or disables usage of ssh certificates signed by trusted CA.
     */
    enabled?: boolean;
}

export interface OrganizationmanagerOsLoginSettingsUserSshKeySettings {
    /**
     * If set to true subject is allowed to manage own ssh keys without having to be assigned specific permissions.
     */
    allowManageOwnKeys?: boolean;
    /**
     * Enables or disables usage of ssh keys assigned to a specific subject.
     */
    enabled?: boolean;
}

export interface OrganizationmanagerSamlFederationSecuritySettings {
    /**
     * Enable encrypted assertions.
     */
    encryptedAssertions: boolean;
}

export interface ServerlessContainerConnectivity {
    networkId: string;
}

export interface ServerlessContainerImage {
    args?: string[];
    commands?: string[];
    digest: string;
    environment?: {[key: string]: string};
    /**
     * Invoke URL for the Yandex Cloud Serverless Container
     */
    url: string;
    workDir?: string;
}

export interface ServerlessContainerLogOptions {
    /**
     * Is logging from container disabled
     */
    disabled?: boolean;
    /**
     * Folder ID for the Yandex Cloud Serverless Container
     */
    folderId?: string;
    /**
     * Log entries are written to specified log group
     */
    logGroupId?: string;
    /**
     * Minimum log entry level
     */
    minLevel?: string;
}

export interface ServerlessContainerProvisionPolicy {
    minInstances: number;
}

export interface ServerlessContainerSecret {
    /**
     * (Required) Container's environment variable in which secret's value will be stored.
     */
    environmentVariable: string;
    /**
     * (Required) Secret's id.
     */
    id: string;
    /**
     * (Required) Secret's entries key which value will be stored in environment variable.
     */
    key: string;
    /**
     * (Required) Secret's version id.
     */
    versionId: string;
}

export interface ServerlessContainerStorageMount {
    /**
     * (Required) Name of the mounting bucket.
     */
    bucket: string;
    /**
     * (Required) Path inside the container to access the directory in which the bucket is mounted.
     */
    mountPointPath: string;
    /**
     * Prefix within the bucket. If you leave this field empty, the entire bucket will be mounted.
     */
    prefix?: string;
    /**
     * Mount the bucket in read-only mode.
     */
    readOnly?: boolean;
}

export interface SmartcaptchaCaptchaOverrideVariant {
    challengeType?: string;
    complexity?: string;
    description?: string;
    preCheckType?: string;
    uuid?: string;
}

export interface SmartcaptchaCaptchaSecurityRule {
    condition?: outputs.SmartcaptchaCaptchaSecurityRuleCondition;
    description?: string;
    name?: string;
    overrideVariantUuid?: string;
    priority?: number;
}

export interface SmartcaptchaCaptchaSecurityRuleCondition {
    headers?: outputs.SmartcaptchaCaptchaSecurityRuleConditionHeader[];
    host?: outputs.SmartcaptchaCaptchaSecurityRuleConditionHost;
    sourceIp?: outputs.SmartcaptchaCaptchaSecurityRuleConditionSourceIp;
    uri?: outputs.SmartcaptchaCaptchaSecurityRuleConditionUri;
}

export interface SmartcaptchaCaptchaSecurityRuleConditionHeader {
    name?: string;
    value: outputs.SmartcaptchaCaptchaSecurityRuleConditionHeaderValue;
}

export interface SmartcaptchaCaptchaSecurityRuleConditionHeaderValue {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SmartcaptchaCaptchaSecurityRuleConditionHost {
    hosts?: outputs.SmartcaptchaCaptchaSecurityRuleConditionHostHost[];
}

export interface SmartcaptchaCaptchaSecurityRuleConditionHostHost {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SmartcaptchaCaptchaSecurityRuleConditionSourceIp {
    geoIpMatch?: outputs.SmartcaptchaCaptchaSecurityRuleConditionSourceIpGeoIpMatch;
    geoIpNotMatch?: outputs.SmartcaptchaCaptchaSecurityRuleConditionSourceIpGeoIpNotMatch;
    ipRangesMatch?: outputs.SmartcaptchaCaptchaSecurityRuleConditionSourceIpIpRangesMatch;
    ipRangesNotMatch?: outputs.SmartcaptchaCaptchaSecurityRuleConditionSourceIpIpRangesNotMatch;
}

export interface SmartcaptchaCaptchaSecurityRuleConditionSourceIpGeoIpMatch {
    locations?: string[];
}

export interface SmartcaptchaCaptchaSecurityRuleConditionSourceIpGeoIpNotMatch {
    locations?: string[];
}

export interface SmartcaptchaCaptchaSecurityRuleConditionSourceIpIpRangesMatch {
    ipRanges?: string[];
}

export interface SmartcaptchaCaptchaSecurityRuleConditionSourceIpIpRangesNotMatch {
    ipRanges?: string[];
}

export interface SmartcaptchaCaptchaSecurityRuleConditionUri {
    path?: outputs.SmartcaptchaCaptchaSecurityRuleConditionUriPath;
    queries?: outputs.SmartcaptchaCaptchaSecurityRuleConditionUriQuery[];
}

export interface SmartcaptchaCaptchaSecurityRuleConditionUriPath {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SmartcaptchaCaptchaSecurityRuleConditionUriQuery {
    key: string;
    value: outputs.SmartcaptchaCaptchaSecurityRuleConditionUriQueryValue;
}

export interface SmartcaptchaCaptchaSecurityRuleConditionUriQueryValue {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface StorageBucketAnonymousAccessFlags {
    configRead?: boolean;
    /**
     * Allows to list object in bucket anonymously.
     */
    list?: boolean;
    /**
     * Allows to read objects in bucket anonymously.
     */
    read?: boolean;
}

export interface StorageBucketCorsRule {
    /**
     * Specifies which headers are allowed.
     */
    allowedHeaders?: string[];
    /**
     * Specifies which methods are allowed. Can be `GET`, `PUT`, `POST`, `DELETE` or `HEAD`.
     */
    allowedMethods: string[];
    /**
     * Specifies which origins are allowed.
     */
    allowedOrigins: string[];
    /**
     * Specifies expose header in the response.
     */
    exposeHeaders?: string[];
    /**
     * Specifies time in seconds that browser can cache the response for a preflight request.
     */
    maxAgeSeconds?: number;
}

export interface StorageBucketGrant {
    /**
     * Canonical user id to grant for. Used only when type is `CanonicalUser`.
     */
    id?: string;
    /**
     * List of permissions to apply for grantee. Valid values are `READ`, `WRITE`, `FULL_CONTROL`.
     */
    permissions: string[];
    /**
     * Type of grantee to apply for. Valid values are `CanonicalUser` and `Group`.
     */
    type: string;
    /**
     * Uri address to grant for. Used only when type is Group.
     *
     * > **Note:** To manage `grant` argument, service account with `storage.admin` role should be used.
     */
    uri?: string;
}

export interface StorageBucketHttps {
    /**
     * — Id of the certificate in Certificate Manager, that will be used for bucket.
     *
     * The `tags` object for setting tags (or labels) for bucket. See [tags](https://cloud.yandex.ru/docs/storage/concepts/tags) for more information.
     */
    certificateId: string;
}

export interface StorageBucketLifecycleRule {
    /**
     * Specifies the number of days after initiating a multipart upload when the multipart upload must be completed.
     */
    abortIncompleteMultipartUploadDays?: number;
    /**
     * Specifies lifecycle rule status.
     */
    enabled: boolean;
    /**
     * Specifies a period in the object's expire (documented below).
     */
    expiration?: outputs.StorageBucketLifecycleRuleExpiration;
    /**
     * Filter block identifies one or more objects to which the rule applies. A Filter must have exactly one of Prefix, Tag, or And specified. The filter supports the following options:
     *
     * - objectSizeGreaterThan - (Optional) Minimum object size to which the rule applies.
     * - objectSizeLessThan - (Optional) Maximum object size to which the rule applies.
     * - prefix - (Optional) Object key prefix identifying one or more objects to which the rule applies.
     * - tag - (Optional) A key and value pair for filtering objects. E.g.: key=key1, value=value1.
     * - and - (Optional) A logical `and` operator applied to one or more filter parameters. It should be used when two or more of the above parameters are used. It supports the following parameters:
     *
     * - objectSizeGreaterThan - (Optional) Minimum object size to which the rule applies.
     * - objectSizeLessThan - (Optional) Maximum object size to which the rule applies.
     * - prefix - (Optional) Object key prefix identifying one or more objects to which the rule applies.
     * - tags - (Optional) Key-value pairs for filtering objects. All of these tags must exist in the object's tags to apply the rule. E.g.: key1=value1, key2=value2
     */
    filter?: outputs.StorageBucketLifecycleRuleFilter;
    /**
     * Unique identifier for the rule. Must be less than or equal to 255 characters in length.
     */
    id: string;
    /**
     * Specifies when noncurrent object versions expire (documented below).
     */
    noncurrentVersionExpiration?: outputs.StorageBucketLifecycleRuleNoncurrentVersionExpiration;
    /**
     * Specifies when noncurrent object versions transitions (documented below).
     *
     * At least one of `abortIncompleteMultipartUploadDays`, `expiration`, `transition`, `noncurrentVersionExpiration`, `noncurrentVersionTransition` must be specified.
     */
    noncurrentVersionTransitions?: outputs.StorageBucketLifecycleRuleNoncurrentVersionTransition[];
    /**
     * Object key prefix identifying one or more objects to which the rule applies.
     *
     * @deprecated Use filter instead
     */
    prefix?: string;
    /**
     * Specifies a period in the object's transitions (documented below).
     */
    transitions?: outputs.StorageBucketLifecycleRuleTransition[];
}

export interface StorageBucketLifecycleRuleExpiration {
    /**
     * Specifies the date after which you want the corresponding action to take effect.
     */
    date?: string;
    /**
     * Specifies the number of days after object creation when the specific rule action takes effect.
     */
    days?: number;
    /**
     * On a versioned bucket (versioning-enabled or versioning-suspended bucket), you can add this element in the lifecycle configuration to direct Object Storage to delete expired object delete markers.
     */
    expiredObjectDeleteMarker?: boolean;
}

export interface StorageBucketLifecycleRuleFilter {
    and?: outputs.StorageBucketLifecycleRuleFilterAnd;
    objectSizeGreaterThan?: number;
    objectSizeLessThan?: number;
    /**
     * Object key prefix identifying one or more objects to which the rule applies.
     */
    prefix?: string;
    tag?: outputs.StorageBucketLifecycleRuleFilterTag;
}

export interface StorageBucketLifecycleRuleFilterAnd {
    objectSizeGreaterThan?: number;
    objectSizeLessThan?: number;
    /**
     * Object key prefix identifying one or more objects to which the rule applies.
     */
    prefix?: string;
    tags?: {[key: string]: string};
}

export interface StorageBucketLifecycleRuleFilterTag {
    key: string;
    value: string;
}

export interface StorageBucketLifecycleRuleNoncurrentVersionExpiration {
    /**
     * Specifies the number of days noncurrent object versions expire.
     */
    days?: number;
}

export interface StorageBucketLifecycleRuleNoncurrentVersionTransition {
    /**
     * Specifies the number of days noncurrent object versions transition.
     */
    days?: number;
    /**
     * Specifies the storage class to which you want the noncurrent object versions to transition. Supported values: [`STANDARD_IA`, `COLD`, `ICE`].
     */
    storageClass: string;
}

export interface StorageBucketLifecycleRuleTransition {
    /**
     * Specifies the date after which you want the corresponding action to take effect.
     */
    date?: string;
    /**
     * Specifies the number of days after object creation when the specific rule action takes effect.
     */
    days?: number;
    /**
     * Specifies the storage class to which you want the object to transition. Supported values: [`STANDARD_IA`, `COLD`, `ICE`].
     */
    storageClass: string;
}

export interface StorageBucketLogging {
    /**
     * The name of the bucket that will receive the log objects.
     */
    targetBucket: string;
    /**
     * To specify a key prefix for log objects.
     */
    targetPrefix?: string;
}

export interface StorageBucketObjectLockConfiguration {
    /**
     * Enable object locking in a bucket. Require versioning to be enabled.
     */
    objectLockEnabled?: string;
    /**
     * Specifies a default locking configuration for added objects. Require objectLockEnabled to be enabled.
     */
    rule?: outputs.StorageBucketObjectLockConfigurationRule;
}

export interface StorageBucketObjectLockConfigurationRule {
    defaultRetention: outputs.StorageBucketObjectLockConfigurationRuleDefaultRetention;
}

export interface StorageBucketObjectLockConfigurationRuleDefaultRetention {
    /**
     * Specifies the number of days after object creation when the specific rule action takes effect.
     */
    days?: number;
    /**
     * Specifies a type of object lock. One of `["GOVERNANCE", "COMPLIANCE"]`.
     */
    mode: string;
    /**
     * Specifies a retention period in years after uploading an object version. It must be a positive integer. You can't set it simultaneously with `days`.
     */
    years?: number;
}

export interface StorageBucketServerSideEncryptionConfiguration {
    /**
     * A single object for server-side encryption by default configuration. (documented below)
     */
    rule: outputs.StorageBucketServerSideEncryptionConfigurationRule;
}

export interface StorageBucketServerSideEncryptionConfigurationRule {
    /**
     * A single object for setting server-side encryption by default. (documented below)
     */
    applyServerSideEncryptionByDefault: outputs.StorageBucketServerSideEncryptionConfigurationRuleApplyServerSideEncryptionByDefault;
}

export interface StorageBucketServerSideEncryptionConfigurationRuleApplyServerSideEncryptionByDefault {
    /**
     * The KMS master key ID used for the SSE-KMS encryption.
     *
     * The `policy` object should contain the only field with the text of the policy. See [policy documentation](https://cloud.yandex.com/docs/storage/concepts/policy) for more information on policy format.
     *
     * Extended parameters of the bucket:
     *
     * > **NOTE:** for this parameters, authorization by `IAM-token` will be used.
     */
    kmsMasterKeyId: string;
    /**
     * The server-side encryption algorithm to use. Single valid value is `aws:kms`
     */
    sseAlgorithm: string;
}

export interface StorageBucketVersioning {
    /**
     * Enable versioning. Once you version-enable a bucket, it can never return to an unversioned state. You can, however, suspend versioning on that bucket.
     */
    enabled?: boolean;
}

export interface StorageBucketWebsite {
    /**
     * An absolute path to the document to return in case of a 4XX error.
     */
    errorDocument?: string;
    /**
     * Storage returns this index document when requests are made to the root domain or any of the subfolders.
     */
    indexDocument?: string;
    /**
     * A hostname to redirect all website requests for this bucket to. Hostname can optionally be prefixed with a protocol (`http://` or `https://`) to use when redirecting requests. The default is the protocol that is used in the original request.
     */
    redirectAllRequestsTo?: string;
    /**
     * A json array containing [routing rules](https://cloud.yandex.com/docs/storage/s3/api-ref/hosting/upload#request-scheme) describing redirect behavior and when redirects are applied.
     *
     * The `CORS` object supports the following:
     */
    routingRules?: string;
}

export interface SwsSecurityProfileSecurityRule {
    description?: string;
    dryRun?: boolean;
    name?: string;
    priority?: number;
    ruleCondition?: outputs.SwsSecurityProfileSecurityRuleRuleCondition;
    smartProtection?: outputs.SwsSecurityProfileSecurityRuleSmartProtection;
}

export interface SwsSecurityProfileSecurityRuleRuleCondition {
    action?: string;
    condition?: outputs.SwsSecurityProfileSecurityRuleRuleConditionCondition;
}

export interface SwsSecurityProfileSecurityRuleRuleConditionCondition {
    authority?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionAuthority;
    headers?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionHeader[];
    httpMethod?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionHttpMethod;
    requestUri?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionRequestUri;
    sourceIp?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionSourceIp;
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionAuthority {
    authorities?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionAuthorityAuthority[];
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionAuthorityAuthority {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionHeader {
    name?: string;
    value: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionHeaderValue;
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionHeaderValue {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionHttpMethod {
    httpMethods?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionHttpMethodHttpMethod[];
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionHttpMethodHttpMethod {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionRequestUri {
    path?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriPath;
    queries?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriQuery[];
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriPath {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriQuery {
    key: string;
    value: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriQueryValue;
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionRequestUriQueryValue {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionSourceIp {
    geoIpMatch?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpGeoIpMatch;
    geoIpNotMatch?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpGeoIpNotMatch;
    ipRangesMatch?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpIpRangesMatch;
    ipRangesNotMatch?: outputs.SwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpIpRangesNotMatch;
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpGeoIpMatch {
    locations?: string[];
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpGeoIpNotMatch {
    locations?: string[];
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpIpRangesMatch {
    ipRanges?: string[];
}

export interface SwsSecurityProfileSecurityRuleRuleConditionConditionSourceIpIpRangesNotMatch {
    ipRanges?: string[];
}

export interface SwsSecurityProfileSecurityRuleSmartProtection {
    condition?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionCondition;
    mode?: string;
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionCondition {
    authority?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionAuthority;
    headers?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionHeader[];
    httpMethod?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionHttpMethod;
    requestUri?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUri;
    sourceIp?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIp;
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionAuthority {
    authorities?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionAuthorityAuthority[];
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionAuthorityAuthority {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionHeader {
    name?: string;
    value: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionHeaderValue;
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionHeaderValue {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionHttpMethod {
    httpMethods?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionHttpMethodHttpMethod[];
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionHttpMethodHttpMethod {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUri {
    path?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriPath;
    queries?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriQuery[];
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriPath {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriQuery {
    key: string;
    value: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriQueryValue;
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionRequestUriQueryValue {
    exactMatch?: string;
    exactNotMatch?: string;
    pireRegexMatch?: string;
    pireRegexNotMatch?: string;
    prefixMatch?: string;
    prefixNotMatch?: string;
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIp {
    geoIpMatch?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpGeoIpMatch;
    geoIpNotMatch?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpGeoIpNotMatch;
    ipRangesMatch?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpIpRangesMatch;
    ipRangesNotMatch?: outputs.SwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpIpRangesNotMatch;
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpGeoIpMatch {
    locations?: string[];
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpGeoIpNotMatch {
    locations?: string[];
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpIpRangesMatch {
    ipRanges?: string[];
}

export interface SwsSecurityProfileSecurityRuleSmartProtectionConditionSourceIpIpRangesNotMatch {
    ipRanges?: string[];
}

export interface VpcAddressDnsRecord {
    /**
     * DNS zone id to create record at.
     */
    dnsZoneId: string;
    /**
     * FQDN for record to address
     */
    fqdn: string;
    /**
     * If PTR record is needed
     */
    ptr?: boolean;
    /**
     * TTL of DNS record
     */
    ttl?: number;
}

export interface VpcAddressExternalIpv4Address {
    /**
     * Allocated IP address.
     */
    address: string;
    /**
     * Enable DDOS protection. Possible values are: "qrator"
     */
    ddosProtectionProvider: string;
    /**
     * Wanted outgoing smtp capability.
     *
     * > **NOTE:** Either one `address` or `zoneId` arguments can be specified.
     * > **NOTE:** Either one `ddosProtectionProvider` or `outgoingSmtpCapability` arguments can be specified.
     * > **NOTE:** Change any argument in `externalIpv4Address` will cause an address recreate
     */
    outgoingSmtpCapability: string;
    /**
     * Zone for allocating address.
     */
    zoneId: string;
}

export interface VpcDefaultSecurityGroupEgress {
    /**
     * Description of the security group.
     */
    description?: string;
    /**
     * (Optional) - Minimum port number.
     */
    fromPort?: number;
    /**
     * Id of the security group.
     */
    id: string;
    /**
     * Labels to assign to this security group.
     */
    labels: {[key: string]: string};
    /**
     * (Optional) - Port number (if applied to a single port).
     */
    port?: number;
    /**
     * (Optional) - Special-purpose targets. `selfSecurityGroup` refers to this particular security group. `loadbalancerHealthchecks` represents [loadbalancer health check nodes](https://cloud.yandex.com/docs/network-load-balancer/concepts/health-check).
     */
    predefinedTarget?: string;
    /**
     * (Required) - One of `ANY`, `TCP`, `UDP`, `ICMP`, `IPV6_ICMP`.
     */
    protocol: string;
    /**
     * (Optional) - Target security group ID for this rule.
     */
    securityGroupId?: string;
    /**
     * (Optional) - Maximum port number.
     */
    toPort?: number;
    /**
     * (Optional) - The blocks of IPv4 addresses for this rule.
     */
    v4CidrBlocks?: string[];
    /**
     * (Optional) - The blocks of IPv6 addresses for this rule. `v6CidrBlocks` argument is currently not supported. It will be available in the future.
     */
    v6CidrBlocks?: string[];
}

export interface VpcDefaultSecurityGroupIngress {
    /**
     * Description of the security group.
     */
    description?: string;
    /**
     * (Optional) - Minimum port number.
     */
    fromPort?: number;
    /**
     * Id of the security group.
     */
    id: string;
    /**
     * Labels to assign to this security group.
     */
    labels: {[key: string]: string};
    /**
     * (Optional) - Port number (if applied to a single port).
     */
    port?: number;
    /**
     * (Optional) - Special-purpose targets. `selfSecurityGroup` refers to this particular security group. `loadbalancerHealthchecks` represents [loadbalancer health check nodes](https://cloud.yandex.com/docs/network-load-balancer/concepts/health-check).
     */
    predefinedTarget?: string;
    /**
     * (Required) - One of `ANY`, `TCP`, `UDP`, `ICMP`, `IPV6_ICMP`.
     */
    protocol: string;
    /**
     * (Optional) - Target security group ID for this rule.
     */
    securityGroupId?: string;
    /**
     * (Optional) - Maximum port number.
     */
    toPort?: number;
    /**
     * (Optional) - The blocks of IPv4 addresses for this rule.
     */
    v4CidrBlocks?: string[];
    /**
     * (Optional) - The blocks of IPv6 addresses for this rule. `v6CidrBlocks` argument is currently not supported. It will be available in the future.
     */
    v6CidrBlocks?: string[];
}

export interface VpcGatewaySharedEgressGateway {
}

export interface VpcRouteTableStaticRoute {
    /**
     * Route prefix in CIDR notation.
     */
    destinationPrefix?: string;
    /**
     * ID of the gateway used ad next hop.
     *
     * > **NOTE:** Only one of `nextHopAddress` or `gatewayId` should be specified.
     */
    gatewayId?: string;
    /**
     * Address of the next hop.
     */
    nextHopAddress?: string;
}

export interface VpcSecurityGroupEgress {
    /**
     * Description of the security group.
     */
    description?: string;
    /**
     * (Optional) - Minimum port number.
     */
    fromPort?: number;
    /**
     * Id of the rule.
     */
    id: string;
    /**
     * Labels to assign to this security group.
     */
    labels: {[key: string]: string};
    /**
     * (Optional) - Port number (if applied to a single port).
     */
    port?: number;
    /**
     * (Optional) - Special-purpose targets. `selfSecurityGroup` refers to this particular security group. `loadbalancerHealthchecks` represents [loadbalancer health check nodes](https://cloud.yandex.com/docs/network-load-balancer/concepts/health-check).
     */
    predefinedTarget?: string;
    /**
     * (Required) - One of `ANY`, `TCP`, `UDP`, `ICMP`, `IPV6_ICMP`.
     */
    protocol: string;
    /**
     * (Optional) - Target security group ID for this rule.
     */
    securityGroupId?: string;
    /**
     * (Optional) - Maximum port number.
     */
    toPort?: number;
    /**
     * (Optional) - The blocks of IPv4 addresses for this rule.
     */
    v4CidrBlocks?: string[];
    /**
     * (Optional) - The blocks of IPv6 addresses for this rule. `v6CidrBlocks` argument is currently not supported. It will be available in the future.
     */
    v6CidrBlocks?: string[];
}

export interface VpcSecurityGroupIngress {
    /**
     * Description of the security group.
     */
    description?: string;
    /**
     * (Optional) - Minimum port number.
     */
    fromPort?: number;
    /**
     * Id of the rule.
     */
    id: string;
    /**
     * Labels to assign to this security group.
     */
    labels: {[key: string]: string};
    /**
     * (Optional) - Port number (if applied to a single port).
     */
    port?: number;
    /**
     * (Optional) - Special-purpose targets. `selfSecurityGroup` refers to this particular security group. `loadbalancerHealthchecks` represents [loadbalancer health check nodes](https://cloud.yandex.com/docs/network-load-balancer/concepts/health-check).
     */
    predefinedTarget?: string;
    /**
     * (Required) - One of `ANY`, `TCP`, `UDP`, `ICMP`, `IPV6_ICMP`.
     */
    protocol: string;
    /**
     * (Optional) - Target security group ID for this rule.
     */
    securityGroupId?: string;
    /**
     * (Optional) - Maximum port number.
     */
    toPort?: number;
    /**
     * (Optional) - The blocks of IPv4 addresses for this rule.
     */
    v4CidrBlocks?: string[];
    /**
     * (Optional) - The blocks of IPv6 addresses for this rule. `v6CidrBlocks` argument is currently not supported. It will be available in the future.
     */
    v6CidrBlocks?: string[];
}

export interface VpcSubnetDhcpOptions {
    /**
     * Domain name.
     */
    domainName?: string;
    /**
     * Domain name server IP addresses.
     */
    domainNameServers?: string[];
    /**
     * NTP server IP addresses.
     */
    ntpServers?: string[];
}

export interface YdbDatabaseDedicatedLocation {
    /**
     * Region for the Yandex Database cluster.
     * The structure is documented below.
     */
    region?: outputs.YdbDatabaseDedicatedLocationRegion;
}

export interface YdbDatabaseDedicatedLocationRegion {
    /**
     * Region ID for the Yandex Database cluster.
     */
    id: string;
}

export interface YdbDatabaseDedicatedScalePolicy {
    /**
     * Fixed scaling policy for the Yandex Database cluster.
     * The structure is documented below.
     *
     * > **NOTE:** Currently, only `fixedScale` is supported.
     */
    fixedScale: outputs.YdbDatabaseDedicatedScalePolicyFixedScale;
}

export interface YdbDatabaseDedicatedScalePolicyFixedScale {
    /**
     * Number of instances for the Yandex Database cluster.
     */
    size: number;
}

export interface YdbDatabaseDedicatedStorageConfig {
    /**
     * Amount of storage groups of selected type for the Yandex Database cluster.
     */
    groupCount: number;
    /**
     * Storage type ID for the Yandex Database cluster.
     * Available presets can be obtained via `yc ydb storage-type list` command.
     */
    storageTypeId: string;
}

export interface YdbDatabaseServerlessServerlessDatabase {
    enableThrottlingRcuLimit: boolean;
    provisionedRcuLimit: number;
    storageSizeLimit: number;
    throttlingRcuLimit: number;
}

export interface YdbTableChangefeedConsumer {
    important: boolean;
    name: string;
    startingMessageTimestampMs: number;
    supportedCodecs: string[];
}

export interface YdbTableColumn {
    /**
     * Column group
     */
    family: string;
    /**
     * Column family name
     */
    name: string;
    /**
     * A column cannot have the NULL data type. (	Default: false	)
     */
    notNull: boolean;
    /**
     * Column data type. YQL data types are used.
     */
    type: string;
}

export interface YdbTableFamily {
    /**
     * Data codec (acceptable values: off, lz4).
     */
    compression: string;
    /**
     * Type of storage device for column data in this group (acceptable values: ssd, rot (from HDD spindle rotation)).
     */
    data: string;
    /**
     * Column family name
     */
    name: string;
}

export interface YdbTablePartitioningSettings {
    autoPartitioningByLoad?: boolean;
    autoPartitioningBySizeEnabled?: boolean;
    autoPartitioningMaxPartitionsCount: number;
    autoPartitioningMinPartitionsCount: number;
    autoPartitioningPartitionSizeMb: number;
    partitionAtKeys: outputs.YdbTablePartitioningSettingsPartitionAtKey[];
    uniformPartitions: number;
}

export interface YdbTablePartitioningSettingsPartitionAtKey {
    keys: string[];
}

export interface YdbTableTtl {
    /**
     * Column name for TTL
     */
    columnName: string;
    /**
     * Interval in the ISO 8601 format
     */
    expireInterval: string;
    unit: string;
}

export interface YdbTopicConsumer {
    important: boolean;
    /**
     * Topic name. Type: string, required. Default value: "".
     */
    name: string;
    startingMessageTimestampMs: number;
    /**
     * Supported data encodings. Types: array[string]. Default value: ["gzip", "raw", "zstd"].
     */
    supportedCodecs: string[];
}

